{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1374d-3694-4547-8165-954a3fb6e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import scipy.stats as scp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "import itertools\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import scipy.signal as scs\n",
    "import json\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import combinations\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set(context='talk', style='white', font='Arial')\n",
    "\n",
    "today = date.today().strftime('%Y%m%d')\n",
    "\n",
    "project_dir = '/Users/catcamacho/Library/CloudStorage/Box-Box/CCP/HBN_study/'\n",
    "data_dir = project_dir + 'proc/group/parcel_timeseries/sub_ts/'\n",
    "out_dir = project_dir + 'proc/clin/'\n",
    "os.makedirs(out_dir,exist_ok=True)\n",
    "\n",
    "big_data_dir = '/Users/catcamacho/Documents/bigdata/hbn_clin/'\n",
    "\n",
    "sample_file = project_dir + 'proc/group/datasets_info/sample_gord.32k_fs_LR.pscalar.nii'\n",
    "atlas_file = project_dir + 'proc/null_lL_WG33/Gordon333_SeitzmanSubcortical.32k_fs_LR.dlabel.nii'\n",
    "\n",
    "ax0 = nib.load(sample_file).header.get_axis(0)\n",
    "ax1 = nib.load(sample_file).header.get_axis(1)\n",
    "\n",
    "TR = 0.8\n",
    "\n",
    "# get network labels\n",
    "parcel_labels = nib.load(sample_file).header.get_axis(1).name\n",
    "network_labels = []\n",
    "for s in parcel_labels:\n",
    "    b = s.split('_')\n",
    "    if len(b)<2:\n",
    "        network_labels.append(b[0])\n",
    "    else:\n",
    "        network_labels.append(b[1])\n",
    "network_labels = np.array(network_labels)\n",
    "network_names, network_sizes = np.unique(network_labels, return_counts=True)\n",
    "\n",
    "# load timeseries data info\n",
    "subinfo = pd.read_csv(project_dir + 'proc/group/datasets/firstleveldatalabels_withpub_thresh0.8_20220412.csv', index_col=0)\n",
    "mfqsr = pd.read_csv(os.path.join(out_dir, 'MFQsr_factorscores_20220629.csv'), index_col=0)\n",
    "mfqsr.index = ['sub-{0}'.format(s) for s in mfqsr.index]\n",
    "mfqsr.index.name='sub'\n",
    "mfqpr = pd.read_csv(os.path.join(out_dir, 'MFQpr_factorscores_20220629.csv'), index_col=0)\n",
    "mfqpr.index = ['sub-{0}'.format(s) for s in mfqpr.index]\n",
    "mfqpr.index.name='sub'\n",
    "\n",
    "scaredsr = pd.read_csv(os.path.join(project_dir, 'phenotypic_data','9994_SCARED_SR_20210322.csv'), index_col='EID', skiprows=[1]).loc[:,['SCARED_SR_SC','SCARED_SR_GD','SCARED_SR_SP']]\n",
    "scaredsr.index = ['sub-{0}'.format(a) for a in scaredsr.index]\n",
    "scaredsr.index.name = 'sub'\n",
    "\n",
    "scaredpr = pd.read_csv(os.path.join(project_dir, 'phenotypic_data','9994_SCARED_P_20210322.csv'), index_col='EID', skiprows=[1]).loc[:,['SCARED_P_SC','SCARED_P_GD','SCARED_P_SP']]\n",
    "scaredpr.index = ['sub-{0}'.format(a) for a in scaredpr.index]\n",
    "scaredpr.index.name = 'sub'\n",
    "\n",
    "adhd = pd.read_csv(os.path.join(project_dir, 'phenotypic_data','9994_SWAN_20210322.csv'), index_col='EID', skiprows=[1]).loc[:,'SWAN_Avg']\n",
    "adhd.index = ['sub-{0}'.format(a) for a in adhd.index]\n",
    "adhd.index.name = 'sub'\n",
    "\n",
    "clininfo = pd.read_csv(os.path.join(out_dir, 'depanx_scores_preproc_20220519.csv'), low_memory=False, index_col=0).drop(['age'],axis=1)\n",
    "subinfo = subinfo.merge(clininfo, how='left', left_index=True, right_index=True)\n",
    "subinfo = subinfo.merge(mfqpr, how='left', left_index=True, right_index=True)\n",
    "subinfo = subinfo.merge(mfqsr, how='left', left_index=True, right_index=True)\n",
    "subinfo = subinfo.merge(adhd, how='left', left_index=True, right_index=True)\n",
    "subinfo = subinfo.drop(['set','sub','cond','SCARED_P_SC','SCARED_SR_SC'], axis=1)\n",
    "subinfo = subinfo.drop_duplicates()\n",
    "subinfo.index.name='sub'\n",
    "\n",
    "subinfo = subinfo.merge(scaredsr, how='left', left_index=True, right_index=True)\n",
    "subinfo = subinfo.merge(scaredpr, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# assign developmental groups\n",
    "subinfo['age_group'] = 'younger'\n",
    "subinfo.loc[(subinfo['age']>10), 'age_group'] = 'older'\n",
    "\n",
    "subinfo = subinfo.loc[(np.isfinite(subinfo['SCARED_SR_SC']) | np.isfinite(subinfo['SCARED_P_SC'])), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076b86f-a5fd-45fe-9c3b-11f99def4cc5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06eb9a7-8374-4f79-a9d3-2ff569e8eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_ts_data(subdf, movie, datadir, outfile):\n",
    "    \"\"\"\n",
    "    combine data for each movie together into 1 file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subdf: DataFrame\n",
    "        A dataframe with subject IDs as the index. Includes IDs for all usable data.\n",
    "    movie: str\n",
    "        Corresponds with the str for the movie content to concatenate (e.g., \"DM\" or \"TP\").\n",
    "    datadir: folder path\n",
    "        Path to folder with the subject timeseries ciftis.\n",
    "    outfile: file path\n",
    "        Path including filename to save the output data of shape Ntimepoints x Nparcels x Nsubjects.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: numpy array\n",
    "        The compiled data of shape Ntimepoints x Nparcels x Nsubjects\n",
    "    \"\"\"\n",
    "    if not isinstance(subdf, pd.DataFrame):\n",
    "        subdf = pd.read_csv(subdf, index_col=0)\n",
    "    \n",
    "    for sub in subdf.index:\n",
    "        file = '{0}{1}_task-movie{2}_bold1_AP_Atlas_rescale_resid0.9_filt_gordonseitzman.32k_fs_LR.ptseries.nii'.format(datadir,sub, movie)\n",
    "        if sub == subdf.index[0]:\n",
    "            data = StandardScaler().fit_transform(nib.load(file).get_fdata())\n",
    "            data = np.expand_dims(data, axis=2)\n",
    "        else:\n",
    "            t = StandardScaler().fit_transform(nib.load(file).get_fdata())\n",
    "            t = np.expand_dims(t, axis=2)\n",
    "            data = np.concatenate([data,t],axis=2)\n",
    "    \n",
    "    print('Compile data from {0} brain regions measured at {1} timepoints from {2} participants.'.format(data.shape[1],data.shape[0],data.shape[2]))\n",
    "    np.save(outfile, data)\n",
    "    return(data)\n",
    "\n",
    "\n",
    "def compute_group_phase(group_ts_data, outfile):\n",
    "    \"\"\"\n",
    "    convert parcel ts to standard units & compute phase angles for each parcel ts\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group_ts_data: filepath OR numpy array\n",
    "        File or numpy array with compiled timeseries data of shape Ntimepoints x Nparcels x Nsubjects \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    group_phase_data: numpy array\n",
    "        The compiled data of shape Ntimepoints x Nparcels x Nsubjects\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(group_ts_data, np.ndarray):\n",
    "        group_ts_data = np.load(group_ts_data)\n",
    "    \n",
    "    group_phase_data = np.zeros_like(group_ts_data)\n",
    "    \n",
    "    for a in range(0,group_ts_data.shape[1]):\n",
    "        for b in range(0,group_ts_data.shape[2]):\n",
    "            group_phase_data[:,a,b] = np.angle(hilbert(group_ts_data[:,a,b]), deg=False)\n",
    "    \n",
    "    np.save(outfile, group_phase_data)\n",
    "    \n",
    "    return(group_phase_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86f2e7-1780-4772-a259-f62a091dc684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_isps(group_phase_data, outprefix, savemean=True, small=False):\n",
    "    \"\"\"\n",
    "    parcel-wise inter-subject phase synchrony- output pairwise IPS and mean global IPS\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group_phase_data: numpy array\n",
    "        The compiled data of shape Ntimepoints x Nparcels x Nsubjects\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    isps_data: numpy array\n",
    "        intersubject phase synchrony data of shape Nparcels x Nsubjects x Nsubjects x Ntimepoints\n",
    "    mean_isps_data: numpy array\n",
    "        intersubject phase synchrony data, averaged across time, of shape Nparcels x Nsubjects x Nsubjects\n",
    "        \n",
    "    \"\"\"\n",
    "    if not isinstance(group_phase_data, np.ndarray):\n",
    "        group_phase_data = np.load(group_phase_data)\n",
    "    \n",
    "    if os.path.isdir(outprefix):\n",
    "        file_name = os.path.join(outprefix, 'isps_data.dat')\n",
    "    else:\n",
    "        file_name = outprefix + 'isps_data.dat'\n",
    "        \n",
    "    if not small:\n",
    "        isps_data = np.memmap(file_name, dtype=np.float32, mode='w+',\n",
    "                              shape=(group_phase_data.shape[1],group_phase_data.shape[2],\n",
    "                                     group_phase_data.shape[2],\n",
    "                                     group_phase_data.shape[0]))\n",
    "    else:\n",
    "        isps_data = np.empty((group_phase_data.shape[1],group_phase_data.shape[2],\n",
    "                              group_phase_data.shape[2],group_phase_data.shape[0]))\n",
    "    \n",
    "    subs = range(0, group_phase_data.shape[2])\n",
    "    for region in range(0, group_phase_data.shape[1]):\n",
    "        combs = itertools.combinations(subs, 2)\n",
    "        for c in combs:\n",
    "            sub1 = group_phase_data[:, region, c[0]]\n",
    "            sub2 = group_phase_data[:, region, c[1]]\n",
    "            a = 1 - np.sin(np.abs(sub1 - sub2) / 2)\n",
    "            isps_data[region,c[0],c[1],:] = a\n",
    "            isps_data[region,c[1],c[0],:] = a\n",
    "        \n",
    "    if small:\n",
    "        np.save(file_name, isps_data)\n",
    "    if savemean:\n",
    "        mask = np.tri(isps_data.shape[2], isps_data.shape[2], -1, dtype=int)\n",
    "        mean_isps_data = np.mean(isps_data[:,mask==1,:], axis=1)\n",
    "        if os.path.isdir(outprefix):\n",
    "            mean_file_name = os.path.join(outprefix, 'mean_isps_data.npy')\n",
    "        else:\n",
    "            mean_file_name = outprefix + 'mean_isps_data.npy'\n",
    "        \n",
    "        np.save(mean_file_name, mean_isps_data.T)\n",
    "    \n",
    "        return(mean_isps_data, isps_data)\n",
    "    else:\n",
    "        return(isps_data)\n",
    "\n",
    "\n",
    "def intersubject_timeseries_correlation(data, outprefix, ax0=ax0, ax1=ax1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy array\n",
    "        data in the shape of Ntimepoints x Nregions x Nsubjects\n",
    "    outprefix: str\n",
    "        name to save ISC data to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    intersub_isc: numpy array\n",
    "        intersubject spearman correlations in the shape of Nregions x Nsubjects x Nsubjects\n",
    "    group_isc: numpy array\n",
    "        group mean spearman correlations in the shape of Nregions\n",
    "    \"\"\"\n",
    "    subs = range(0,data.shape[2])\n",
    "    \n",
    "    intersub_isc = np.zeros((data.shape[1],data.shape[2],data.shape[2]))\n",
    "    group_isc = np.zeros((data.shape[1]))\n",
    "    mask = np.tri(data.shape[2], data.shape[2], -1, dtype=int)\n",
    "    \n",
    "    for r in range(0, data.shape[1]):\n",
    "        intersub_isc[r, :, :]= np.corrcoef(data[:, r, :], rowvar=False)\n",
    "            \n",
    "    for r in range(0, data.shape[1]):\n",
    "        group_isc[r] = np.mean(intersub_isc[r,:,:][mask==1])\n",
    "    \n",
    "    np.save(outprefix + 'intersub_timeseries_ISC.npy', intersub_isc)\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(np.expand_dims(group_isc, axis=0), (ax0, ax1))\n",
    "    nib.save(img, outprefix + 'mean_timseries_ISC.pscalar.nii')\n",
    "    \n",
    "    return(intersub_isc, group_isc)\n",
    "\n",
    "\n",
    "def intersubject_distance(data, outfile_prefix):\n",
    "    \"\"\"\n",
    "    Compute static pairwise intersubject similarity\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy array\n",
    "        1D array of subject data (i.e., each participant contributes exactly 1 measure)\n",
    "    outfilename: str\n",
    "        name to save distance data to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    isdistances: numpy array\n",
    "        intersubject distances in the shape of Nsubjects x Nsubjects x Nmetrics\n",
    "    \"\"\"\n",
    "    subs = range(0,data.shape[0])\n",
    "\n",
    "\n",
    "    # NN\n",
    "    nn = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        nn[c[0],c[1]] = np.max(data) - abs(data[c[0]] - data[c[1]])\n",
    "        nn[c[1],c[0]] = np.max(data) - abs(data[c[0]] - data[c[1]])\n",
    "    np.save(outfile_prefix + '_NN.npy', nn)\n",
    "\n",
    "    # AnnaK mean\n",
    "    annakmean = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        annakmean[c[0],c[1]] = (data[c[0]] + data[c[1]]) / 2\n",
    "        annakmean[c[1],c[0]] = (data[c[0]] + data[c[1]]) / 2\n",
    "    np.save(outfile_prefix + '_annakmean.npy', annakmean)\n",
    "    \n",
    "    # AnnaK max min mean\n",
    "    AnnaKmaxminmean = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        AnnaKmaxminmean[c[0],c[1]] = np.max(data) - ((data[c[0]] + data[c[1]]) / 2)\n",
    "        AnnaKmaxminmean[c[1],c[0]] = np.max(data) - ((data[c[0]] + data[c[1]]) / 2)\n",
    "    np.save(outfile_prefix + '_annakmaxminmean.npy', AnnaKmaxminmean)\n",
    "\n",
    "    # AnnaK min\n",
    "    annakmin = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        annakmin[c[0],c[1]] = min([data[c[0]],data[c[1]]])\n",
    "        annakmin[c[1],c[0]] = min([data[c[0]],data[c[1]]])\n",
    "    np.save(outfile_prefix + '_annakmin.npy', annakmin)\n",
    "\n",
    "    # AnnaK max minus min\n",
    "    annakmaxminmax = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        annakmaxminmax[c[0],c[1]] =np.max(data) -  max([data[c[0]],data[c[1]]])\n",
    "        annakmaxminmax[c[1],c[0]] = np.max(data) - max([data[c[0]],data[c[1]]])\n",
    "    np.save(outfile_prefix + '_annakmaxminmax.npy', annakmaxminmax)\n",
    "        \n",
    "    # AnnaK absmean\n",
    "    annakabsmean = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        annakabsmean[c[0],c[1]] = abs(data[c[0]] - data[c[1]]) * ((data[c[0]] + data[c[1]]) / 2)\n",
    "        annakabsmean[c[1],c[0]] = abs(data[c[0]] - data[c[1]]) * ((data[c[0]] + data[c[1]]) / 2)\n",
    "    np.save(outfile_prefix + '_annakabsmean.npy', annakabsmean)\n",
    "    \n",
    "    isdistances = {'NN': nn, \n",
    "                   'AnnaKmean': annakmean, \n",
    "                   'AnnaKmin': annakmin, \n",
    "                   'AnnaKabsmean': annakabsmean, \n",
    "                   'AnnaKmaxminmean': AnnaKmaxminmean, \n",
    "                   'AnnaKmaxminmax': annakmaxminmax}\n",
    "    return(isdistances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98988a-b4cb-48d3-b379-83c831526050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_brain_bx_isrsa(brain_sim_data, bx_sim_data, outfilename=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    brain_sim_data: numpy ndarray\n",
    "        Data in the shape of Nsubjects x Nsubjects\n",
    "    bx_sim_data: numpy ndarray\n",
    "        Data in the shape of Nsubjects x Nsubjects\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rsa_report: pandas DataFrame\n",
    "        Pandas DataFrame with inter-subject representational similarity statistics\n",
    "    \"\"\"\n",
    "    rsa_report = pd.DataFrame(columns=['SpearR','SpearPvalue'])\n",
    "    \n",
    "    mask = np.tri(bx_sim_data.shape[0], bx_sim_data.shape[0], -1, dtype=int)\n",
    "    bx_sim = bx_sim_data[mask==1]\n",
    "    brain_sim = brain_sim_data[mask==1]\n",
    "    \n",
    "    r, p = scp.spearmanr(bx_sim, brain_sim)\n",
    "    rsa_report.loc[0,'SpearR'] = r\n",
    "    rsa_report.loc[0,'SpearPvalue'] = p\n",
    "    if outfilename:\n",
    "        sns.scatterplot(bx_sim, brain_sim)\n",
    "        plt.title('Similarity Correlation')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(outfilename)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    return(rsa_report)\n",
    "\n",
    "def regional_perm_bx_isrsa(regional_sim_data, bx_sim_data, outprefix, alpha=0.05, n_perms=1000, ax0=ax0, ax1=ax1):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    regional_sim_data: numpy ndarray\n",
    "        Data in the shape of Nregions x Nsubjects x Nsubjects\n",
    "    bx_sim_data: numpy ndarray\n",
    "        Data in the shape of Nsubjects x Nsubjects\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    region_isrsa: numpy ndarray\n",
    "        Data in the shape of Nregions\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = np.tri(bx_sim_data.shape[1], bx_sim_data.shape[1], -1, dtype=int)\n",
    "\n",
    "    # flatten behavior lower triangle\n",
    "    bx_sim = bx_sim_data[mask==1]\n",
    "\n",
    "    region_isrsa = np.zeros((regional_sim_data.shape[0]))\n",
    "\n",
    "    for region in range(0, regional_sim_data.shape[0]):\n",
    "            brain_sim = regional_sim_data[region,:,:][mask==1]\n",
    "            r, p = scp.spearmanr(bx_sim, brain_sim)\n",
    "            region_isrsa[region] = r\n",
    "\n",
    "    shuff_bx = bx_sim\n",
    "    perm_isrsa_null = np.zeros((n_perms, regional_sim_data.shape[0]))\n",
    "\n",
    "    # make null distributions for each TR and region\n",
    "    for a in range(0,n_perms):\n",
    "        np.random.shuffle(shuff_bx)\n",
    "        for region in range(0,regional_sim_data.shape[0]):\n",
    "            brain_sim = regional_sim_data[region,:,:][mask==1]\n",
    "            r, p = scp.spearmanr(shuff_bx, brain_sim)\n",
    "            perm_isrsa_null[a, region] = r\n",
    "\n",
    "    # compute permuted P threshold per region/TR\n",
    "    raw_pvals = np.zeros(region_isrsa.shape)\n",
    "    flat_null = perm_isrsa_null.flatten()\n",
    "    for i, a in enumerate(region_isrsa):\n",
    "        raw_pvals[i] = (np.sum((flat_null>=a).astype(int)) + 1) / (flat_null.shape[0] + 1)\n",
    "        \n",
    "    # save ciftis with raw values\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(np.expand_dims(raw_pvals, axis=0), (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permsim_raw_pval.pscalar.nii')\n",
    "    \n",
    "    img = nib.cifti2.cifti2.Cifti2Image(np.expand_dims(region_isrsa, axis=0), (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permsim_raw_rho.pscalar.nii')\n",
    "    \n",
    "    \n",
    "    # save cifti with significant rhos only\n",
    "    thresh_mask = raw_pvals<alpha\n",
    "\n",
    "    # pvals\n",
    "    thresh_pval = raw_pvals\n",
    "    thresh_pval[thresh_mask==0] = np.nan\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(np.expand_dims(thresh_pval, axis=0), (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permsim_masked_pval{0}.pscalar.nii'.format(alpha))\n",
    "\n",
    "    # rhos\n",
    "    thresh_isrsa = region_isrsa\n",
    "    thresh_isrsa[thresh_mask==0] = np.nan\n",
    "    thresh_isrsa[thresh_isrsa<0] = np.nan\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(np.expand_dims(thresh_isrsa, axis=0), (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permsim_masked_rho{0}.pscalar.nii'.format(alpha))\n",
    "    return(thresh_isrsa)\n",
    "\n",
    "\n",
    "# compute dynamic similarity\n",
    "def dynamic_brain_bx_isrsa(time_region_sim_data, bx_sim_data, outprefix, ax1=ax1):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    time_region_sim_data: numpy ndarray\n",
    "        Data in the shape of Nregions x Nsubjects x Nsubjects x Ntimepoints\n",
    "    bx_sim_data: numpy ndarray\n",
    "        Data in the shape of Nsubjects x Nsubjects\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dynamic_isrsa: pandas DataFrame\n",
    "        Pandas DataFrame with inter-subject representational similarity statistics\n",
    "    \"\"\"\n",
    "    mask = np.tri(bx_sim_data.shape[1], bx_sim_data.shape[1], -1, dtype=int)\n",
    "\n",
    "    # flatten behavior lower triangle\n",
    "    bx_sim = bx_sim_data[mask==1]\n",
    "\n",
    "    dynamic_isrsa = np.zeros((isps_data.shape[3],isps_data.shape[0]))\n",
    "\n",
    "    for region in range(0, isps_data.shape[0]):\n",
    "        for tr in range(0, isps_data.shape[3]):\n",
    "            brain_sim = isps_data[region,:,:,tr][mask==1]\n",
    "            r, p = scp.spearmanr(bx_sim, brain_sim)\n",
    "            dynamic_isrsa[tr, region] = r\n",
    "            \n",
    "    np.save(outfile_prefix + '_dynamicsim.npy', dynamic_isrsa)\n",
    "    \n",
    "    ax0 = nib.cifti2.cifti2_axes.SeriesAxis(0,0.8,dynamic_isrsa.shape[0], unit='second')\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(dynamic_isrsa, (ax0, ax1))\n",
    "    nib.save(img, outfile_prefix + '_dynamicsim.ptseries.nii')\n",
    "\n",
    "    return(dynamic_isrsa)\n",
    "\n",
    "\n",
    "def perm_sig_dynamic_isrsa(time_region_sim_data, bx_sim_data, outprefix, n_perms=30, ax1=ax1, alpha=0.05, TR=0.8):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dynamic_isrsa = dynamic_brain_bx_isrsa(time_region_sim_data, bx_sim_data, outprefix)\n",
    "    \n",
    "    if os.path.isfile(os.path.join(outprefix + '_perm_isrsa_null.dat')):\n",
    "        perm_isrsa_null = np.memmap(os.path.join(outprefix + '_perm_isrsa_null.dat'), dtype=np.float32, mode='r',\n",
    "                                    shape=(n_perms, isps_data.shape[3], isps_data.shape[0]))\n",
    "    else:\n",
    "        perm_isrsa_null = np.memmap(os.path.join(outprefix + '_perm_isrsa_null.dat'), dtype=np.float32, mode='w+',\n",
    "                                    shape=(n_perms, isps_data.shape[3], isps_data.shape[0]))\n",
    "        shuff_bx = bx_sim_data\n",
    "\n",
    "        # make null distributions for each TR and region\n",
    "        for a in range(0,n_perms):\n",
    "            np.random.shuffle(shuff_bx)\n",
    "            perm_isrsa_null[a, :, :] = dynamic_brain_bx_isrsa(time_region_sim_data, shuff_bx, outprefix)\n",
    "    \n",
    "    # compute permuted P threshold per region/TR\n",
    "    orig_shape = dynamic_isrsa.shape\n",
    "    flat_dyn_isrsa = dynamic_isrsa.flatten()\n",
    "    raw_pvals = np.zeros(flat_dyn_isrsa.shape)\n",
    "    flat_null = perm_isrsa_null.flatten()\n",
    "    for i, a in enumerate(flat_dyn_isrsa):\n",
    "        raw_pvals[i] = (np.sum((flat_null>=a).astype(int)) + 1) / (flat_null.shape[0] + 1)\n",
    "\n",
    "    raw_pvals = np.reshape(raw_pvals, orig_shape)\n",
    "    \n",
    "    # save cifti\n",
    "    ax0 = nib.cifti2.cifti2_axes.SeriesAxis(0,TR,raw_pvals.shape[0], unit='second')\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(dynamic_isrsa, (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permdynamicsim_raw_rho.ptseries.nii')\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(raw_pvals, (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permdynamicsim_raw_pval.ptseries.nii')\n",
    "    \n",
    "    # save cifti with significant rhos only\n",
    "    thresh_mask = raw_pvals<alpha\n",
    "\n",
    "    # pvals\n",
    "    thresh_pval = raw_pvals\n",
    "    thresh_pval[thresh_mask==0] = np.nan\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(thresh_pval, (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permdynamicsim_thresh_pval{0}.ptseries.nii'.format(alpha))\n",
    "    # rhos\n",
    "    thresh_isrsa = dynamic_isrsa\n",
    "    thresh_isrsa[thresh_mask==0] = np.nan\n",
    "    thresh_isrsa[thresh_isrsa<0] = np.nan\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(thresh_isrsa, (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permdynamicsim_thresh_rho{0}.ptseries.nii'.format(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39bc5b-45a3-40d8-8c67-5d6e19ef51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_isrsa_fdr(disc_rho, disc_pval, rep_rho, rep_pval, outprefix, alpha=0.05, bon_alpha=True,replace_zeros=True, ax0=ax0, ax1=ax1):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    disc_rho = nib.load(disc_rho).get_fdata()\n",
    "    disc_pval = nib.load(disc_pval).get_fdata()\n",
    "    rep_rho = nib.load(rep_rho).get_fdata()\n",
    "    rep_pval = nib.load(rep_pval).get_fdata()\n",
    "    \n",
    "    if replace_zeros:\n",
    "        disc_pval[disc_pval==0] = np.nan\n",
    "        rep_pval[rep_pval==0] = np.nan\n",
    "    \n",
    "    if bon_alpha==True:\n",
    "        bon_alpha = np.sqrt(alpha/disc_pval.shape[1])\n",
    "    else:\n",
    "        bon_alpha = alpha\n",
    "\n",
    "    dmask = (disc_pval<bon_alpha).astype(int)\n",
    "    rmask = (rep_pval<bon_alpha).astype(int)\n",
    "\n",
    "    mask = np.zeros(dmask.shape)\n",
    "    mask[(dmask==1) & (rmask==1)] = 1\n",
    "\n",
    "    bonrho = np.empty(mask.shape)\n",
    "    bonrho[mask==1] = np.add(disc_rho[mask==1],rep_rho[mask==1])/2\n",
    "    bonrho[mask==0] = np.nan\n",
    "\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(bonrho, (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_maskedrho_fdr{0}.pscalar.nii'.format(round(bon_alpha,5)))\n",
    "    \n",
    "    \n",
    "def dynamic_isrsa_fdr(disc_rho, rep_rho, discN, repN, video_dur, ratings_file, outprefix, TR=TR, alpha=0.05, parcel_labels=parcel_labels):\n",
    "    \n",
    "    # assign thresholds\n",
    "    bon_alpha = np.sqrt(0.05/disc_rho.shape[1])\n",
    "    discbonnmint = scp.t.ppf(1-bon_alpha, discN-1)\n",
    "    discuncorrmint = scp.t.ppf(1-alpha, discN-1)\n",
    "    repbonnmint = scp.t.ppf(1-bon_alpha, repN-1)\n",
    "    repuncorrmint = scp.t.ppf(1-alpha, repN-1)\n",
    "\n",
    "    # convert rho to t-stat\n",
    "    disc_tstat = disc_rho/np.sqrt((1-disc_rho**2)/(discN-2))\n",
    "    rep_tstat = rep_rho/np.sqrt((1-rep_rho**2)/(repN-2))\n",
    "\n",
    "    # find sig ts that replicate\n",
    "    disc_sigts =  (disc_tstat > discbonnmint).astype(int)\n",
    "    rep_sigts =  (rep_tstat > repbonnmint).astype(int)\n",
    "\n",
    "    sigts = np.zeros(disc_sigts.shape).astype(int)\n",
    "    sigts[(disc_sigts==1) & (rep_sigts==1)] = 1\n",
    "    if sigts.max()==0:\n",
    "        print('No significant findings.')\n",
    "    else:\n",
    "        print('At least one region is significantly simliar.')\n",
    "    \n",
    "    # Plot and analysis replicating peaks\n",
    "    times = np.arange(0, video_dur, TR)\n",
    "\n",
    "    results = dict()\n",
    "\n",
    "    ### Make plots ###\n",
    "    for p, parcel in enumerate(parcel_labels):\n",
    "        if np.max(sigts.T[p])==1:\n",
    "            # set up figure\n",
    "            fig, ax = plt.subplots(2,1,figsize=(12,6), sharex=True, sharey=True)\n",
    "            # plot discovery\n",
    "            dupeaks, duproperties = scs.find_peaks(disc_tstat.T[p], width=5, prominence=discuncorrmint)\n",
    "\n",
    "            ax[0].plot(times, disc_tstat.T[p], color='k')\n",
    "            for i_c, c in enumerate(duproperties['prominences']):\n",
    "                ax[0].axvspan(duproperties['left_ips'][i_c]*TR, duproperties['right_ips'][i_c]*TR,\n",
    "                            color='r', alpha=0.3)\n",
    "            dcpeaks, dcproperties = scs.find_peaks(disc_tstat.T[p], width=5, prominence=discbonnmint)\n",
    "            for i_c, c in enumerate(dcproperties['prominences']):\n",
    "                ax[0].axvspan(dcproperties['left_ips'][i_c]*TR, dcproperties['right_ips'][i_c]*TR,\n",
    "                            color='b', alpha=0.3)\n",
    "            ax[0].set_xlim([0,video_dur])\n",
    "            ax[0].set_title(parcel + ': Discovery', weight='bold')\n",
    "            ax[0].set_ylabel('Similarity (t-stat)')\n",
    "\n",
    "            # plot replication\n",
    "            rupeaks, ruproperties = scs.find_peaks(rep_tstat.T[p], width=5, prominence=repuncorrmint)\n",
    "            ax[1].plot(times, rep_tstat.T[p], color='k')\n",
    "            for i_c, c in enumerate(ruproperties['prominences']):\n",
    "                ax[1].axvspan(ruproperties['left_ips'][i_c]*TR, ruproperties['right_ips'][i_c]*TR,\n",
    "                            color='r', alpha=0.3)\n",
    "            rcpeaks, rcproperties = scs.find_peaks(rep_tstat.T[p], width=5, prominence=repbonnmint)\n",
    "            for i_c, c in enumerate(rcproperties['prominences']):\n",
    "                ax[1].axvspan(rcproperties['left_ips'][i_c]*TR, rcproperties['right_ips'][i_c]*TR,\n",
    "                            color='b', alpha=0.3)\n",
    "            ax[1].set_xlim([0,video_dur])\n",
    "            ax[1].set_title(parcel + ': Replication', weight='bold')\n",
    "            ax[1].set_ylabel('Similarity (t-stat)')\n",
    "            ax[1].set_xlabel('Time (s)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(outprefix + 'peak_similarity_{0}.svg'.format(parcel_labels[p]))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            # save results\n",
    "            dcproperties = {k:v.tolist() for k,v in dcproperties.items()}\n",
    "            duproperties = {k:v.tolist() for k,v in duproperties.items()}\n",
    "            rcproperties = {k:v.tolist() for k,v in rcproperties.items()}\n",
    "            ruproperties = {k:v.tolist() for k,v in ruproperties.items()}\n",
    "            if (len(ruproperties['left_ips'])>0) & (len(duproperties['left_ips'])>0):\n",
    "                results[parcel] = {'Discovery': {'corr': dcproperties, 'uncorr': duproperties}, \n",
    "                                   'Replication': {'corr': rcproperties, 'uncorr': ruproperties}}\n",
    "            \n",
    "            \n",
    "    ### Characterize differences in ratings for high versus low similarity points in the video ###\n",
    "    \n",
    "    # load ratings and shift forward 6 TRs\n",
    "    ratings = pd.read_csv(ratings_file, index_col=0)\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "    ratings.loc[:,ratings.columns] = MinMaxScaler().fit_transform(ratings.to_numpy())\n",
    "    ratings.index = range(6,int(video_dur/TR)+6)\n",
    "    ratings = pd.concat([ratings, pd.DataFrame(np.nan, index=np.arange(0,6,1), columns=ratings.columns)])\n",
    "    ratings = ratings.sort_index()\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "    \n",
    "    # characterize areas within and outside the peaks\n",
    "    for parcel in results.keys():\n",
    "        discmask = np.zeros(times.shape)\n",
    "        for i, c in enumerate(results[parcel]['Discovery']['uncorr']['prominences']):\n",
    "            start = round(results[parcel]['Discovery']['uncorr']['left_ips'][i])\n",
    "            end = round(results[parcel]['Discovery']['uncorr']['right_ips'][i])\n",
    "            discmask[start:end] = 1\n",
    "\n",
    "        repmask = np.zeros(times.shape)\n",
    "        for i, c in enumerate(results[parcel]['Replication']['uncorr']['prominences']):\n",
    "            start = round(results[parcel]['Replication']['uncorr']['left_ips'][i])\n",
    "            end = round(results[parcel]['Replication']['uncorr']['right_ips'][i])\n",
    "            repmask[start:end] = 1\n",
    "\n",
    "        tmask = np.zeros(times.shape).astype(int)\n",
    "        tmask[(discmask==1) & (repmask==1)] = 1\n",
    "\n",
    "        ratesigdiff = pd.Series(index=ratings.columns, name='sig', dtype=int)\n",
    "        ratemeans = dict()\n",
    "        ratestats = dict()\n",
    "        if tmask.max()==1:\n",
    "            for measure in ratings.columns:\n",
    "                underpeak = ratings.loc[tmask==1, measure]\n",
    "                outofpeak = ratings.loc[tmask==0, measure]\n",
    "                underpeakmean = np.nanmean(underpeak)\n",
    "                outofpeakmean = np.nanmean(outofpeak)\n",
    "                rate_stat, rate_pval = scp.ttest_ind(underpeak, outofpeak, nan_policy='omit')\n",
    "                ratestats[measure] = {'tstat': rate_stat, 'pval': rate_pval}\n",
    "                ratesigdiff[measure] = rate_pval < alpha\n",
    "                ratemeans[measure] = {'underpeak': underpeakmean, 'outofpeak': outofpeakmean}\n",
    "\n",
    "            results[parcel]['RatingsAnalysis'] = {'MeanRatings': ratemeans, \n",
    "                                                  'Stats': ratestats}\n",
    "\n",
    "            # plot the differences\n",
    "            sigratingsnames = ratesigdiff[ratesigdiff==True].index.to_list()\n",
    "\n",
    "            if len(sigratingsnames) < 4:\n",
    "                ratings_withpeakinfo = ratings\n",
    "                ratings_withpeakinfo['UnderPeak'] = tmask\n",
    "                temp = pd.melt(ratings_withpeakinfo, id_vars='UnderPeak', value_vars=sigratingsnames, value_name='Level',var_name='Rating')\n",
    "                plt.figure(figsize=(2+2*len(sigratingsnames),6))\n",
    "                sns.barplot(x='Rating', y='Level', hue='UnderPeak', data=temp, palette=['#FFFFFF','#BCB1C2'] , linewidth=2, edgecolor='k')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(outprefix + 'peak_ratings_sigdiff_{0}.svg'.format(parcel))\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "            else:\n",
    "                underpeak = ratings.loc[tmask==1, sigratingsnames].mean(axis=0).to_frame()\n",
    "                underpeak.columns = ['mean']\n",
    "                underpeak['width']=0.4\n",
    "                underpeak['measure'] = underpeak.index\n",
    "                outofpeak = ratings.loc[tmask==0, sigratingsnames].mean(axis=0).to_frame()\n",
    "                outofpeak.columns = ['mean']\n",
    "                outofpeak['width']=0.7\n",
    "                outofpeak['measure'] = outofpeak.index\n",
    "\n",
    "                fig = go.Figure()\n",
    "\n",
    "                fig.add_trace(go.Barpolar(\n",
    "                    r=outofpeak['mean'],\n",
    "                    theta=outofpeak['measure'],\n",
    "                    width=outofpeak['width'],\n",
    "                    base=0,\n",
    "                    name='OutOfPeak',\n",
    "                    marker_color='#FFFFFF',\n",
    "                    marker_line_color='black',\n",
    "                    marker_line_width=2,\n",
    "                    opacity=1,\n",
    "                ))\n",
    "\n",
    "                fig.add_trace(go.Barpolar(\n",
    "                      r=underpeak['mean'],\n",
    "                      theta=underpeak['measure'],\n",
    "                    width=underpeak['width'],\n",
    "                      name='UnderPeak',\n",
    "                    base=0,\n",
    "                    marker_color='#BAB3BF',\n",
    "                    marker_line_color='black',\n",
    "                    marker_line_width=2,\n",
    "                    opacity=1,\n",
    "                ))\n",
    "\n",
    "                fig.update_layout(\n",
    "                    template='plotly_white',\n",
    "                  polar=dict(\n",
    "                      angularaxis_tickfont_size = 14,\n",
    "                    radialaxis=dict(\n",
    "                      visible=True,\n",
    "                      range=[0, 1]\n",
    "                    )),\n",
    "                  showlegend=False\n",
    "                )\n",
    "\n",
    "                fig.write_image(outprefix + 'polar_ratings_sigdiff_{0}.svg'.format(parcel))\n",
    "\n",
    "        \n",
    "    #save results\n",
    "    with open(outprefix + 'full_results_corrp{0}_prom{1}_width{2}.json'.format(round(bon_alpha,3), round(discbonnmint,2), 5), 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "    return(results)\n",
    "\n",
    "\n",
    "def synchrony_discrep_fdr_netlevel(disc_mean_sim, disc_null, rep_mean_sim, outprefix, video_dur, alpha=0.05, \n",
    "                                   TR=TR, parcel_labels=parcel_labels, atlas_file=atlas_file):\n",
    "    # assign thresholds\n",
    "    bon_alpha = np.sqrt(0.05/disc_mean_sim.shape[1])\n",
    "\n",
    "    n = int(round((len(disc_null)+1)*bon_alpha,0))\n",
    "    discbonmin = disc_null[len(disc_null)-n]\n",
    "    n = int(round((len(rep_null)+1)*bon_alpha,0))\n",
    "    repbonmin = rep_null[len(rep_null)-n]\n",
    "    if discbonmin < np.percentile(disc_mean_sim, 1-alpha):\n",
    "        discbonmin = np.percentile(disc_mean_sim, 1-alpha)\n",
    "        repbonmin = np.percentile(rep_mean_sim, 1-alpha)\n",
    "    print(\"Disc sig threshold: {0}\".format(discbonmin))\n",
    "    print(\"Rep sig threshold: {0}\".format(repbonmin))\n",
    "\n",
    "    # find sig rs that replicate\n",
    "    disc_sig = (disc_mean_sim > discbonmin).astype(int)\n",
    "    rep_sig = (rep_mean_sim > repbonmin).astype(int)\n",
    "\n",
    "    sigrs = np.zeros(disc_sig.shape).astype(int)\n",
    "    sigrs[(disc_sig==1) & (rep_sig==1)] = 1\n",
    "    \n",
    "    # find parcels with at least 5 consecutive seconds of sig synchrony\n",
    "    sig_parcels = np.zeros(sigrs.shape[1]).astype(int)\n",
    "    sigrs_df = pd.DataFrame(sigrs, columns=parcel_labels)\n",
    "    sigrs_df['time'] = range(0, len(sigrs_df))\n",
    "    for i, a in enumerate(parcel_labels):\n",
    "        sigrs_df['segment'] = (sigrs_df[a].diff(1) != 0).astype(int).cumsum()\n",
    "        dur = pd.DataFrame({'dur': sigrs_df.groupby('segment').time.last() - sigrs_df.groupby('segment').time.first(),\n",
    "                            'value': sigrs_df.groupby('segment')[a].mean()}).reset_index(drop=True)\n",
    "        dur = dur.loc[dur['value']==1]\n",
    "        if dur['dur'].max() > 6:\n",
    "            sig_parcels[i] = 1\n",
    "\n",
    "    # average synchrony within networks and plot\n",
    "    masked_disc = disc_mean_sim\n",
    "    masked_disc[:,sig_parcels==0]=np.nan\n",
    "    disc_sync_df = pd.DataFrame(disc_mean_sim, columns=network_labels)\n",
    "    disc_sync_df = disc_sync_df.groupby(by=network_labels, axis=1).mean().dropna(axis=1)\n",
    "\n",
    "    masked_rep = rep_mean_sim\n",
    "    masked_rep[:,sig_parcels==0]=np.nan\n",
    "    rep_sync_df = pd.DataFrame(rep_mean_sim, columns=network_labels)\n",
    "    rep_sync_df = rep_sync_df.groupby(by=network_labels, axis=1).mean().dropna(axis=1)\n",
    "\n",
    "    # make figs for sig parcels sorted by network/region group\n",
    "    color = (142/255, 50/255, 209/255, 1)\n",
    "\n",
    "    ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "    data = nib.load(atlas_file).get_fdata()\n",
    "    ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "    newmap=dict()\n",
    "    newmap[0] = ax0[0][1][0]\n",
    "    for net in disc_sync_df:\n",
    "        parcels_keep = parcel_labels[(network_labels==net) & (sig_parcels==1)]\n",
    "        for a in range(1,len(parcel_labels) +1):\n",
    "            if parcel_labels[a-1] in parcels_keep:\n",
    "                newmap[a] = (parcel_labels[a-1], color)\n",
    "            else:\n",
    "                newmap[a] = (parcel_labels[a-1], (1,1,1,0))\n",
    "        ax0.label[0] = newmap\n",
    "        img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "        nib.save(img, outprefix + 'sig_averaged_parcels_{0}.dlabel.nii'.format(net))\n",
    "        \n",
    "    # Plot and analysis replicating peaks\n",
    "    times = np.arange(0, video_dur, TR)\n",
    "\n",
    "    results = dict()\n",
    "\n",
    "    ### Make plots ###\n",
    "    for net in disc_sync_df.columns:\n",
    "        fig, ax = plt.subplots(2,1,figsize=(12,6), sharex=True, sharey=True)\n",
    "        # plot discovery\n",
    "        ax[0].plot(times, disc_sync_df[net], color='k')\n",
    "        dcpeaks, dcproperties = scs.find_peaks(disc_sync_df[net], width=5, height=discbonmin)\n",
    "        for i_c, c in enumerate(dcproperties['prominences']):\n",
    "            ax[0].axvspan(dcproperties['left_ips'][i_c]*TR, dcproperties['right_ips'][i_c]*TR,\n",
    "                        color='#BAB3BF', alpha=0.9)\n",
    "        ax[0].set_xlim([0,video_dur])\n",
    "        ax[0].set_title(net + ': Discovery', weight='bold')\n",
    "        ax[0].set_ylabel('Synchrony')\n",
    "\n",
    "        # plot replication\n",
    "        ax[1].plot(times, rep_sync_df[net], color='k')\n",
    "        rcpeaks, rcproperties = scs.find_peaks(rep_sync_df[net], width=5, height=repbonmin)\n",
    "        for i_c, c in enumerate(rcproperties['prominences']):\n",
    "            ax[1].axvspan(rcproperties['left_ips'][i_c]*TR, rcproperties['right_ips'][i_c]*TR,\n",
    "                        color='#BAB3BF', alpha=0.9)\n",
    "        ax[1].set_xlim([0,video_dur])\n",
    "        ax[1].set_title(net + ': Replication', weight='bold')\n",
    "        ax[1].set_ylabel('Synchrony')\n",
    "        ax[1].set_xlabel('Time (s)')\n",
    "        plt.tight_layout()\n",
    "        #plt.show()\n",
    "        plt.savefig(outprefix + 'peak_similarity_{0}.svg'.format(net))\n",
    "        plt.close()\n",
    "\n",
    "        # save results\n",
    "        dcproperties = {k:v.tolist() for k,v in dcproperties.items()}\n",
    "        rcproperties = {k:v.tolist() for k,v in rcproperties.items()}\n",
    "        if (len(rcproperties['left_ips'])>0) & (len(dcproperties['left_ips'])>0):\n",
    "            results[net] = {'Discovery': {'corr': dcproperties},\n",
    "                               'Replication': {'corr': rcproperties}}\n",
    "\n",
    "\n",
    "    ### Characterize differences in ratings for high versus low similarity points in the video ###\n",
    "\n",
    "    # load ratings and shift forward 6 TRs (4.8 seconds)\n",
    "    ratings = pd.read_csv(ratings_file, index_col=0)\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "    ratings.loc[:,ratings.columns] = MinMaxScaler().fit_transform(ratings.to_numpy())\n",
    "    ratings.index = range(6,int(video_dur/TR)+6)\n",
    "    ratings = pd.concat([ratings, pd.DataFrame(np.nan, index=np.arange(0,6,1), columns=ratings.columns)])\n",
    "    ratings = ratings.sort_index()\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "\n",
    "    # identify areas within and outside the peaks\n",
    "    for parcel in results.keys():\n",
    "        discmask = np.zeros(times.shape)\n",
    "        for i, c in enumerate(results[parcel]['Discovery']['corr']['peak_heights']):\n",
    "            start = round(results[parcel]['Discovery']['corr']['left_ips'][i])\n",
    "            end = round(results[parcel]['Discovery']['corr']['right_ips'][i])\n",
    "            discmask[start:end] = 1\n",
    "\n",
    "        repmask = np.zeros(times.shape)\n",
    "        for i, c in enumerate(results[parcel]['Replication']['corr']['peak_heights']):\n",
    "            start = round(results[parcel]['Replication']['corr']['left_ips'][i])\n",
    "            end = round(results[parcel]['Replication']['corr']['right_ips'][i])\n",
    "            repmask[start:end] = 1\n",
    "\n",
    "        tmask = np.zeros(times.shape).astype(int)\n",
    "        tmask[(discmask==1) & (repmask==1)] = 1\n",
    "\n",
    "        ratesigdiff = pd.Series(index=ratings.columns, name='sig', dtype=int)\n",
    "        ratemeans = dict()\n",
    "        ratestats = dict()\n",
    "        \n",
    "        if tmask.max()==1:\n",
    "            for measure in ratings.columns:\n",
    "                underpeak = ratings.loc[tmask==1, measure]\n",
    "                outofpeak = ratings.loc[tmask==0, measure]\n",
    "                underpeakmean = np.nanmean(underpeak)\n",
    "                outofpeakmean = np.nanmean(outofpeak)\n",
    "                rate_stat, rate_pval = scp.ttest_ind(underpeak, outofpeak, nan_policy='omit')\n",
    "                ratestats[measure] = {'tstat': rate_stat, 'pval': rate_pval}\n",
    "                ratesigdiff[measure] = rate_pval < alpha\n",
    "                ratemeans[measure] = {'underpeak': underpeakmean, 'outofpeak': outofpeakmean}\n",
    "\n",
    "            results[parcel]['RatingsAnalysis'] = {'MeanRatings': ratemeans, \n",
    "                                                  'Stats': ratestats}\n",
    "\n",
    "            # plot the differences\n",
    "            sigratingsnames = ratesigdiff[ratesigdiff==True].index.to_list()\n",
    "            \n",
    "            try:\n",
    "                if len(sigratingsnames) < 4:\n",
    "                    ratings_withpeakinfo = ratings\n",
    "                    ratings_withpeakinfo['UnderPeak'] = tmask\n",
    "                    temp = pd.melt(ratings_withpeakinfo, id_vars='UnderPeak', value_vars=sigratingsnames, value_name='Level',var_name='Rating')\n",
    "                    plt.figure(figsize=(2+2*len(sigratingsnames),6))\n",
    "                    fig = sns.barplot(x='Rating', y='Level', hue='UnderPeak', data=temp, palette=['#FFFFFF','#BCB1C2'] , linewidth=2, edgecolor='k')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(outprefix + 'peak_ratings_sigdiff_{0}.svg'.format(parcel))\n",
    "                    #plt.show()\n",
    "                    plt.close()\n",
    "                    ratings = ratings.drop('UnderPeak', axis=1)\n",
    "                else:\n",
    "                    if 'UnderPeak' in ratings.columns:\n",
    "                        ratings = ratings.drop('UnderPeak', axis=1)\n",
    "                    underpeak = ratings.loc[tmask==1, sigratingsnames].mean(axis=0).to_frame()\n",
    "                    if 'UnderPeak' in underpeak.index:\n",
    "                        underpeak = underpeak.drop('UnderPeak', axis=0)\n",
    "                    underpeak.columns = ['mean']\n",
    "                    underpeak['width']=0.4\n",
    "                    underpeak['measure'] = underpeak.index\n",
    "                    outofpeak = ratings.loc[tmask==0, sigratingsnames].mean(axis=0).to_frame()\n",
    "                    if 'UnderPeak' in outofpeak.columns:\n",
    "                        outofpeak = outofpeak.drop('UnderPeak', axis=0)\n",
    "                    outofpeak.columns = ['mean']\n",
    "                    outofpeak['width']=0.7\n",
    "                    outofpeak['measure'] = outofpeak.index\n",
    "\n",
    "                    fig = go.Figure()\n",
    "\n",
    "                    fig.add_trace(go.Barpolar(\n",
    "                        r=outofpeak['mean'],\n",
    "                        theta=outofpeak['measure'],\n",
    "                        width=outofpeak['width'],\n",
    "                        base=0,\n",
    "                        name='OutOfPeak',\n",
    "                        marker_color='#FFFFFF',\n",
    "                        marker_line_color='black',\n",
    "                        marker_line_width=2,\n",
    "                        opacity=1,\n",
    "                    ))\n",
    "\n",
    "                    fig.add_trace(go.Barpolar(\n",
    "                          r=underpeak['mean'],\n",
    "                          theta=underpeak['measure'],\n",
    "                        width=underpeak['width'],\n",
    "                          name='UnderPeak',\n",
    "                        base=0,\n",
    "                        marker_color='#BAB3BF',\n",
    "                        marker_line_color='black',\n",
    "                        marker_line_width=2,\n",
    "                        opacity=1,\n",
    "                    ))\n",
    "\n",
    "                    fig.update_layout(\n",
    "                        template='plotly_white',\n",
    "                      polar=dict(\n",
    "                          angularaxis_tickfont_size = 14,\n",
    "                        radialaxis=dict(\n",
    "                          visible=True,\n",
    "                          range=[0, 1]\n",
    "                        )),\n",
    "                      showlegend=False\n",
    "                    )\n",
    "\n",
    "                    fig.write_image(outprefix + 'polar_ratings_sigdiff_{0}.svg'.format(parcel))\n",
    "            except:\n",
    "                print('no overlap between discovery and replication')\n",
    "\n",
    "    #save results\n",
    "    with open(outprefix + 'full_results_corrp{0}_prom{1}_width{2}.json'.format(round(bon_alpha,3), round(discbonmin,2), 5), 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "        \n",
    "    return(disc_sync_df, rep_sync_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f97b14-2dd3-4f00-b504-f53b3ebda24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peak_rating_diffs(peak_mask, ratings_file, video_dur, out_file, color='#BAB3BF', TR=TR, alpha=0.05, fdr=True, shift=6):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    peak_mask: numpy ndarray\n",
    "        timeseries mask in the shape of Nsamples, with 1 indicating peak and 0 indiciating nonpeak.\n",
    "    ratings_file: filename\n",
    "        CSV file containing the ratings to use to chracterize peak versus non-peak\n",
    "    video_dur: float\n",
    "        duration in seconds of the movie\n",
    "    out_file: string\n",
    "        Name to save the plot under\n",
    "    TR = float\n",
    "        Repetition Time in seconds\n",
    "    alpha: float\n",
    "        p-value to determine significance for t-tests (peak versus nonpeak)\n",
    "    shift: int\n",
    "        Number of samples to shift the ratings over to account for the delayed hemodynamic response\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results: dict\n",
    "        dictionary with full t-test results across all ratings\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    results={'MeanRatings':dict(), 'Stats':dict(), 'pvals': dict()}\n",
    "\n",
    "    ratings = pd.read_csv(ratings_file, index_col=0)\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "    ratings.loc[:,ratings.columns] = MinMaxScaler().fit_transform(ratings.to_numpy())\n",
    "    ratings.index = range(6,int(video_dur/TR)+6)\n",
    "    ratings = pd.concat([ratings, pd.DataFrame(np.nan, index=np.arange(0,6,1), columns=ratings.columns)])\n",
    "    ratings = ratings.sort_index()\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "\n",
    "    for measure in ratings.columns:\n",
    "        underpeak = ratings.loc[peak_mask==1, measure]\n",
    "        outofpeak = ratings.loc[peak_mask==0, measure]\n",
    "        underpeakmean = np.nanmean(underpeak)\n",
    "        outofpeakmean = np.nanmean(outofpeak)\n",
    "        rate_stat, rate_pval = scp.ttest_ind(underpeak, outofpeak, nan_policy='omit')\n",
    "        results['Stats'][measure] = {'tstat': rate_stat, 'pval': rate_pval}\n",
    "        results['pvals'][measure] = rate_pval\n",
    "        results['MeanRatings'][measure] = {'underpeak': underpeakmean, 'outofpeak': outofpeakmean}\n",
    "\n",
    "    # plot the differences\n",
    "    if not fdr:\n",
    "        sigratingsnames = [k for (k, v) in results['pvals'].items() if v<alpha]\n",
    "    else: \n",
    "        from statsmodels.stats.multitest import multipletests\n",
    "        pvals = list(results['pvals'].values())\n",
    "        meas_names = list(results['pvals'].keys())\n",
    "        sig, q, _, _ = multipletests(pvals, alpha, method='fdr_bh')\n",
    "        sigratingsnames = [m for i,m in enumerate(meas_names) if q[i]<alpha]\n",
    "\n",
    "    if (len(sigratingsnames) < 3) & (len(sigratingsnames) > 0):\n",
    "        ratings_withpeakinfo = ratings\n",
    "        ratings_withpeakinfo['UnderPeak'] = peak_mask\n",
    "        temp = pd.melt(ratings_withpeakinfo, id_vars='UnderPeak', value_vars=sigratingsnames, value_name='Level',var_name='Rating')\n",
    "        plt.figure(figsize=(2+2*len(sigratingsnames),6))\n",
    "        f = sns.barplot(x='Rating', y='Level', hue='UnderPeak', data=temp, palette=['#FFFFFF', color], linewidth=2, edgecolor='k')\n",
    "        f.legend_.remove()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_file)\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        ratings = ratings.drop('UnderPeak', axis=1)\n",
    "    elif len(sigratingsnames) >= 3:\n",
    "        if 'UnderPeak' in ratings.columns:\n",
    "            ratings = ratings.drop('UnderPeak', axis=1)\n",
    "        underpeak = ratings.loc[peak_mask==1, sigratingsnames].mean(axis=0).to_frame()\n",
    "        if 'UnderPeak' in underpeak.index:\n",
    "            underpeak = underpeak.drop('UnderPeak', axis=0)\n",
    "        underpeak.columns = ['mean']\n",
    "        underpeak['width']=0.4\n",
    "        underpeak['measure'] = underpeak.index\n",
    "        outofpeak = ratings.loc[peak_mask==0, sigratingsnames].mean(axis=0).to_frame()\n",
    "        if 'UnderPeak' in outofpeak.columns:\n",
    "            outofpeak = outofpeak.drop('UnderPeak', axis=0)\n",
    "        outofpeak.columns = ['mean']\n",
    "        outofpeak['width']=0.7\n",
    "        outofpeak['measure'] = outofpeak.index\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Barpolar(\n",
    "            r=outofpeak['mean'],\n",
    "            theta=outofpeak['measure'],\n",
    "            width=outofpeak['width'],\n",
    "            base=0,\n",
    "            name='OutOfPeak',\n",
    "            marker_color='#FFFFFF',\n",
    "            marker_line_color='black',\n",
    "            marker_line_width=2,\n",
    "            opacity=1,\n",
    "        ))\n",
    "\n",
    "        fig.add_trace(go.Barpolar(\n",
    "              r=underpeak['mean'],\n",
    "              theta=underpeak['measure'],\n",
    "            width=underpeak['width'],\n",
    "              name='UnderPeak',\n",
    "            base=0,\n",
    "            marker_color=color,\n",
    "            marker_line_color='black',\n",
    "            marker_line_width=2,\n",
    "            opacity=1,\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            template='plotly_white',\n",
    "            font=dict(family='Arial', size=12, color='black'),\n",
    "          polar=dict(\n",
    "              angularaxis_tickfont_size = 24,\n",
    "            radialaxis=dict(\n",
    "              visible=False,\n",
    "              range=[0, 1]\n",
    "            )),\n",
    "          showlegend=False\n",
    "        )\n",
    "\n",
    "        fig.write_image(out_file)\n",
    "    else:\n",
    "        print('No differences between peak and nonpeak ratings')\n",
    "        \n",
    "    return(results)\n",
    "\n",
    "\n",
    "def match_peak_to_clips(peaktimes, shift, video_file, outfolder, ratings_file, bool_list=None, dim_list=None):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    peaktimes: DataFrame\n",
    "        DataFrame containing and index of unique values (will be how clips are named) and a 'start' and 'end' column \n",
    "        with times in seconds.\n",
    "    shift: float\n",
    "        Time in seconds to shift the peak onset/offset by (is subtracted from each)\n",
    "    video_file: str\n",
    "        the moviepy compatible video file to pull clips from\n",
    "    outfolder: str\n",
    "        the folder path to save the clips under\n",
    "    ratings_file: str\n",
    "        CSV containing a pandas dataframe with an index of time in seconds and columns of video features to plot\n",
    "    '''\n",
    "    \n",
    "    import moviepy.video.io.ffmpeg_tools as mpff\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    features = pd.read_csv(ratings_file, index_col=0)\n",
    "    features.loc[:,:] = MinMaxScaler().fit_transform(features.to_numpy())\n",
    "\n",
    "    for clipnum in peaktimes.index:\n",
    "        outfile = os.path.join(outfolder, 'clip{0}.mp4'.format(clipnum))\n",
    "        start = peaktimes.loc[clipnum,'start'] - shift - 4\n",
    "        end = peaktimes.loc[clipnum,'end'] - shift + 4\n",
    "        if start<0:\n",
    "            start=0\n",
    "        if end>0:\n",
    "            mpff.ffmpeg_extract_subclip(video_file, start, end, outfile)\n",
    "        if not bool_list and not dim_list:\n",
    "            fig, ax = plt.subplots(figsize=(6,4))\n",
    "            features.loc[start:end,:].plot(kind='line', ax=ax)\n",
    "            plt.tight_layout()\n",
    "            sns.despine()\n",
    "            plt.savefig(os.path.join(outfolder, 'clip{0}.svg'.format(clipnum)))\n",
    "            plt.close()\n",
    "        else:\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(8,1.5*len(bool_list)))\n",
    "                ax = features.loc[start:end,bool_list].plot(subplots=True, kind='area', ax=ax, xlim=(start,end), sharex=True, sharey=True)\n",
    "                for a in range(0, len(bool_list)):\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'start'] - shift, color='k', linestyle='-')\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'end'] - shift, color='k', linestyle='-')\n",
    "                sns.despine()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(outfolder, 'clip{0}_boolvarsleg.svg'.format(clipnum)))\n",
    "                plt.close()\n",
    "                fig, ax = plt.subplots(figsize=(8,1.5*len(bool_list)))\n",
    "                ax = features.loc[start:end,bool_list].plot(subplots=True, kind='area', ax=ax, legend=False, xlim=(start,end), sharex=True, sharey=True)\n",
    "                for a in range(0, len(bool_list)):\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'start'] - shift, color='k', linestyle='-')\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'end'] - shift, color='k', linestyle='-')\n",
    "                sns.despine()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(outfolder, 'clip{0}_boolvars_noleg.svg'.format(clipnum)))\n",
    "                plt.close()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(6,1.5*len(dim_list)))\n",
    "                ax = features.loc[start:end,dim_list].plot(subplots=True, ax=ax, xlim=(start,end), sharex=True)\n",
    "                for a in range(0, len(dim_list)):\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'start'] - shift, color='k', linestyle='-')\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'end'] - shift, color='k', linestyle='-')\n",
    "                sns.despine()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(outfolder, 'clip{0}_dimvarsleg.svg'.format(clipnum)))\n",
    "                plt.close()\n",
    "                fig, ax = plt.subplots(figsize=(6,1.5*len(dim_list)))\n",
    "                ax = features.loc[start:end,dim_list].plot(subplots=True, ax=ax, legend=False, xlim=(start,end), sharex=True)\n",
    "                for a in range(0, len(dim_list)):\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'start'] - shift, color='k', linestyle='-')\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'end'] - shift, color='k', linestyle='-')\n",
    "                sns.despine()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(outfolder, 'clip{0}_dimvars_nleg.svg'.format(clipnum)))\n",
    "                plt.close()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "def plot_network_activation(ts_data, nois, network_labels, time, groups, group_labels, outdir):\n",
    "    \n",
    "    colors = ['b','k','r','g','y']\n",
    "    # convert to percent signal change\n",
    "    ts_psc_data = (ts_data - ts_data.min(axis=0, keepdims=True)) / (ts_data.max(axis=0, keepdims=True) - ts_data.min(axis=0, keepdims=True))\n",
    "    mean_sig = np.mean(ts_psc_data, axis=0, keepdims=True)\n",
    "    ts_psc_data = ((ts_psc_data-mean_sig)/mean_sig)*100\n",
    "\n",
    "    # average across network/region\n",
    "    ts_psc_net_data = np.zeros((ts_psc_data.shape[0], len(nois), ts_psc_data.shape[2]))\n",
    "    for i, n in enumerate(nois):\n",
    "        ts_psc_net_data[:,i,:] = np.mean(ts_psc_data[:,network_labels==n,:], axis=1)\n",
    "\n",
    "    # plot group level traces\n",
    "    fig, ax = plt.subplots(len(nois),1,figsize=(12,2*len(nois)), sharex=True)\n",
    "    for i, g in enumerate(groups):\n",
    "        mean_sig = np.mean(ts_psc_net_data[:,:,group_labels==g], axis=2)\n",
    "        for j, net in enumerate(nois):  \n",
    "            ax[j].plot(time, mean_sig[:,j], color=colors[i], label=g)\n",
    "            ax[j].set_xlim((0,time[-1]))\n",
    "            ax[j].set_title(net)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, 'network_activation_noleg.svg'))\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(outdir, 'network_activation_leg.svg'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870ee24-0e56-433c-9d05-34ada584ec99",
   "metadata": {
    "tags": []
   },
   "source": [
    "## dynamic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6413483-e35c-4bec-a2ef-1a8ed2086fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for age in ['all','older','younger']:\n",
    "    if age=='younger':\n",
    "        clins = ['SCARED_P_SC']\n",
    "    else:\n",
    "        clins = ['SCARED_P_SC', 'SCARED_SR_SC']\n",
    "    \n",
    "    for clin in clins:\n",
    "        if 'SR' in clin:\n",
    "            other = 'MFQ_SR_Total'\n",
    "        else:\n",
    "            other = 'MFQ_P_Total'\n",
    "        for sample in ['rubic','cbic']:\n",
    "            for movie in ['TP','DM']:\n",
    "                if age=='all':\n",
    "                    sampleinfo = subinfo.loc[(subinfo['site']==sample) & (subinfo['movie']==movie) & np.isfinite(subinfo[clin]),:]\n",
    "                    sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other]] = \\\n",
    "                    IterativeImputer(random_state=42).fit_transform(sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other]])\n",
    "                    res = smf.ols('{0} ~ age + female + meanFD + {1}'.format(clin, other), data=sampleinfo).fit()\n",
    "                    sampleinfo[clin] = res.resid.to_frame().iloc[:,0]\n",
    "                else:\n",
    "                    sampleinfo = subinfo.loc[(subinfo['age_group']==age) & (subinfo['site']==sample) & (subinfo['movie']==movie) & np.isfinite(subinfo[clin]),:]\n",
    "                    sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other]] = \\\n",
    "                    IterativeImputer(random_state=42).fit_transform(sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other]])\n",
    "                    res = smf.ols('{0} ~ female + meanFD + {1}'.format(clin, other), data=sampleinfo).fit()\n",
    "                    sampleinfo[clin] = res.resid.to_frame().iloc[:,0]\n",
    "\n",
    "                print(age, clin, movie, sample)\n",
    "                bigdata_outfolder = os.path.join(big_data_dir, 'agegroup_similarity_regagesxs', age, \n",
    "                                      'dynamic_movie{0}_{1}'.format(movie, clin), 'peak_analysis_20')\n",
    "                os.makedirs(bigdata_outfolder, exist_ok=True)\n",
    "                out_folder = os.path.join(out_dir, 'agegroup_similarity_regagesxs', age, \n",
    "                                      'dynamic_movie{0}_{1}'.format(movie, clin), 'peak_analysis_20')\n",
    "                os.makedirs(out_folder, exist_ok=True)\n",
    "                group_data_file = os.path.join(out_folder, 'compiled_timeseries_data_{0}_movie{1}.npy'.format(sample, movie))\n",
    "                if os.path.isfile(group_data_file):\n",
    "                    group_data = np.load(group_data_file)\n",
    "                else:\n",
    "                    group_data = compile_ts_data(sampleinfo, movie, data_dir, group_data_file)\n",
    "\n",
    "                # compute isps for upper and lower 20% scoring children\n",
    "                top = (sampleinfo[clin]>=np.percentile(sampleinfo[clin], 80)).astype(int)\n",
    "                bottom = (sampleinfo[clin]<=np.percentile(sampleinfo[clin], 20)).astype(int)\n",
    "\n",
    "                phase_file = os.path.join(out_folder, 'upper_phase_data_{0}_movie{1}.npy'.format(sample, movie))\n",
    "                if not os.path.isfile(phase_file):\n",
    "                    phase_data = compute_group_phase(group_data[:,:, top==1], phase_file)\n",
    "                else:\n",
    "                    phase_data = np.load(phase_file)\n",
    "                top_activation = group_data[:,:, top==1]\n",
    "                np.save(os.path.join(out_folder, 'upper_signal_data_{0}_movie{1}.npy'.format(sample, movie)), top_activation)\n",
    "\n",
    "                outprefix = os.path.join(bigdata_outfolder, 'upper_movie{0}_{1}_'.format(movie, sample))\n",
    "                if not os.path.isfile(outprefix + 'isps_data.dat.npy'):\n",
    "                    _, _ = compute_isps(phase_data, outprefix, small=True)\n",
    "\n",
    "                phase_file = os.path.join(out_folder, 'lower_phase_data_{0}_movie{1}.npy'.format(sample, movie))\n",
    "                if not os.path.isfile(phase_file):\n",
    "                    phase_data = compute_group_phase(group_data[:,:, bottom==1], phase_file)\n",
    "                else:\n",
    "                    phase_data = np.load(phase_file)\n",
    "                bottom_activation = group_data[:,:, bottom==1]\n",
    "                np.save(os.path.join(out_folder, 'lower_signal_data_{0}_movie{1}.npy'.format(sample, movie)), bottom_activation)\n",
    "\n",
    "                outprefix = os.path.join(bigdata_outfolder, 'lower_movie{0}_{1}_'.format(movie, sample))\n",
    "                if not os.path.isfile(outprefix + 'isps_data.dat.npy'):\n",
    "                    _, _ = compute_isps(phase_data, outprefix, small=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5a15c-a369-4831-91f6-ecee07d69369",
   "metadata": {},
   "source": [
    "### what scenes elicit increased global synchrony in low/high symptom children?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d78f3-c486-4345-8c28-bf114ba518cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for age in ['all','younger','older']:\n",
    "    if age=='younger':\n",
    "        clins = ['SCARED_P_SC']\n",
    "    else:\n",
    "        clins = ['SCARED_P_SC', 'SCARED_SR_SC']\n",
    "    \n",
    "    for clin in clins:\n",
    "        for movie in ['TP','DM']:\n",
    "            print(age, clin, movie)\n",
    "            if movie=='DM':\n",
    "                video_file = '/Users/catcamacho/Library/CloudStorage/Box-Box/CCP/HBN_study/HBN_video_coding/Videos/Despicable_Me_1000.mp4'\n",
    "                movie_dur = 600\n",
    "            else:\n",
    "                video_file = '/Users/catcamacho/Library/CloudStorage/Box-Box/CCP/HBN_study/HBN_video_coding/Videos/The_Present_0321.mp4'\n",
    "                movie_dur = 200\n",
    "\n",
    "            both_data = {'color':'gray','parcel_info':{}}\n",
    "            time = np.arange(0,movie_dur, TR)\n",
    "            print(time.shape)\n",
    "\n",
    "            ratings_file = os.path.join(project_dir, 'HBN_video_coding/processing/summary/{0}_summary_codes_intuitivenames.csv'.format(movie))\n",
    "            sig_parcs_file = os.path.join(out_dir, 'agegroup_similarity_regagesxs', age, \n",
    "                                          'ts_isc_movie{0}_{1}'.format(movie, clin), 'top_model_fits_replicable.dlabel.nii')\n",
    "            sig_models = nib.load(sig_parcs_file).header.get_axis(0).label[0]\n",
    "            model_labels = []\n",
    "            mapping = [model_labels.append(a) for (a,b) in sig_models.values()]\n",
    "            model_labels = model_labels[1:]\n",
    "            model_labels = [a.split('_')[0] for a in model_labels]\n",
    "            model_labels = np.array(model_labels)\n",
    "\n",
    "            out_folder = os.path.join(out_dir, 'agegroup_similarity_regagesxs', age, \n",
    "                                      'dynamic_movie{0}_{1}'.format(movie, clin), 'peak_analysis_20')\n",
    "            bigdata_outfolder = os.path.join(big_data_dir, 'agegroup_similarity_regagesxs', age, \n",
    "                                             'dynamic_movie{0}_{1}'.format(movie, clin), 'peak_analysis_20')\n",
    "\n",
    "            for rel in ['lower','upper']:\n",
    "                rel_folder = os.path.join(out_folder, 'global_comparison_ref-{0}'.format(rel))\n",
    "                os.makedirs(rel_folder, exist_ok=True)\n",
    "                if rel=='upper':\n",
    "                    sigparcs = parcel_labels[model_labels=='AnnaKmin']\n",
    "                    print(sigparcs)\n",
    "                    siglabel = 'AnnaKmin'\n",
    "                    disc_ref_isps = np.load(os.path.join(bigdata_outfolder, 'upper_movie{0}_rubic_isps_data.dat.npy'.format(movie)))\n",
    "                    mask = np.tri(disc_ref_isps.shape[2], disc_ref_isps.shape[2], -1, dtype=int)\n",
    "                    disc_ref_isps = disc_ref_isps[:,mask==1,:]\n",
    "                    print(disc_ref_isps.shape)\n",
    "                    disc_ref_signal = np.load(os.path.join(out_folder, 'upper_signal_data_rubic_movie{0}.npy'.format(movie)))\n",
    "\n",
    "                    rep_ref_isps = np.load(os.path.join(bigdata_outfolder, 'upper_movie{0}_cbic_isps_data.dat.npy'.format(movie)))\n",
    "                    mask = np.tri(rep_ref_isps.shape[2], rep_ref_isps.shape[2], -1, dtype=int)\n",
    "                    rep_ref_isps = rep_ref_isps[:,mask==1,:]\n",
    "                    rep_ref_signal = np.load(os.path.join(out_folder, 'upper_signal_data_cbic_movie{0}.npy'.format(movie)))\n",
    "                    print(rep_ref_isps.shape)\n",
    "                    \n",
    "                    disc_comp_isps = np.load(os.path.join(bigdata_outfolder, 'lower_movie{0}_rubic_isps_data.dat.npy'.format(movie)))\n",
    "                    mask = np.tri(disc_comp_isps.shape[2], disc_comp_isps.shape[2], -1, dtype=int)\n",
    "                    disc_comp_isps = disc_comp_isps[:,mask==1,:]\n",
    "                    disc_comp_signal = np.load(os.path.join(out_folder, 'lower_signal_data_rubic_movie{0}.npy'.format(movie)))\n",
    "                    print(disc_comp_isps.shape)\n",
    "\n",
    "                    rep_comp_isps = np.load(os.path.join(bigdata_outfolder, 'lower_movie{0}_cbic_isps_data.dat.npy'.format(movie)))\n",
    "                    mask = np.tri(rep_comp_isps.shape[2], rep_comp_isps.shape[2], -1, dtype=int)\n",
    "                    rep_comp_isps = rep_comp_isps[:,mask==1,:]\n",
    "                    rep_comp_signal = np.load(os.path.join(out_folder, 'lower_signal_data_cbic_movie{0}.npy'.format(movie)))\n",
    "                    print(rep_comp_isps.shape)\n",
    "\n",
    "                elif rel=='lower':\n",
    "                    sigparcs = parcel_labels[model_labels=='AnnaKmaxminmax']\n",
    "                    print(sigparcs)\n",
    "                    siglabel = 'AnnaKmaxminmax'\n",
    "                    disc_comp_isps = np.load(os.path.join(bigdata_outfolder, 'upper_movie{0}_rubic_isps_data.dat.npy'.format(movie)))\n",
    "                    mask = np.tri(disc_comp_isps.shape[2], disc_comp_isps.shape[2], -1, dtype=int)\n",
    "                    disc_comp_isps = disc_comp_isps[:,mask==1,:]\n",
    "                    disc_comp_signal = np.load(os.path.join(out_folder, 'upper_signal_data_rubic_movie{0}.npy'.format(movie)))\n",
    "\n",
    "                    rep_comp_isps = np.load(os.path.join(bigdata_outfolder, 'upper_movie{0}_cbic_isps_data.dat.npy'.format(movie)))\n",
    "                    mask = np.tri(rep_comp_isps.shape[2], rep_comp_isps.shape[2], -1, dtype=int)\n",
    "                    rep_comp_isps = rep_comp_isps[:,mask==1,:]\n",
    "                    rep_comp_signal = np.load(os.path.join(out_folder, 'upper_signal_data_cbic_movie{0}.npy'.format(movie)))\n",
    "\n",
    "                    disc_ref_isps = np.load(os.path.join(bigdata_outfolder, 'lower_movie{0}_rubic_isps_data.dat.npy'.format(movie)))\n",
    "                    mask = np.tri(disc_ref_isps.shape[2], disc_ref_isps.shape[2], -1, dtype=int)\n",
    "                    disc_ref_isps = disc_ref_isps[:,mask==1,:]\n",
    "                    disc_ref_signal = np.load(os.path.join(out_folder, 'lower_signal_data_rubic_movie{0}.npy'.format(movie)))\n",
    "\n",
    "                    rep_ref_isps = np.load(os.path.join(bigdata_outfolder, 'lower_movie{0}_cbic_isps_data.dat.npy'.format(movie)))\n",
    "                    mask = np.tri(rep_ref_isps.shape[2], rep_ref_isps.shape[2], -1, dtype=int)\n",
    "                    rep_ref_isps = rep_ref_isps[:,mask==1,:]\n",
    "                    rep_ref_signal = np.load(os.path.join(out_folder, 'lower_signal_data_rubic_movie{0}.npy'.format(movie)))\n",
    "\n",
    "                if len(sigparcs)==0:\n",
    "                    print('no sig parcels found', clin, movie, rel)\n",
    "                    print(sigparcs)\n",
    "                    break\n",
    "\n",
    "                sig_rho = nib.load(os.path.join(out_dir, 'agegroup_similarity_regagesxs', age, 'ts_isc_movie{0}_{1}'.format(movie,clin), \n",
    "                                                'movie{0}_isc_{1}_{2}_maskedrho_fdr0.01127.pscalar.nii'.format(movie, clin, siglabel))).get_fdata()\n",
    "                sig_rho_parcels = parcel_labels[np.isfinite(np.squeeze(sig_rho))]\n",
    "\n",
    "                disc_ref_isps = np.mean(disc_ref_isps[model_labels==siglabel,:,:], axis=0)\n",
    "                rep_ref_isps = np.mean(rep_ref_isps[model_labels==siglabel,:,:], axis=0)\n",
    "                disc_comp_isps = np.mean(disc_comp_isps[model_labels==siglabel,:,:], axis=0)\n",
    "                rep_comp_isps = np.mean(rep_comp_isps[model_labels==siglabel,:,:], axis=0)\n",
    "\n",
    "                peaks = pd.DataFrame(0,index=range(0,len(time)), columns=['disc','rep'])\n",
    "                peaks['time'] = time\n",
    "                \n",
    "                # Create permuted T distribution\n",
    "                perm_dist_file = os.path.join(bigdata_outfolder, 'perm_tdist.npy')\n",
    "                if not os.path.isfile(perm_dist_file):\n",
    "                    null_dist = np.empty(10000)\n",
    "                    n_ref = disc_ref_isps.shape[0]\n",
    "                    perm_data = np.concatenate([disc_ref_isps,disc_comp_isps], axis=0)\n",
    "                    perm_data_shape = perm_data.shape\n",
    "                    for i in range(0,10000):\n",
    "                        perm_data = perm_data.flatten()\n",
    "                        np.random.shuffle(perm_data)\n",
    "                        perm_data = np.reshape(perm_data, perm_data_shape)\n",
    "                        null_dist[i], _ = scp.ttest_ind(np.squeeze(perm_data[:n_ref,0]), np.squeeze(perm_data[n_ref:,0]), axis=None, \n",
    "                                                        alternative='greater', equal_var=False)\n",
    "                    np.save(perm_dist_file, null_dist)\n",
    "                else:\n",
    "                    null_dist = np.load(perm_dist_file)\n",
    "\n",
    "                # rolling t-test for discovery sample\n",
    "                for t in range(0, disc_ref_isps.shape[1]):\n",
    "                    tstat, _ = scp.ttest_ind(np.squeeze(disc_ref_isps[:,t]), np.squeeze(disc_comp_isps[:,t]), axis=None, \n",
    "                                             alternative='greater', equal_var=False)\n",
    "                    peaks.loc[t,'disc_tstat'] = tstat\n",
    "                    p = (np.sum((null_dist>=tstat).astype(int)) + 1) / 10001\n",
    "\n",
    "                    if p<0.05:\n",
    "                        peaks.loc[t,'disc'] = 1\n",
    "                    else:\n",
    "                        peaks.loc[t,'disc'] = 0\n",
    "\n",
    "                # rolling t-test for replication sample\n",
    "                for t in range(0, disc_ref_isps.shape[1]):\n",
    "                    tstat, _ = scp.ttest_ind(np.squeeze(rep_ref_isps[:,t]), np.squeeze(rep_comp_isps[:,t]), axis=None, \n",
    "                                             alternative='greater', equal_var=False)\n",
    "                    p = (np.sum((null_dist>=tstat).astype(int)) + 1) / 10001\n",
    "                    peaks.loc[t,'rep_tstat'] = tstat\n",
    "                    if p<0.05:\n",
    "                        peaks.loc[t,'rep'] = 1\n",
    "                    else:\n",
    "                        peaks.loc[t,'rep'] = 0\n",
    "\n",
    "                # organize and clean peaks\n",
    "                both_data['global']={'long_peaks':peaks}\n",
    "                both_data['global']['mean_ref_disc'] = np.mean(disc_ref_isps, axis=0)\n",
    "                both_data['global']['mean_ref_rep'] = np.mean(rep_ref_isps, axis=0)\n",
    "                both_data['global']['mean_comp_disc'] = np.mean(disc_comp_isps, axis=0)\n",
    "                both_data['global']['mean_comp_rep'] = np.mean(rep_comp_isps, axis=0)\n",
    "                both_data['global']['long_peaks'].index = time\n",
    "                peaks['both'] = ((peaks['disc']==1) & (peaks['rep']==1)).astype(int)\n",
    "                peaks['segment'] = (peaks['both'].diff(1) != 0).astype(int).cumsum()\n",
    "                res = pd.DataFrame({'start': peaks.groupby('segment').time.first(),\n",
    "                                    'end': peaks.groupby('segment').time.last(),\n",
    "                                    'dur': peaks.groupby('segment').time.last()-peaks.groupby('segment').time.first(),\n",
    "                                    'disc_mean':peaks.groupby('segment')['disc_tstat'].mean(),\n",
    "                                    'rep_mean':peaks.groupby('segment')['rep_tstat'].mean(),\n",
    "                                    'disc_max':peaks.groupby('segment')['disc_tstat'].max(),\n",
    "                                    'rep_max':peaks.groupby('segment')['rep_tstat'].max(),\n",
    "                                    'value': peaks.groupby('segment')['both'].mean()}).reset_index(drop=True)\n",
    "                peaks_to_delete = res.loc[(res['value'] == 1) & (res['dur']<4), :]\n",
    "                if len(peaks_to_delete)>0:\n",
    "                    for a in peaks_to_delete.index:\n",
    "                        both_data['global']['long_peaks'].loc[peaks_to_delete.loc[a,'start']:peaks_to_delete.loc[a,'end'], 'both'] = 0\n",
    "                peaks = res.loc[(res['value'] == 1) & (res['dur']>=4), :]\n",
    "\n",
    "                # plot discovery and replication\n",
    "                fig, ax = plt.subplots(2,1,figsize=(12,6), sharex=True, sharey=True)\n",
    "                for i_c, c in enumerate(peaks['start']):\n",
    "                    ax[0].axvspan(peaks['start'].to_numpy()[i_c], peaks['end'].to_numpy()[i_c], color='#9A38E7',fill=True, alpha=0.4)\n",
    "                    ax[1].axvspan(peaks['start'].to_numpy()[i_c], peaks['end'].to_numpy()[i_c], color='#9A38E7',fill=True, alpha=0.4)\n",
    "\n",
    "                ax[0].plot(time, np.squeeze(np.mean(disc_comp_isps, axis=0)), color='k')\n",
    "                ax[0].plot(time, np.squeeze(np.mean(disc_ref_isps, axis=0)), color='#9A38E7')\n",
    "                ax[0].set_xlim((0,movie_dur))\n",
    "                ax[0].set_title('Discovery')            \n",
    "                ax[1].plot(time, np.squeeze(np.mean(rep_comp_isps, axis=0)), color='k')\n",
    "                ax[1].plot(time, np.squeeze(np.mean(rep_ref_isps, axis=0)), color='#9A38E7')\n",
    "                ax[1].set_xlim((0,movie_dur))\n",
    "                ax[1].set_title('Replication')\n",
    "\n",
    "                both_data['global']['peaks'] = peaks\n",
    "                plt.suptitle('global')\n",
    "                sns.despine()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(rel_folder, 'bysite_synchrony_{0}_{1}.svg'.format('global', rel)))\n",
    "                plt.close()\n",
    "\n",
    "                # plot discovery and replication tstats\n",
    "                fig, ax = plt.subplots(2,1,figsize=(12,6), sharex=True, sharey=True)\n",
    "                for i_c, c in enumerate(peaks['start']):\n",
    "                    ax[0].axvspan(peaks['start'].to_numpy()[i_c], peaks['end'].to_numpy()[i_c], color=both_data['color'],fill=True, alpha=0.4)\n",
    "                    ax[1].axvspan(peaks['start'].to_numpy()[i_c], peaks['end'].to_numpy()[i_c], color=both_data['color'],fill=True, alpha=0.4)\n",
    "\n",
    "                ax[0].plot(time, both_data['global']['long_peaks']['disc_tstat'], color='k')\n",
    "                ax[0].set_xlim((0,movie_dur))\n",
    "                ax[0].set_title('Discovery')            \n",
    "                ax[1].plot(time, both_data['global']['long_peaks']['rep_tstat'], color='k')\n",
    "                ax[1].set_xlim((0,movie_dur))\n",
    "                ax[1].set_title('Replication')\n",
    "\n",
    "                both_data['global']['peaks'] = peaks\n",
    "                plt.suptitle('global')\n",
    "                sns.despine()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(rel_folder, 'Network_bysite_tstat_{0}_{1}.svg'.format('global', rel)))\n",
    "                plt.close()\n",
    "\n",
    "                # plot just discovery\n",
    "                fig, ax = plt.subplots(1,1,figsize=(16,2.5), sharex=True)\n",
    "                peaks = both_data['global']['peaks']\n",
    "                for i_c, c in enumerate(peaks['start']):\n",
    "                    ax.axvspan(peaks['start'].to_numpy()[i_c], peaks['end'].to_numpy()[i_c], color=both_data['color'],fill=True, alpha=0.4)\n",
    "                    ax.axvspan(peaks['start'].to_numpy()[i_c], peaks['end'].to_numpy()[i_c], color=both_data['color'],fill=True, alpha=0.4)\n",
    "\n",
    "                ax.plot(time, np.squeeze(np.mean(disc_comp_isps, axis=0)), color='k')\n",
    "                ax.plot(time, np.squeeze(np.mean(disc_ref_isps, axis=0)), color='#9A38E7')\n",
    "                ax.set_xlim((0,movie_dur))\n",
    "                ax.set_xlabel('Time (s)')\n",
    "                ax.set_ylabel('Synchrony')\n",
    "                sns.despine()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(rel_folder, 'Networks_all_synchrony_{0}20.svg'.format(rel)))\n",
    "                plt.close()\n",
    "\n",
    "                ### Find differences in peak versus non-peak in terms of video features\n",
    "                peak_mask = both_data['global']['long_peaks']['both'].to_numpy()\n",
    "                ratings_file = '/Users/catcamacho/Library/CloudStorage/Box-Box/CCP/HBN_study/HBN_video_coding/processing/v1/summary/{0}_summary_codes_intuitivenames.csv'.format(movie)\n",
    "                out_file = os.path.join(rel_folder, '{0}_peak_differences_{1}.svg'.format('global',rel))\n",
    "                if (peak_mask.max()==1):\n",
    "                    both_data['global']['peak_quant_analysis'] = find_peak_rating_diffs(peak_mask, ratings_file, movie_dur, out_file, \n",
    "                                                                                       color=both_data['color'], TR=TR, alpha=0.05, fdr=True, shift=6)\n",
    "\n",
    "                ### find which clips are promoting the most activation\n",
    "                peaktimes = both_data['global']['peaks']\n",
    "                peaktimes.loc[:,'meanact'] = peaktimes.loc[:,['disc_mean','rep_mean']].mean(axis=1)\n",
    "                peaktimes.loc[:,'maxact'] = peaktimes.loc[:,['disc_max','rep_max']].mean(axis=1)\n",
    "                peaktimes = peaktimes.sort_values(by='maxact', ascending=False)\n",
    "                peaktimes.index = range(1, len(peaktimes)+1)\n",
    "                both_data['global']['peak_qual_analysis'] = peaktimes\n",
    "                outfolder = os.path.join(rel_folder, 'clips_{0}_{1}'.format('global',rel))\n",
    "                os.makedirs(outfolder, exist_ok=True)\n",
    "                peaktimes.to_csv(os.path.join(outfolder, 'peak_analysis_{0}_{1}.csv'.format('global',rel)))\n",
    "                shift = 5 #in seconds\n",
    "                ratings_file = '/Users/catcamacho/Library/CloudStorage/Box-Box/CCP/HBN_study/HBN_video_coding/processing/v1/summary/{0}_summary_codes10Hz_intuitivenames.csv'.format(movie)\n",
    "                match_peak_to_clips(peaktimes, shift, video_file, outfolder, ratings_file, \n",
    "                                    bool_list=['faces','body','closeup','spoken_words','written_words'],\n",
    "                                    dim_list=['positive','negative','brightness','loudness','saliency'])\n",
    "\n",
    "                ### create activation maps\n",
    "                for a in peaktimes.index.to_list():\n",
    "                    start = int(peaktimes.loc[a,'start']/TR)\n",
    "                    end = int(peaktimes.loc[a, 'end']/TR)\n",
    "                    mean_sig_disc_ref = np.full((1, parcel_labels.shape[0]), np.nan)\n",
    "                    mean_sig_disc_comp = np.full((1, parcel_labels.shape[0]), np.nan)\n",
    "                    mean_sig_rep_ref = np.full((1, parcel_labels.shape[0]), np.nan)\n",
    "                    mean_sig_rep_comp = np.full((1, parcel_labels.shape[0]), np.nan)\n",
    "                    activation_disc_tstat = np.full((1, parcel_labels.shape[0]), np.nan)\n",
    "                    activation_disc_pval = np.full((1, parcel_labels.shape[0]), np.nan)\n",
    "                    activation_rep_tstat = np.full((1, parcel_labels.shape[0]), np.nan)\n",
    "                    activation_rep_pval = np.full((1, parcel_labels.shape[0]), np.nan)\n",
    "\n",
    "                    for parc in sig_rho_parcels:\n",
    "                        parcind = np.where(parcel_labels==parc)[0][0]\n",
    "                        # compute mean activation for each segment\n",
    "                        mean_sig_disc_ref[0, parcind] = np.mean(disc_ref_signal[start:end, parcind,:], axis=None)\n",
    "                        mean_sig_rep_ref[0, parcind] = np.mean(rep_ref_signal[start:end, parcind,:], axis=None)\n",
    "                        mean_sig_disc_comp[0, parcind] = np.mean(disc_comp_signal[start:end, parcind,:], axis=None)\n",
    "                        mean_sig_rep_comp[0, parcind] = np.mean(rep_comp_signal[start:end, parcind,:], axis=None)\n",
    "\n",
    "                        # Compute t-stat for each parcel and each segment\n",
    "                        tstat, pval = scp.ttest_ind(np.squeeze(np.mean(disc_ref_signal[start:end, parcind,:], axis=0)), \n",
    "                                                    np.squeeze(np.mean(disc_comp_signal[start:end, parcind,:], axis=0)), \n",
    "                                                    axis=None, alternative='two-sided')\n",
    "                        activation_disc_tstat[0, parcind] = tstat\n",
    "                        activation_disc_pval[0, parcind] = pval\n",
    "                        del tstat, pval\n",
    "                        tstat, pval = scp.ttest_ind(np.squeeze(np.mean(rep_ref_signal[start:end, parcind,:], axis=0)), \n",
    "                                                    np.squeeze(np.mean(rep_comp_signal[start:end, parcind,:], axis=0)), \n",
    "                                                    axis=None, alternative='two-sided')\n",
    "\n",
    "                        activation_rep_tstat[0, parcind] = tstat\n",
    "                        activation_rep_pval[0, parcind] = pval\n",
    "                        del tstat, pval, parcind\n",
    "\n",
    "                    # save as ciftis\n",
    "                    ciftilabels = ['mean_activation_ref_disc','mean_activation_comp_disc','mean_activation_ref_rep','mean_activation_comp_rep', \n",
    "                                   'activation_disc_tstat', 'activation_disc_pval', 'activation_rep_tstat', 'activation_rep_pval']\n",
    "                    ciftidata = [mean_sig_disc_ref, mean_sig_disc_comp, mean_sig_rep_ref, mean_sig_rep_comp, activation_disc_tstat, \n",
    "                                 activation_disc_pval, activation_rep_tstat, activation_rep_pval]\n",
    "                    for i, c in enumerate(ciftilabels):\n",
    "                        img = nib.cifti2.cifti2.Cifti2Image(ciftidata[i], (ax0, ax1))\n",
    "                        nib.save(img, os.path.join(outfolder, 'clip{0}_{1}.pscalar.nii'.format(a, c)))\n",
    "\n",
    "                ## pickle results\n",
    "                f = open(os.path.join(rel_folder, 'final_peaks_data_{0}.pkl'.format(rel)),'wb')\n",
    "                pickle.dump(both_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d986dd9-d123-49a4-a5f0-f9cca4bea93e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
