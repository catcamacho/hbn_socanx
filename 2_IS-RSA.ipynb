{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56238876-8b1d-40e6-9fb9-649397a4ddae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea5196-0d53-46bb-b413-91f0be64e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import scipy.stats as scp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "import itertools\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import scipy.signal as scs\n",
    "import json\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "from itertools import combinations\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set(context='talk', style='white', font='Arial')\n",
    "\n",
    "today = date.today().strftime('%Y%m%d')\n",
    "\n",
    "project_dir = '/Users/catcamacho/Library/CloudStorage/Box-Box/CCP/HBN_study/'\n",
    "data_dir = project_dir + 'proc/group/parcel_timeseries/sub_ts/'\n",
    "out_dir = project_dir + 'proc/clin/'\n",
    "os.makedirs(out_dir,exist_ok=True)\n",
    "\n",
    "big_data_dir = '/Users/catcamacho/Documents/bigdata/hbn_clin/'\n",
    "\n",
    "sample_file = project_dir + 'proc/group/datasets_info/sample_gord.32k_fs_LR.pscalar.nii'\n",
    "atlas_file = project_dir + 'proc/null_lL_WG33/Gordon333_SeitzmanSubcortical.32k_fs_LR.dlabel.nii'\n",
    "\n",
    "ax0 = nib.load(sample_file).header.get_axis(0)\n",
    "ax1 = nib.load(sample_file).header.get_axis(1)\n",
    "\n",
    "TR = 0.8\n",
    "\n",
    "# get network labels\n",
    "parcel_labels = nib.load(sample_file).header.get_axis(1).name\n",
    "network_labels = []\n",
    "for s in parcel_labels:\n",
    "    b = s.split('_')\n",
    "    if len(b)<2:\n",
    "        network_labels.append(b[0])\n",
    "    else:\n",
    "        network_labels.append(b[1])\n",
    "network_labels = np.array(network_labels)\n",
    "network_names, network_sizes = np.unique(network_labels, return_counts=True)\n",
    "\n",
    "# load subject info\n",
    "subinfo = pd.read_csv(project_dir + 'proc/group/datasets/firstleveldatalabels_withpub_thresh0.8_20220412.csv', index_col=0)\n",
    "subinfo = subinfo.drop(['set','sub','cond'], axis=1)\n",
    "subinfo = subinfo.drop_duplicates()\n",
    "subinfo.index.name='sub'\n",
    "\n",
    "# assign developmental groups\n",
    "subinfo['age_group'] = 'younger'\n",
    "subinfo.loc[(subinfo['age']>10), 'age_group'] = 'older'\n",
    "\n",
    "# load clinical data\n",
    "scaredsr = pd.read_csv(os.path.join(project_dir, 'phenotypic_data','9994_SCARED_SR_20210322.csv'), \n",
    "                       index_col='EID', skiprows=[1]).loc[:,['SCARED_SR_SC','SCARED_SR_GD','SCARED_SR_SP']]\n",
    "scaredsr.index = ['sub-{0}'.format(a) for a in scaredsr.index]\n",
    "scaredsr.index.name = 'sub'\n",
    "\n",
    "scaredpr = pd.read_csv(os.path.join(project_dir, 'phenotypic_data','9994_SCARED_P_20210322.csv'), \n",
    "                       index_col='EID', skiprows=[1]).loc[:,['SCARED_P_SC','SCARED_P_GD','SCARED_P_SP']]\n",
    "scaredpr.index = ['sub-{0}'.format(a) for a in scaredpr.index]\n",
    "scaredpr.index.name = 'sub'\n",
    "\n",
    "mfqsr = pd.read_csv(os.path.join(project_dir, 'phenotypic_data','9994_MFQ_SR_20210322.csv'), index_col='EID', skiprows=[1]).loc[:,'MFQ_SR_Total']\n",
    "mfqsr.index = ['sub-{0}'.format(a) for a in mfqsr.index]\n",
    "mfqsr.index.name = 'sub'\n",
    "\n",
    "mfqpr = pd.read_csv(os.path.join(project_dir, 'phenotypic_data','9994_MFQ_P_20210322.csv'), index_col='EID', skiprows=[1]).loc[:,'MFQ_P_Total']\n",
    "mfqpr.index = ['sub-{0}'.format(a) for a in mfqpr.index]\n",
    "mfqpr.index.name = 'sub'\n",
    "\n",
    "adhd = pd.read_csv(os.path.join(project_dir, 'phenotypic_data','9994_SWAN_20210322.csv'), index_col='EID', skiprows=[1]).loc[:,'SWAN_Avg']\n",
    "adhd.index = ['sub-{0}'.format(a) for a in adhd.index]\n",
    "adhd.index.name = 'sub'\n",
    "\n",
    "clincols = ['DX_{0}_Cat'.format(str(a).zfill(2)) for a in range(1,10)] + ['DX_{0}_Sub'.format(str(a).zfill(2)) for a in range(1,10)]\n",
    "clindx = pd.read_csv(os.path.join(project_dir, 'phenotypic_data','9994_ConsensusDx_20210322_nodupes.csv'), index_col=0).loc[:,clincols]\n",
    "clindx = clindx.fillna('none')\n",
    "clindx['combined_dx'] = clindx.loc[:, clincols].agg(lambda x: ','.join(x.values), axis=1)\n",
    "clindx.index = ['sub-{0}'.format(a) for a in clindx.index]\n",
    "clindx.index.name = 'sub'\n",
    "\n",
    "subinfo = subinfo.merge(adhd, how='left', left_index=True, right_index=True)\n",
    "subinfo = subinfo.merge(mfqsr, how='left', left_index=True, right_index=True)\n",
    "subinfo = subinfo.merge(mfqpr, how='left', left_index=True, right_index=True)\n",
    "subinfo = subinfo.merge(scaredsr, how='left', left_index=True, right_index=True)\n",
    "subinfo = subinfo.merge(scaredpr, how='left', left_index=True, right_index=True)\n",
    "subinfo = subinfo.merge(clindx.loc[:, 'combined_dx'], how='left', left_index=True, right_index=True)\n",
    "subinfo = subinfo.drop_duplicates()\n",
    "\n",
    "# drop anyone missing clinical symptom data\n",
    "subinfo = subinfo.loc[(np.isfinite(subinfo['SCARED_SR_SC']) | np.isfinite(subinfo['SCARED_P_SC'])), :]\n",
    "subinfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b8021-62f1-4357-8a51-8b3ff6f883ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subinfo['no_dx'] = subinfo['combined_dx'].str.contains('No Diagnosis Given,').replace({True: 1, False: 0})\n",
    "subinfo['missing_dx'] = 0\n",
    "subinfo.loc[subinfo['combined_dx'].isnull(),'missing_dx'] = 1\n",
    "subinfo['dep_dx'] = subinfo['combined_dx'].str.contains('Depressive Disorders').replace({True: 1, False: 0})\n",
    "subinfo['anx_dx'] = subinfo['combined_dx'].str.contains('Anxiety Disorders').replace({True: 1, False: 0})\n",
    "subinfo['bip_dx'] = subinfo['combined_dx'].str.contains('Bipolar and Related Disorders').replace({True: 1, False: 0})\n",
    "subinfo['disrup_dx'] = subinfo['combined_dx'].str.contains('Disruptive, Impulse Control and Conduct Disorders').replace({True: 1, False: 0})\n",
    "subinfo['elim_dx'] = subinfo['combined_dx'].str.contains('Elimination Disorders').replace({True: 1, False: 0})\n",
    "subinfo['eat_dx'] = subinfo['combined_dx'].str.contains('Eating Disorders').replace({True: 1, False: 0})\n",
    "subinfo['learn_dx'] = subinfo['combined_dx'].str.contains('Learning Disorder').replace({True: 1, False: 0})\n",
    "subinfo['adhd_dx'] = subinfo['combined_dx'].str.contains('Attention-Deficit/Hyperactivity').replace({True: 1, False: 0})\n",
    "subinfo['asd_dx'] = subinfo['combined_dx'].str.contains('Autism Spectrum Disorder').replace({True: 1, False: 0})\n",
    "subinfo['obsess_dx'] = subinfo['combined_dx'].str.contains('Obsessive Compulsive and Related Disorders').replace({True: 1, False: 0})\n",
    "subinfo['trauma_dx'] = subinfo['combined_dx'].str.contains('Trauma and Stressor Related Disorders').replace({True: 1, False: 0})\n",
    "subinfo.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941edfa-4b95-4aa4-b88e-7b330c8b0a45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a925b1-1ea9-449b-8a86-9cd7269f3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_ts_data(subdf, movie, datadir, outfile):\n",
    "    \"\"\"\n",
    "    combine data for each movie together into 1 file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subdf: DataFrame\n",
    "        A dataframe with subject IDs as the index. Includes IDs for all usable data.\n",
    "    movie: str\n",
    "        Corresponds with the str for the movie content to concatenate (e.g., \"DM\" or \"TP\").\n",
    "    datadir: folder path\n",
    "        Path to folder with the subject timeseries ciftis.\n",
    "    outfile: file path\n",
    "        Path including filename to save the output data of shape Ntimepoints x Nparcels x Nsubjects.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: numpy array\n",
    "        The compiled data of shape Ntimepoints x Nparcels x Nsubjects\n",
    "    \"\"\"\n",
    "    if not isinstance(subdf, pd.DataFrame):\n",
    "        subdf = pd.read_csv(subdf, index_col=0)\n",
    "    \n",
    "    for sub in subdf.index:\n",
    "        file = '{0}{1}_task-movie{2}_bold1_AP_Atlas_rescale_resid0.9_filt_gordonseitzman.32k_fs_LR.ptseries.nii'.format(datadir,sub, movie)\n",
    "        if sub == subdf.index[0]:\n",
    "            data = StandardScaler().fit_transform(nib.load(file).get_fdata())\n",
    "            data = np.expand_dims(data, axis=2)\n",
    "        else:\n",
    "            t = StandardScaler().fit_transform(nib.load(file).get_fdata())\n",
    "            t = np.expand_dims(t, axis=2)\n",
    "            data = np.concatenate([data,t],axis=2)\n",
    "    \n",
    "    print('Compile data from {0} brain regions measured at {1} timepoints from {2} participants.'.format(data.shape[1],data.shape[0],data.shape[2]))\n",
    "    np.save(outfile, data)\n",
    "    return(data)\n",
    "\n",
    "\n",
    "def compute_group_phase(group_ts_data, outfile):\n",
    "    \"\"\"\n",
    "    convert parcel ts to standard units & compute phase angles for each parcel ts\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group_ts_data: filepath OR numpy array\n",
    "        File or numpy array with compiled timeseries data of shape Ntimepoints x Nparcels x Nsubjects \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    group_phase_data: numpy array\n",
    "        The compiled data of shape Ntimepoints x Nparcels x Nsubjects\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(group_ts_data, np.ndarray):\n",
    "        group_ts_data = np.load(group_ts_data)\n",
    "    \n",
    "    group_phase_data = np.zeros_like(group_ts_data)\n",
    "    \n",
    "    for a in range(0,group_ts_data.shape[1]):\n",
    "        for b in range(0,group_ts_data.shape[2]):\n",
    "            group_phase_data[:,a,b] = np.angle(hilbert(group_ts_data[:,a,b]), deg=False)\n",
    "    \n",
    "    np.save(outfile, group_phase_data)\n",
    "    \n",
    "    return(group_phase_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81182bd-96e6-41a5-9c0d-b30169fea334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_isps(group_phase_data, outprefix, savemean=True, small=False):\n",
    "    \"\"\"\n",
    "    parcel-wise inter-subject phase synchrony- output pairwise IPS and mean global IPS\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group_phase_data: numpy array\n",
    "        The compiled data of shape Ntimepoints x Nparcels x Nsubjects\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    isps_data: numpy array\n",
    "        intersubject phase synchrony data of shape Nparcels x Nsubjects x Nsubjects x Ntimepoints\n",
    "    mean_isps_data: numpy array\n",
    "        intersubject phase synchrony data, averaged across time, of shape Nparcels x Nsubjects x Nsubjects\n",
    "        \n",
    "    \"\"\"\n",
    "    if not isinstance(group_phase_data, np.ndarray):\n",
    "        group_phase_data = np.load(group_phase_data)\n",
    "    \n",
    "    if os.path.isdir(outprefix):\n",
    "        file_name = os.path.join(outprefix, 'isps_data.dat')\n",
    "    else:\n",
    "        file_name = outprefix + 'isps_data.dat'\n",
    "        \n",
    "    if not small:\n",
    "        isps_data = np.memmap(file_name, dtype=np.float32, mode='w+',\n",
    "                              shape=(group_phase_data.shape[1],group_phase_data.shape[2],\n",
    "                                     group_phase_data.shape[2],\n",
    "                                     group_phase_data.shape[0]))\n",
    "    else:\n",
    "        isps_data = np.empty((group_phase_data.shape[1],group_phase_data.shape[2],\n",
    "                              group_phase_data.shape[2],group_phase_data.shape[0]))\n",
    "    \n",
    "    subs = range(0, group_phase_data.shape[2])\n",
    "    for region in range(0, group_phase_data.shape[1]):\n",
    "        combs = itertools.combinations(subs, 2)\n",
    "        for c in combs:\n",
    "            sub1 = group_phase_data[:, region, c[0]]\n",
    "            sub2 = group_phase_data[:, region, c[1]]\n",
    "            a = 1 - np.sin(np.abs(sub1 - sub2) / 2)\n",
    "            isps_data[region,c[0],c[1],:] = a\n",
    "            isps_data[region,c[1],c[0],:] = a\n",
    "        \n",
    "    if small:\n",
    "        np.save(file_name, isps_data)\n",
    "    if savemean:\n",
    "        mask = np.tri(isps_data.shape[2], isps_data.shape[2], -1, dtype=int)\n",
    "        mean_isps_data = np.mean(isps_data[:,mask==1,:], axis=1)\n",
    "        if os.path.isdir(outprefix):\n",
    "            mean_file_name = os.path.join(outprefix, 'mean_isps_data.npy')\n",
    "        else:\n",
    "            mean_file_name = outprefix + 'mean_isps_data.npy'\n",
    "        \n",
    "        np.save(mean_file_name, mean_isps_data.T)\n",
    "    \n",
    "        return(mean_isps_data, isps_data)\n",
    "    else:\n",
    "        return(isps_data)\n",
    "\n",
    "\n",
    "def intersubject_timeseries_correlation(data, outprefix, ax0=ax0, ax1=ax1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy array\n",
    "        data in the shape of Ntimepoints x Nregions x Nsubjects\n",
    "    outprefix: str\n",
    "        name to save ISC data to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    intersub_isc: numpy array\n",
    "        intersubject spearman correlations in the shape of Nregions x Nsubjects x Nsubjects\n",
    "    group_isc: numpy array\n",
    "        group mean spearman correlations in the shape of Nregions\n",
    "    \"\"\"\n",
    "    subs = range(0,data.shape[2])\n",
    "    \n",
    "    intersub_isc = np.zeros((data.shape[1],data.shape[2],data.shape[2]))\n",
    "    group_isc = np.zeros((data.shape[1]))\n",
    "    mask = np.tri(data.shape[2], data.shape[2], -1, dtype=int)\n",
    "    \n",
    "    for r in range(0, data.shape[1]):\n",
    "        intersub_isc[r, :, :]= np.corrcoef(data[:, r, :], rowvar=False)\n",
    "            \n",
    "    for r in range(0, data.shape[1]):\n",
    "        group_isc[r] = np.mean(intersub_isc[r,:,:][mask==1])\n",
    "    \n",
    "    np.save(outprefix + 'intersub_timeseries_ISC.npy', intersub_isc)\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(np.expand_dims(group_isc, axis=0), (ax0, ax1))\n",
    "    nib.save(img, outprefix + 'mean_timseries_ISC.pscalar.nii')\n",
    "    \n",
    "    return(intersub_isc, group_isc)\n",
    "\n",
    "\n",
    "def intersubject_distance(data, outfile_prefix):\n",
    "    \"\"\"\n",
    "    Compute static pairwise intersubject similarity\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy array\n",
    "        1D array of subject data (i.e., each participant contributes exactly 1 measure)\n",
    "    outfilename: str\n",
    "        name to save distance data to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    isdistances: numpy array\n",
    "        intersubject distances in the shape of Nsubjects x Nsubjects x Nmetrics\n",
    "    \"\"\"\n",
    "    subs = range(0,data.shape[0])\n",
    "\n",
    "\n",
    "    # NN\n",
    "    nn = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        nn[c[0],c[1]] = np.max(data) - abs(data[c[0]] - data[c[1]])\n",
    "        nn[c[1],c[0]] = np.max(data) - abs(data[c[0]] - data[c[1]])\n",
    "    np.save(outfile_prefix + '_NN.npy', nn)\n",
    "\n",
    "    # AnnaK mean\n",
    "    annakmean = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        annakmean[c[0],c[1]] = (data[c[0]] + data[c[1]]) / 2\n",
    "        annakmean[c[1],c[0]] = (data[c[0]] + data[c[1]]) / 2\n",
    "    np.save(outfile_prefix + '_annakmean.npy', annakmean)\n",
    "    \n",
    "    # AnnaK max min mean\n",
    "    AnnaKmaxminmean = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        AnnaKmaxminmean[c[0],c[1]] = np.max(data) - ((data[c[0]] + data[c[1]]) / 2)\n",
    "        AnnaKmaxminmean[c[1],c[0]] = np.max(data) - ((data[c[0]] + data[c[1]]) / 2)\n",
    "    np.save(outfile_prefix + '_annakmaxminmean.npy', AnnaKmaxminmean)\n",
    "\n",
    "    # AnnaK min\n",
    "    annakmin = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        annakmin[c[0],c[1]] = min([data[c[0]],data[c[1]]])\n",
    "        annakmin[c[1],c[0]] = min([data[c[0]],data[c[1]]])\n",
    "    np.save(outfile_prefix + '_annakmin.npy', annakmin)\n",
    "\n",
    "    # AnnaK max minus min\n",
    "    annakmaxminmax = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        annakmaxminmax[c[0],c[1]] =np.max(data) -  max([data[c[0]],data[c[1]]])\n",
    "        annakmaxminmax[c[1],c[0]] = np.max(data) - max([data[c[0]],data[c[1]]])\n",
    "    np.save(outfile_prefix + '_annakmaxminmax.npy', annakmaxminmax)\n",
    "        \n",
    "    # AnnaK absmean\n",
    "    annakabsmean = np.zeros((data.shape[0],data.shape[0]))\n",
    "    combs = itertools.combinations(subs, 2)\n",
    "    for c in combs:\n",
    "        annakabsmean[c[0],c[1]] = abs(data[c[0]] - data[c[1]]) * ((data[c[0]] + data[c[1]]) / 2)\n",
    "        annakabsmean[c[1],c[0]] = abs(data[c[0]] - data[c[1]]) * ((data[c[0]] + data[c[1]]) / 2)\n",
    "    np.save(outfile_prefix + '_annakabsmean.npy', annakabsmean)\n",
    "    \n",
    "    isdistances = {'NN': nn, \n",
    "                   'AnnaKmean': annakmean, \n",
    "                   'AnnaKmin': annakmin, \n",
    "                   'AnnaKabsmean': annakabsmean, \n",
    "                   'AnnaKmaxminmean': AnnaKmaxminmean, \n",
    "                   'AnnaKmaxminmax': annakmaxminmax}\n",
    "    return(isdistances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8e153-1ebc-45a3-9edd-0d9d8fc4c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_brain_bx_isrsa(brain_sim_data, bx_sim_data, outfilename=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    brain_sim_data: numpy ndarray\n",
    "        Data in the shape of Nsubjects x Nsubjects\n",
    "    bx_sim_data: numpy ndarray\n",
    "        Data in the shape of Nsubjects x Nsubjects\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rsa_report: pandas DataFrame\n",
    "        Pandas DataFrame with inter-subject representational similarity statistics\n",
    "    \"\"\"\n",
    "    rsa_report = pd.DataFrame(columns=['SpearR','SpearPvalue'])\n",
    "    \n",
    "    mask = np.tri(bx_sim_data.shape[0], bx_sim_data.shape[0], -1, dtype=int)\n",
    "    bx_sim = bx_sim_data[mask==1]\n",
    "    brain_sim = brain_sim_data[mask==1]\n",
    "    \n",
    "    r, p = scp.spearmanr(bx_sim, brain_sim)\n",
    "    rsa_report.loc[0,'SpearR'] = r\n",
    "    rsa_report.loc[0,'SpearPvalue'] = p\n",
    "    if outfilename:\n",
    "        sns.scatterplot(bx_sim, brain_sim)\n",
    "        plt.title('Similarity Correlation')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(outfilename)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    return(rsa_report)\n",
    "\n",
    "def regional_perm_bx_isrsa(regional_sim_data, bx_sim_data, outprefix, alpha=0.05, n_perms=1000, ax0=ax0, ax1=ax1):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    regional_sim_data: numpy ndarray\n",
    "        Data in the shape of Nregions x Nsubjects x Nsubjects\n",
    "    bx_sim_data: numpy ndarray\n",
    "        Data in the shape of Nsubjects x Nsubjects\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    region_isrsa: numpy ndarray\n",
    "        Data in the shape of Nregions\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = np.tri(bx_sim_data.shape[1], bx_sim_data.shape[1], -1, dtype=int)\n",
    "\n",
    "    # flatten behavior lower triangle\n",
    "    bx_sim = bx_sim_data[mask==1]\n",
    "\n",
    "    region_isrsa = np.zeros((regional_sim_data.shape[0]))\n",
    "\n",
    "    for region in range(0, regional_sim_data.shape[0]):\n",
    "            brain_sim = regional_sim_data[region,:,:][mask==1]\n",
    "            r, p = scp.spearmanr(bx_sim, brain_sim)\n",
    "            region_isrsa[region] = r\n",
    "\n",
    "    shuff_bx = bx_sim\n",
    "    perm_isrsa_null = np.zeros((n_perms, regional_sim_data.shape[0]))\n",
    "\n",
    "    # make null distributions for each TR and region\n",
    "    for a in range(0,n_perms):\n",
    "        np.random.shuffle(shuff_bx)\n",
    "        for region in range(0,regional_sim_data.shape[0]):\n",
    "            brain_sim = regional_sim_data[region,:,:][mask==1]\n",
    "            r, p = scp.spearmanr(shuff_bx, brain_sim)\n",
    "            perm_isrsa_null[a, region] = r\n",
    "\n",
    "    # compute permuted P threshold per region/TR\n",
    "    raw_pvals = np.zeros(region_isrsa.shape)\n",
    "    flat_null = perm_isrsa_null.flatten()\n",
    "    for i, a in enumerate(region_isrsa):\n",
    "        raw_pvals[i] = (np.sum((flat_null>=a).astype(int)) + 1) / (flat_null.shape[0] + 1)\n",
    "        \n",
    "    # save ciftis with raw values\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(np.expand_dims(raw_pvals, axis=0), (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permsim_raw_pval.pscalar.nii')\n",
    "    \n",
    "    img = nib.cifti2.cifti2.Cifti2Image(np.expand_dims(region_isrsa, axis=0), (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permsim_raw_rho.pscalar.nii')\n",
    "    \n",
    "    \n",
    "    # save cifti with significant rhos only\n",
    "    thresh_mask = raw_pvals<alpha\n",
    "\n",
    "    # pvals\n",
    "    thresh_pval = raw_pvals\n",
    "    thresh_pval[thresh_mask==0] = np.nan\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(np.expand_dims(thresh_pval, axis=0), (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permsim_masked_pval{0}.pscalar.nii'.format(alpha))\n",
    "\n",
    "    # rhos\n",
    "    thresh_isrsa = region_isrsa\n",
    "    thresh_isrsa[thresh_mask==0] = np.nan\n",
    "    thresh_isrsa[thresh_isrsa<0] = np.nan\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(np.expand_dims(thresh_isrsa, axis=0), (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permsim_masked_rho{0}.pscalar.nii'.format(alpha))\n",
    "    return(thresh_isrsa)\n",
    "\n",
    "\n",
    "# compute dynamic similarity\n",
    "def dynamic_brain_bx_isrsa(time_region_sim_data, bx_sim_data, outprefix, ax1=ax1):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    time_region_sim_data: numpy ndarray\n",
    "        Data in the shape of Nregions x Nsubjects x Nsubjects x Ntimepoints\n",
    "    bx_sim_data: numpy ndarray\n",
    "        Data in the shape of Nsubjects x Nsubjects\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dynamic_isrsa: pandas DataFrame\n",
    "        Pandas DataFrame with inter-subject representational similarity statistics\n",
    "    \"\"\"\n",
    "    mask = np.tri(bx_sim_data.shape[1], bx_sim_data.shape[1], -1, dtype=int)\n",
    "\n",
    "    # flatten behavior lower triangle\n",
    "    bx_sim = bx_sim_data[mask==1]\n",
    "\n",
    "    dynamic_isrsa = np.zeros((isps_data.shape[3],isps_data.shape[0]))\n",
    "\n",
    "    for region in range(0, isps_data.shape[0]):\n",
    "        for tr in range(0, isps_data.shape[3]):\n",
    "            brain_sim = isps_data[region,:,:,tr][mask==1]\n",
    "            r, p = scp.spearmanr(bx_sim, brain_sim)\n",
    "            dynamic_isrsa[tr, region] = r\n",
    "            \n",
    "    np.save(outfile_prefix + '_dynamicsim.npy', dynamic_isrsa)\n",
    "    \n",
    "    ax0 = nib.cifti2.cifti2_axes.SeriesAxis(0,0.8,dynamic_isrsa.shape[0], unit='second')\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(dynamic_isrsa, (ax0, ax1))\n",
    "    nib.save(img, outfile_prefix + '_dynamicsim.ptseries.nii')\n",
    "\n",
    "    return(dynamic_isrsa)\n",
    "\n",
    "\n",
    "def perm_sig_dynamic_isrsa(time_region_sim_data, bx_sim_data, outprefix, n_perms=30, ax1=ax1, alpha=0.05, TR=0.8):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dynamic_isrsa = dynamic_brain_bx_isrsa(time_region_sim_data, bx_sim_data, outprefix)\n",
    "    \n",
    "    if os.path.isfile(os.path.join(outprefix + '_perm_isrsa_null.dat')):\n",
    "        perm_isrsa_null = np.memmap(os.path.join(outprefix + '_perm_isrsa_null.dat'), dtype=np.float32, mode='r',\n",
    "                                    shape=(n_perms, isps_data.shape[3], isps_data.shape[0]))\n",
    "    else:\n",
    "        perm_isrsa_null = np.memmap(os.path.join(outprefix + '_perm_isrsa_null.dat'), dtype=np.float32, mode='w+',\n",
    "                                    shape=(n_perms, isps_data.shape[3], isps_data.shape[0]))\n",
    "        shuff_bx = bx_sim_data\n",
    "\n",
    "        # make null distributions for each TR and region\n",
    "        for a in range(0,n_perms):\n",
    "            np.random.shuffle(shuff_bx)\n",
    "            perm_isrsa_null[a, :, :] = dynamic_brain_bx_isrsa(time_region_sim_data, shuff_bx, outprefix)\n",
    "    \n",
    "    # compute permuted P threshold per region/TR\n",
    "    orig_shape = dynamic_isrsa.shape\n",
    "    flat_dyn_isrsa = dynamic_isrsa.flatten()\n",
    "    raw_pvals = np.zeros(flat_dyn_isrsa.shape)\n",
    "    flat_null = perm_isrsa_null.flatten()\n",
    "    for i, a in enumerate(flat_dyn_isrsa):\n",
    "        raw_pvals[i] = (np.sum((flat_null>=a).astype(int)) + 1) / (flat_null.shape[0] + 1)\n",
    "\n",
    "    raw_pvals = np.reshape(raw_pvals, orig_shape)\n",
    "    \n",
    "    # save cifti\n",
    "    ax0 = nib.cifti2.cifti2_axes.SeriesAxis(0,TR,raw_pvals.shape[0], unit='second')\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(dynamic_isrsa, (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permdynamicsim_raw_rho.ptseries.nii')\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(raw_pvals, (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permdynamicsim_raw_pval.ptseries.nii')\n",
    "    \n",
    "    # save cifti with significant rhos only\n",
    "    thresh_mask = raw_pvals<alpha\n",
    "\n",
    "    # pvals\n",
    "    thresh_pval = raw_pvals\n",
    "    thresh_pval[thresh_mask==0] = np.nan\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(thresh_pval, (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permdynamicsim_thresh_pval{0}.ptseries.nii'.format(alpha))\n",
    "    # rhos\n",
    "    thresh_isrsa = dynamic_isrsa\n",
    "    thresh_isrsa[thresh_mask==0] = np.nan\n",
    "    thresh_isrsa[thresh_isrsa<0] = np.nan\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(thresh_isrsa, (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_permdynamicsim_thresh_rho{0}.ptseries.nii'.format(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d77eea7-6ee6-4180-9c49-b459cf427212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_isrsa_fdr(disc_rho, disc_pval, rep_rho, rep_pval, outprefix, alpha=0.05, bon_alpha=True,replace_zeros=True, ax0=ax0, ax1=ax1):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    disc_rho = nib.load(disc_rho).get_fdata()\n",
    "    disc_pval = nib.load(disc_pval).get_fdata()\n",
    "    rep_rho = nib.load(rep_rho).get_fdata()\n",
    "    rep_pval = nib.load(rep_pval).get_fdata()\n",
    "    \n",
    "    if replace_zeros:\n",
    "        disc_pval[disc_pval==0] = np.nan\n",
    "        rep_pval[rep_pval==0] = np.nan\n",
    "    \n",
    "    if bon_alpha==True:\n",
    "        bon_alpha = np.sqrt(alpha/disc_pval.shape[1])\n",
    "    else:\n",
    "        bon_alpha = alpha\n",
    "\n",
    "    dmask = (disc_pval<bon_alpha).astype(int)\n",
    "    rmask = (rep_pval<bon_alpha).astype(int)\n",
    "\n",
    "    mask = np.zeros(dmask.shape)\n",
    "    mask[(dmask==1) & (rmask==1)] = 1\n",
    "\n",
    "    bonrho = np.empty(mask.shape)\n",
    "    bonrho[mask==1] = np.add(disc_rho[mask==1],rep_rho[mask==1])/2\n",
    "    bonrho[mask==0] = np.nan\n",
    "\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(bonrho, (ax0, ax1))\n",
    "    nib.save(img, outprefix + '_maskedrho_fdr{0}.pscalar.nii'.format(round(bon_alpha,5)))\n",
    "    \n",
    "    \n",
    "def dynamic_isrsa_fdr(disc_rho, rep_rho, discN, repN, video_dur, ratings_file, outprefix, TR=TR, alpha=0.05, parcel_labels=parcel_labels):\n",
    "    \n",
    "    # assign thresholds\n",
    "    bon_alpha = np.sqrt(0.05/disc_rho.shape[1])\n",
    "    discbonnmint = scp.t.ppf(1-bon_alpha, discN-1)\n",
    "    discuncorrmint = scp.t.ppf(1-alpha, discN-1)\n",
    "    repbonnmint = scp.t.ppf(1-bon_alpha, repN-1)\n",
    "    repuncorrmint = scp.t.ppf(1-alpha, repN-1)\n",
    "\n",
    "    # convert rho to t-stat\n",
    "    disc_tstat = disc_rho/np.sqrt((1-disc_rho**2)/(discN-2))\n",
    "    rep_tstat = rep_rho/np.sqrt((1-rep_rho**2)/(repN-2))\n",
    "\n",
    "    # find sig ts that replicate\n",
    "    disc_sigts =  (disc_tstat > discbonnmint).astype(int)\n",
    "    rep_sigts =  (rep_tstat > repbonnmint).astype(int)\n",
    "\n",
    "    sigts = np.zeros(disc_sigts.shape).astype(int)\n",
    "    sigts[(disc_sigts==1) & (rep_sigts==1)] = 1\n",
    "    if sigts.max()==0:\n",
    "        print('No significant findings.')\n",
    "    else:\n",
    "        print('At least one region is significantly simliar.')\n",
    "    \n",
    "    # Plot and analysis replicating peaks\n",
    "    times = np.arange(0, video_dur, TR)\n",
    "\n",
    "    results = dict()\n",
    "\n",
    "    ### Make plots ###\n",
    "    for p, parcel in enumerate(parcel_labels):\n",
    "        if np.max(sigts.T[p])==1:\n",
    "            # set up figure\n",
    "            fig, ax = plt.subplots(2,1,figsize=(12,6), sharex=True, sharey=True)\n",
    "            # plot discovery\n",
    "            dupeaks, duproperties = scs.find_peaks(disc_tstat.T[p], width=5, prominence=discuncorrmint)\n",
    "\n",
    "            ax[0].plot(times, disc_tstat.T[p], color='k')\n",
    "            for i_c, c in enumerate(duproperties['prominences']):\n",
    "                ax[0].axvspan(duproperties['left_ips'][i_c]*TR, duproperties['right_ips'][i_c]*TR,\n",
    "                            color='r', alpha=0.3)\n",
    "            dcpeaks, dcproperties = scs.find_peaks(disc_tstat.T[p], width=5, prominence=discbonnmint)\n",
    "            for i_c, c in enumerate(dcproperties['prominences']):\n",
    "                ax[0].axvspan(dcproperties['left_ips'][i_c]*TR, dcproperties['right_ips'][i_c]*TR,\n",
    "                            color='b', alpha=0.3)\n",
    "            ax[0].set_xlim([0,video_dur])\n",
    "            ax[0].set_title(parcel + ': Discovery', weight='bold')\n",
    "            ax[0].set_ylabel('Similarity (t-stat)')\n",
    "\n",
    "            # plot replication\n",
    "            rupeaks, ruproperties = scs.find_peaks(rep_tstat.T[p], width=5, prominence=repuncorrmint)\n",
    "            ax[1].plot(times, rep_tstat.T[p], color='k')\n",
    "            for i_c, c in enumerate(ruproperties['prominences']):\n",
    "                ax[1].axvspan(ruproperties['left_ips'][i_c]*TR, ruproperties['right_ips'][i_c]*TR,\n",
    "                            color='r', alpha=0.3)\n",
    "            rcpeaks, rcproperties = scs.find_peaks(rep_tstat.T[p], width=5, prominence=repbonnmint)\n",
    "            for i_c, c in enumerate(rcproperties['prominences']):\n",
    "                ax[1].axvspan(rcproperties['left_ips'][i_c]*TR, rcproperties['right_ips'][i_c]*TR,\n",
    "                            color='b', alpha=0.3)\n",
    "            ax[1].set_xlim([0,video_dur])\n",
    "            ax[1].set_title(parcel + ': Replication', weight='bold')\n",
    "            ax[1].set_ylabel('Similarity (t-stat)')\n",
    "            ax[1].set_xlabel('Time (s)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(outprefix + 'peak_similarity_{0}.svg'.format(parcel_labels[p]))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            # save results\n",
    "            dcproperties = {k:v.tolist() for k,v in dcproperties.items()}\n",
    "            duproperties = {k:v.tolist() for k,v in duproperties.items()}\n",
    "            rcproperties = {k:v.tolist() for k,v in rcproperties.items()}\n",
    "            ruproperties = {k:v.tolist() for k,v in ruproperties.items()}\n",
    "            if (len(ruproperties['left_ips'])>0) & (len(duproperties['left_ips'])>0):\n",
    "                results[parcel] = {'Discovery': {'corr': dcproperties, 'uncorr': duproperties}, \n",
    "                                   'Replication': {'corr': rcproperties, 'uncorr': ruproperties}}\n",
    "            \n",
    "            \n",
    "    ### Characterize differences in ratings for high versus low similarity points in the video ###\n",
    "    \n",
    "    # load ratings and shift forward 6 TRs\n",
    "    ratings = pd.read_csv(ratings_file, index_col=0)\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "    ratings.loc[:,ratings.columns] = MinMaxScaler().fit_transform(ratings.to_numpy())\n",
    "    ratings.index = range(6,int(video_dur/TR)+6)\n",
    "    ratings = pd.concat([ratings, pd.DataFrame(np.nan, index=np.arange(0,6,1), columns=ratings.columns)])\n",
    "    ratings = ratings.sort_index()\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "    \n",
    "    # characterize areas within and outside the peaks\n",
    "    for parcel in results.keys():\n",
    "        discmask = np.zeros(times.shape)\n",
    "        for i, c in enumerate(results[parcel]['Discovery']['uncorr']['prominences']):\n",
    "            start = round(results[parcel]['Discovery']['uncorr']['left_ips'][i])\n",
    "            end = round(results[parcel]['Discovery']['uncorr']['right_ips'][i])\n",
    "            discmask[start:end] = 1\n",
    "\n",
    "        repmask = np.zeros(times.shape)\n",
    "        for i, c in enumerate(results[parcel]['Replication']['uncorr']['prominences']):\n",
    "            start = round(results[parcel]['Replication']['uncorr']['left_ips'][i])\n",
    "            end = round(results[parcel]['Replication']['uncorr']['right_ips'][i])\n",
    "            repmask[start:end] = 1\n",
    "\n",
    "        tmask = np.zeros(times.shape).astype(int)\n",
    "        tmask[(discmask==1) & (repmask==1)] = 1\n",
    "\n",
    "        ratesigdiff = pd.Series(index=ratings.columns, name='sig', dtype=int)\n",
    "        ratemeans = dict()\n",
    "        ratestats = dict()\n",
    "        if tmask.max()==1:\n",
    "            for measure in ratings.columns:\n",
    "                underpeak = ratings.loc[tmask==1, measure]\n",
    "                outofpeak = ratings.loc[tmask==0, measure]\n",
    "                underpeakmean = np.nanmean(underpeak)\n",
    "                outofpeakmean = np.nanmean(outofpeak)\n",
    "                rate_stat, rate_pval = scp.ttest_ind(underpeak, outofpeak, nan_policy='omit')\n",
    "                ratestats[measure] = {'tstat': rate_stat, 'pval': rate_pval}\n",
    "                ratesigdiff[measure] = rate_pval < alpha\n",
    "                ratemeans[measure] = {'underpeak': underpeakmean, 'outofpeak': outofpeakmean}\n",
    "\n",
    "            results[parcel]['RatingsAnalysis'] = {'MeanRatings': ratemeans, \n",
    "                                                  'Stats': ratestats}\n",
    "\n",
    "            # plot the differences\n",
    "            sigratingsnames = ratesigdiff[ratesigdiff==True].index.to_list()\n",
    "\n",
    "            if len(sigratingsnames) < 4:\n",
    "                ratings_withpeakinfo = ratings\n",
    "                ratings_withpeakinfo['UnderPeak'] = tmask\n",
    "                temp = pd.melt(ratings_withpeakinfo, id_vars='UnderPeak', value_vars=sigratingsnames, value_name='Level',var_name='Rating')\n",
    "                plt.figure(figsize=(2+2*len(sigratingsnames),6))\n",
    "                sns.barplot(x='Rating', y='Level', hue='UnderPeak', data=temp, palette=['#FFFFFF','#BCB1C2'] , linewidth=2, edgecolor='k')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(outprefix + 'peak_ratings_sigdiff_{0}.svg'.format(parcel))\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "            else:\n",
    "                underpeak = ratings.loc[tmask==1, sigratingsnames].mean(axis=0).to_frame()\n",
    "                underpeak.columns = ['mean']\n",
    "                underpeak['width']=0.4\n",
    "                underpeak['measure'] = underpeak.index\n",
    "                outofpeak = ratings.loc[tmask==0, sigratingsnames].mean(axis=0).to_frame()\n",
    "                outofpeak.columns = ['mean']\n",
    "                outofpeak['width']=0.7\n",
    "                outofpeak['measure'] = outofpeak.index\n",
    "\n",
    "                fig = go.Figure()\n",
    "\n",
    "                fig.add_trace(go.Barpolar(\n",
    "                    r=outofpeak['mean'],\n",
    "                    theta=outofpeak['measure'],\n",
    "                    width=outofpeak['width'],\n",
    "                    base=0,\n",
    "                    name='OutOfPeak',\n",
    "                    marker_color='#FFFFFF',\n",
    "                    marker_line_color='black',\n",
    "                    marker_line_width=2,\n",
    "                    opacity=1,\n",
    "                ))\n",
    "\n",
    "                fig.add_trace(go.Barpolar(\n",
    "                      r=underpeak['mean'],\n",
    "                      theta=underpeak['measure'],\n",
    "                    width=underpeak['width'],\n",
    "                      name='UnderPeak',\n",
    "                    base=0,\n",
    "                    marker_color='#BAB3BF',\n",
    "                    marker_line_color='black',\n",
    "                    marker_line_width=2,\n",
    "                    opacity=1,\n",
    "                ))\n",
    "\n",
    "                fig.update_layout(\n",
    "                    template='plotly_white',\n",
    "                  polar=dict(\n",
    "                      angularaxis_tickfont_size = 14,\n",
    "                    radialaxis=dict(\n",
    "                      visible=True,\n",
    "                      range=[0, 1]\n",
    "                    )),\n",
    "                  showlegend=False\n",
    "                )\n",
    "\n",
    "                fig.write_image(outprefix + 'polar_ratings_sigdiff_{0}.svg'.format(parcel))\n",
    "\n",
    "        \n",
    "    #save results\n",
    "    with open(outprefix + 'full_results_corrp{0}_prom{1}_width{2}.json'.format(round(bon_alpha,3), round(discbonnmint,2), 5), 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "    return(results)\n",
    "\n",
    "\n",
    "def synchrony_discrep_fdr_netlevel(disc_mean_sim, disc_null, rep_mean_sim, outprefix, video_dur, alpha=0.05, \n",
    "                                   TR=TR, parcel_labels=parcel_labels, atlas_file=atlas_file):\n",
    "    # assign thresholds\n",
    "    bon_alpha = np.sqrt(0.05/disc_mean_sim.shape[1])\n",
    "\n",
    "    n = int(round((len(disc_null)+1)*bon_alpha,0))\n",
    "    discbonmin = disc_null[len(disc_null)-n]\n",
    "    n = int(round((len(rep_null)+1)*bon_alpha,0))\n",
    "    repbonmin = rep_null[len(rep_null)-n]\n",
    "    if discbonmin < np.percentile(disc_mean_sim, 1-alpha):\n",
    "        discbonmin = np.percentile(disc_mean_sim, 1-alpha)\n",
    "        repbonmin = np.percentile(rep_mean_sim, 1-alpha)\n",
    "    print(\"Disc sig threshold: {0}\".format(discbonmin))\n",
    "    print(\"Rep sig threshold: {0}\".format(repbonmin))\n",
    "\n",
    "    # find sig rs that replicate\n",
    "    disc_sig = (disc_mean_sim > discbonmin).astype(int)\n",
    "    rep_sig = (rep_mean_sim > repbonmin).astype(int)\n",
    "\n",
    "    sigrs = np.zeros(disc_sig.shape).astype(int)\n",
    "    sigrs[(disc_sig==1) & (rep_sig==1)] = 1\n",
    "    \n",
    "    # find parcels with at least 5 consecutive seconds of sig synchrony\n",
    "    sig_parcels = np.zeros(sigrs.shape[1]).astype(int)\n",
    "    sigrs_df = pd.DataFrame(sigrs, columns=parcel_labels)\n",
    "    sigrs_df['time'] = range(0, len(sigrs_df))\n",
    "    for i, a in enumerate(parcel_labels):\n",
    "        sigrs_df['segment'] = (sigrs_df[a].diff(1) != 0).astype(int).cumsum()\n",
    "        dur = pd.DataFrame({'dur': sigrs_df.groupby('segment').time.last() - sigrs_df.groupby('segment').time.first(),\n",
    "                            'value': sigrs_df.groupby('segment')[a].mean()}).reset_index(drop=True)\n",
    "        dur = dur.loc[dur['value']==1]\n",
    "        if dur['dur'].max() > 6:\n",
    "            sig_parcels[i] = 1\n",
    "\n",
    "    # average synchrony within networks and plot\n",
    "    masked_disc = disc_mean_sim\n",
    "    masked_disc[:,sig_parcels==0]=np.nan\n",
    "    disc_sync_df = pd.DataFrame(disc_mean_sim, columns=network_labels)\n",
    "    disc_sync_df = disc_sync_df.groupby(by=network_labels, axis=1).mean().dropna(axis=1)\n",
    "\n",
    "    masked_rep = rep_mean_sim\n",
    "    masked_rep[:,sig_parcels==0]=np.nan\n",
    "    rep_sync_df = pd.DataFrame(rep_mean_sim, columns=network_labels)\n",
    "    rep_sync_df = rep_sync_df.groupby(by=network_labels, axis=1).mean().dropna(axis=1)\n",
    "\n",
    "    # make figs for sig parcels sorted by network/region group\n",
    "    color = (142/255, 50/255, 209/255, 1)\n",
    "\n",
    "    ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "    data = nib.load(atlas_file).get_fdata()\n",
    "    ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "    newmap=dict()\n",
    "    newmap[0] = ax0[0][1][0]\n",
    "    for net in disc_sync_df:\n",
    "        parcels_keep = parcel_labels[(network_labels==net) & (sig_parcels==1)]\n",
    "        for a in range(1,len(parcel_labels) +1):\n",
    "            if parcel_labels[a-1] in parcels_keep:\n",
    "                newmap[a] = (parcel_labels[a-1], color)\n",
    "            else:\n",
    "                newmap[a] = (parcel_labels[a-1], (1,1,1,0))\n",
    "        ax0.label[0] = newmap\n",
    "        img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "        nib.save(img, outprefix + 'sig_averaged_parcels_{0}.dlabel.nii'.format(net))\n",
    "        \n",
    "    # Plot and analysis replicating peaks\n",
    "    times = np.arange(0, video_dur, TR)\n",
    "\n",
    "    results = dict()\n",
    "\n",
    "    ### Make plots ###\n",
    "    for net in disc_sync_df.columns:\n",
    "        fig, ax = plt.subplots(2,1,figsize=(12,6), sharex=True, sharey=True)\n",
    "        # plot discovery\n",
    "        ax[0].plot(times, disc_sync_df[net], color='k')\n",
    "        dcpeaks, dcproperties = scs.find_peaks(disc_sync_df[net], width=5, height=discbonmin)\n",
    "        for i_c, c in enumerate(dcproperties['prominences']):\n",
    "            ax[0].axvspan(dcproperties['left_ips'][i_c]*TR, dcproperties['right_ips'][i_c]*TR,\n",
    "                        color='#BAB3BF', alpha=0.9)\n",
    "        ax[0].set_xlim([0,video_dur])\n",
    "        ax[0].set_title(net + ': Discovery', weight='bold')\n",
    "        ax[0].set_ylabel('Synchrony')\n",
    "\n",
    "        # plot replication\n",
    "        ax[1].plot(times, rep_sync_df[net], color='k')\n",
    "        rcpeaks, rcproperties = scs.find_peaks(rep_sync_df[net], width=5, height=repbonmin)\n",
    "        for i_c, c in enumerate(rcproperties['prominences']):\n",
    "            ax[1].axvspan(rcproperties['left_ips'][i_c]*TR, rcproperties['right_ips'][i_c]*TR,\n",
    "                        color='#BAB3BF', alpha=0.9)\n",
    "        ax[1].set_xlim([0,video_dur])\n",
    "        ax[1].set_title(net + ': Replication', weight='bold')\n",
    "        ax[1].set_ylabel('Synchrony')\n",
    "        ax[1].set_xlabel('Time (s)')\n",
    "        plt.tight_layout()\n",
    "        #plt.show()\n",
    "        plt.savefig(outprefix + 'peak_similarity_{0}.svg'.format(net))\n",
    "        plt.close()\n",
    "\n",
    "        # save results\n",
    "        dcproperties = {k:v.tolist() for k,v in dcproperties.items()}\n",
    "        rcproperties = {k:v.tolist() for k,v in rcproperties.items()}\n",
    "        if (len(rcproperties['left_ips'])>0) & (len(dcproperties['left_ips'])>0):\n",
    "            results[net] = {'Discovery': {'corr': dcproperties},\n",
    "                               'Replication': {'corr': rcproperties}}\n",
    "\n",
    "\n",
    "    ### Characterize differences in ratings for high versus low similarity points in the video ###\n",
    "\n",
    "    # load ratings and shift forward 6 TRs (4.8 seconds)\n",
    "    ratings = pd.read_csv(ratings_file, index_col=0)\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "    ratings.loc[:,ratings.columns] = MinMaxScaler().fit_transform(ratings.to_numpy())\n",
    "    ratings.index = range(6,int(video_dur/TR)+6)\n",
    "    ratings = pd.concat([ratings, pd.DataFrame(np.nan, index=np.arange(0,6,1), columns=ratings.columns)])\n",
    "    ratings = ratings.sort_index()\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "\n",
    "    # identify areas within and outside the peaks\n",
    "    for parcel in results.keys():\n",
    "        discmask = np.zeros(times.shape)\n",
    "        for i, c in enumerate(results[parcel]['Discovery']['corr']['peak_heights']):\n",
    "            start = round(results[parcel]['Discovery']['corr']['left_ips'][i])\n",
    "            end = round(results[parcel]['Discovery']['corr']['right_ips'][i])\n",
    "            discmask[start:end] = 1\n",
    "\n",
    "        repmask = np.zeros(times.shape)\n",
    "        for i, c in enumerate(results[parcel]['Replication']['corr']['peak_heights']):\n",
    "            start = round(results[parcel]['Replication']['corr']['left_ips'][i])\n",
    "            end = round(results[parcel]['Replication']['corr']['right_ips'][i])\n",
    "            repmask[start:end] = 1\n",
    "\n",
    "        tmask = np.zeros(times.shape).astype(int)\n",
    "        tmask[(discmask==1) & (repmask==1)] = 1\n",
    "\n",
    "        ratesigdiff = pd.Series(index=ratings.columns, name='sig', dtype=int)\n",
    "        ratemeans = dict()\n",
    "        ratestats = dict()\n",
    "        \n",
    "        if tmask.max()==1:\n",
    "            for measure in ratings.columns:\n",
    "                underpeak = ratings.loc[tmask==1, measure]\n",
    "                outofpeak = ratings.loc[tmask==0, measure]\n",
    "                underpeakmean = np.nanmean(underpeak)\n",
    "                outofpeakmean = np.nanmean(outofpeak)\n",
    "                rate_stat, rate_pval = scp.ttest_ind(underpeak, outofpeak, nan_policy='omit')\n",
    "                ratestats[measure] = {'tstat': rate_stat, 'pval': rate_pval}\n",
    "                ratesigdiff[measure] = rate_pval < alpha\n",
    "                ratemeans[measure] = {'underpeak': underpeakmean, 'outofpeak': outofpeakmean}\n",
    "\n",
    "            results[parcel]['RatingsAnalysis'] = {'MeanRatings': ratemeans, \n",
    "                                                  'Stats': ratestats}\n",
    "\n",
    "            # plot the differences\n",
    "            sigratingsnames = ratesigdiff[ratesigdiff==True].index.to_list()\n",
    "            \n",
    "            try:\n",
    "                if len(sigratingsnames) < 4:\n",
    "                    ratings_withpeakinfo = ratings\n",
    "                    ratings_withpeakinfo['UnderPeak'] = tmask\n",
    "                    temp = pd.melt(ratings_withpeakinfo, id_vars='UnderPeak', value_vars=sigratingsnames, value_name='Level',var_name='Rating')\n",
    "                    plt.figure(figsize=(2+2*len(sigratingsnames),6))\n",
    "                    fig = sns.barplot(x='Rating', y='Level', hue='UnderPeak', data=temp, palette=['#FFFFFF','#BCB1C2'] , linewidth=2, edgecolor='k')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(outprefix + 'peak_ratings_sigdiff_{0}.svg'.format(parcel))\n",
    "                    #plt.show()\n",
    "                    plt.close()\n",
    "                    ratings = ratings.drop('UnderPeak', axis=1)\n",
    "                else:\n",
    "                    if 'UnderPeak' in ratings.columns:\n",
    "                        ratings = ratings.drop('UnderPeak', axis=1)\n",
    "                    underpeak = ratings.loc[tmask==1, sigratingsnames].mean(axis=0).to_frame()\n",
    "                    if 'UnderPeak' in underpeak.index:\n",
    "                        underpeak = underpeak.drop('UnderPeak', axis=0)\n",
    "                    underpeak.columns = ['mean']\n",
    "                    underpeak['width']=0.4\n",
    "                    underpeak['measure'] = underpeak.index\n",
    "                    outofpeak = ratings.loc[tmask==0, sigratingsnames].mean(axis=0).to_frame()\n",
    "                    if 'UnderPeak' in outofpeak.columns:\n",
    "                        outofpeak = outofpeak.drop('UnderPeak', axis=0)\n",
    "                    outofpeak.columns = ['mean']\n",
    "                    outofpeak['width']=0.7\n",
    "                    outofpeak['measure'] = outofpeak.index\n",
    "\n",
    "                    fig = go.Figure()\n",
    "\n",
    "                    fig.add_trace(go.Barpolar(\n",
    "                        r=outofpeak['mean'],\n",
    "                        theta=outofpeak['measure'],\n",
    "                        width=outofpeak['width'],\n",
    "                        base=0,\n",
    "                        name='OutOfPeak',\n",
    "                        marker_color='#FFFFFF',\n",
    "                        marker_line_color='black',\n",
    "                        marker_line_width=2,\n",
    "                        opacity=1,\n",
    "                    ))\n",
    "\n",
    "                    fig.add_trace(go.Barpolar(\n",
    "                          r=underpeak['mean'],\n",
    "                          theta=underpeak['measure'],\n",
    "                        width=underpeak['width'],\n",
    "                          name='UnderPeak',\n",
    "                        base=0,\n",
    "                        marker_color='#BAB3BF',\n",
    "                        marker_line_color='black',\n",
    "                        marker_line_width=2,\n",
    "                        opacity=1,\n",
    "                    ))\n",
    "\n",
    "                    fig.update_layout(\n",
    "                        template='plotly_white',\n",
    "                      polar=dict(\n",
    "                          angularaxis_tickfont_size = 14,\n",
    "                        radialaxis=dict(\n",
    "                          visible=True,\n",
    "                          range=[0, 1]\n",
    "                        )),\n",
    "                      showlegend=False\n",
    "                    )\n",
    "\n",
    "                    fig.write_image(outprefix + 'polar_ratings_sigdiff_{0}.svg'.format(parcel))\n",
    "            except:\n",
    "                print('no overlap between discovery and replication')\n",
    "\n",
    "    #save results\n",
    "    with open(outprefix + 'full_results_corrp{0}_prom{1}_width{2}.json'.format(round(bon_alpha,3), round(discbonmin,2), 5), 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "        \n",
    "    return(disc_sync_df, rep_sync_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5783f5-d0c4-4df1-9a1c-4d8e3e396fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peak_rating_diffs(peak_mask, ratings_file, video_dur, out_file, color='#BAB3BF', TR=TR, alpha=0.05, fdr=True, shift=6):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    peak_mask: numpy ndarray\n",
    "        timeseries mask in the shape of Nsamples, with 1 indicating peak and 0 indiciating nonpeak.\n",
    "    ratings_file: filename\n",
    "        CSV file containing the ratings to use to chracterize peak versus non-peak\n",
    "    video_dur: float\n",
    "        duration in seconds of the movie\n",
    "    out_file: string\n",
    "        Name to save the plot under\n",
    "    TR = float\n",
    "        Repetition Time in seconds\n",
    "    alpha: float\n",
    "        p-value to determine significance for t-tests (peak versus nonpeak)\n",
    "    shift: int\n",
    "        Number of samples to shift the ratings over to account for the delayed hemodynamic response\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results: dict\n",
    "        dictionary with full t-test results across all ratings\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    results={'MeanRatings':dict(), 'Stats':dict(), 'pvals': dict()}\n",
    "\n",
    "    ratings = pd.read_csv(ratings_file, index_col=0)\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "    ratings.loc[:,ratings.columns] = MinMaxScaler().fit_transform(ratings.to_numpy())\n",
    "    ratings.index = range(6,int(video_dur/TR)+6)\n",
    "    ratings = pd.concat([ratings, pd.DataFrame(np.nan, index=np.arange(0,6,1), columns=ratings.columns)])\n",
    "    ratings = ratings.sort_index()\n",
    "    ratings = ratings.iloc[0:int(video_dur/TR),:]\n",
    "\n",
    "    for measure in ratings.columns:\n",
    "        underpeak = ratings.loc[peak_mask==1, measure]\n",
    "        outofpeak = ratings.loc[peak_mask==0, measure]\n",
    "        underpeakmean = np.nanmean(underpeak)\n",
    "        outofpeakmean = np.nanmean(outofpeak)\n",
    "        rate_stat, rate_pval = scp.ttest_ind(underpeak, outofpeak, nan_policy='omit')\n",
    "        results['Stats'][measure] = {'tstat': rate_stat, 'pval': rate_pval}\n",
    "        results['pvals'][measure] = rate_pval\n",
    "        results['MeanRatings'][measure] = {'underpeak': underpeakmean, 'outofpeak': outofpeakmean}\n",
    "\n",
    "    # plot the differences\n",
    "    if not fdr:\n",
    "        sigratingsnames = [k for (k, v) in results['pvals'].items() if v<alpha]\n",
    "    else: \n",
    "        from statsmodels.stats.multitest import multipletests\n",
    "        pvals = list(results['pvals'].values())\n",
    "        meas_names = list(results['pvals'].keys())\n",
    "        sig, q, _, _ = multipletests(pvals, alpha, method='fdr_bh')\n",
    "        sigratingsnames = [m for i,m in enumerate(meas_names) if q[i]<alpha]\n",
    "\n",
    "    if (len(sigratingsnames) < 3) & (len(sigratingsnames) > 0):\n",
    "        ratings_withpeakinfo = ratings\n",
    "        ratings_withpeakinfo['UnderPeak'] = peak_mask\n",
    "        temp = pd.melt(ratings_withpeakinfo, id_vars='UnderPeak', value_vars=sigratingsnames, value_name='Level',var_name='Rating')\n",
    "        plt.figure(figsize=(2+2*len(sigratingsnames),6))\n",
    "        f = sns.barplot(x='Rating', y='Level', hue='UnderPeak', data=temp, palette=['#FFFFFF', color], linewidth=2, edgecolor='k')\n",
    "        f.legend_.remove()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_file)\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        ratings = ratings.drop('UnderPeak', axis=1)\n",
    "    elif len(sigratingsnames) >= 3:\n",
    "        if 'UnderPeak' in ratings.columns:\n",
    "            ratings = ratings.drop('UnderPeak', axis=1)\n",
    "        underpeak = ratings.loc[peak_mask==1, sigratingsnames].mean(axis=0).to_frame()\n",
    "        if 'UnderPeak' in underpeak.index:\n",
    "            underpeak = underpeak.drop('UnderPeak', axis=0)\n",
    "        underpeak.columns = ['mean']\n",
    "        underpeak['width']=0.4\n",
    "        underpeak['measure'] = underpeak.index\n",
    "        outofpeak = ratings.loc[peak_mask==0, sigratingsnames].mean(axis=0).to_frame()\n",
    "        if 'UnderPeak' in outofpeak.columns:\n",
    "            outofpeak = outofpeak.drop('UnderPeak', axis=0)\n",
    "        outofpeak.columns = ['mean']\n",
    "        outofpeak['width']=0.7\n",
    "        outofpeak['measure'] = outofpeak.index\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Barpolar(\n",
    "            r=outofpeak['mean'],\n",
    "            theta=outofpeak['measure'],\n",
    "            width=outofpeak['width'],\n",
    "            base=0,\n",
    "            name='OutOfPeak',\n",
    "            marker_color='#FFFFFF',\n",
    "            marker_line_color='black',\n",
    "            marker_line_width=2,\n",
    "            opacity=1,\n",
    "        ))\n",
    "\n",
    "        fig.add_trace(go.Barpolar(\n",
    "              r=underpeak['mean'],\n",
    "              theta=underpeak['measure'],\n",
    "            width=underpeak['width'],\n",
    "              name='UnderPeak',\n",
    "            base=0,\n",
    "            marker_color=color,\n",
    "            marker_line_color='black',\n",
    "            marker_line_width=2,\n",
    "            opacity=1,\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            template='plotly_white',\n",
    "            font=dict(family='Arial', size=12, color='black'),\n",
    "          polar=dict(\n",
    "              angularaxis_tickfont_size = 24,\n",
    "            radialaxis=dict(\n",
    "              visible=False,\n",
    "              range=[0, 1]\n",
    "            )),\n",
    "          showlegend=False\n",
    "        )\n",
    "\n",
    "        fig.write_image(out_file)\n",
    "    else:\n",
    "        print('No differences between peak and nonpeak ratings')\n",
    "        \n",
    "    return(results)\n",
    "\n",
    "\n",
    "def match_peak_to_clips(peaktimes, shift, video_file, outfolder, ratings_file, bool_list=None, dim_list=None):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    peaktimes: DataFrame\n",
    "        DataFrame containing and index of unique values (will be how clips are named) and a 'start' and 'end' column \n",
    "        with times in seconds.\n",
    "    shift: float\n",
    "        Time in seconds to shift the peak onset/offset by (is subtracted from each)\n",
    "    video_file: str\n",
    "        the moviepy compatible video file to pull clips from\n",
    "    outfolder: str\n",
    "        the folder path to save the clips under\n",
    "    ratings_file: str\n",
    "        CSV containing a pandas dataframe with an index of time in seconds and columns of video features to plot\n",
    "    '''\n",
    "    \n",
    "    import moviepy.video.io.ffmpeg_tools as mpff\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    features = pd.read_csv(ratings_file, index_col=0)\n",
    "    features.loc[:,:] = MinMaxScaler().fit_transform(features.to_numpy())\n",
    "\n",
    "    for clipnum in peaktimes.index:\n",
    "        outfile = os.path.join(outfolder, 'clip{0}.mp4'.format(clipnum))\n",
    "        start = peaktimes.loc[clipnum,'start'] - shift - 4\n",
    "        end = peaktimes.loc[clipnum,'end'] - shift + 4\n",
    "        if start<0:\n",
    "            start=0\n",
    "        if end>0:\n",
    "            mpff.ffmpeg_extract_subclip(video_file, start, end, outfile)\n",
    "        if not bool_list and not dim_list:\n",
    "            fig, ax = plt.subplots(figsize=(6,4))\n",
    "            features.loc[start:end,:].plot(kind='line', ax=ax)\n",
    "            plt.tight_layout()\n",
    "            sns.despine()\n",
    "            plt.savefig(os.path.join(outfolder, 'clip{0}.svg'.format(clipnum)))\n",
    "            plt.close()\n",
    "        else:\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(8,1.5*len(bool_list)))\n",
    "                ax = features.loc[start:end,bool_list].plot(subplots=True, kind='area', ax=ax, xlim=(start,end), sharex=True, sharey=True)\n",
    "                for a in range(0, len(bool_list)):\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'start'] - shift, color='k', linestyle='-')\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'end'] - shift, color='k', linestyle='-')\n",
    "                sns.despine()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(outfolder, 'clip{0}_boolvarsleg.svg'.format(clipnum)))\n",
    "                plt.close()\n",
    "                fig, ax = plt.subplots(figsize=(8,1.5*len(bool_list)))\n",
    "                ax = features.loc[start:end,bool_list].plot(subplots=True, kind='area', ax=ax, legend=False, xlim=(start,end), sharex=True, sharey=True)\n",
    "                for a in range(0, len(bool_list)):\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'start'] - shift, color='k', linestyle='-')\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'end'] - shift, color='k', linestyle='-')\n",
    "                sns.despine()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(outfolder, 'clip{0}_boolvars_noleg.svg'.format(clipnum)))\n",
    "                plt.close()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(6,1.5*len(dim_list)))\n",
    "                ax = features.loc[start:end,dim_list].plot(subplots=True, ax=ax, xlim=(start,end), sharex=True)\n",
    "                for a in range(0, len(dim_list)):\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'start'] - shift, color='k', linestyle='-')\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'end'] - shift, color='k', linestyle='-')\n",
    "                sns.despine()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(outfolder, 'clip{0}_dimvarsleg.svg'.format(clipnum)))\n",
    "                plt.close()\n",
    "                fig, ax = plt.subplots(figsize=(6,1.5*len(dim_list)))\n",
    "                ax = features.loc[start:end,dim_list].plot(subplots=True, ax=ax, legend=False, xlim=(start,end), sharex=True)\n",
    "                for a in range(0, len(dim_list)):\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'start'] - shift, color='k', linestyle='-')\n",
    "                    ax[a].axvline(x=peaktimes.loc[clipnum,'end'] - shift, color='k', linestyle='-')\n",
    "                sns.despine()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(outfolder, 'clip{0}_dimvars_nleg.svg'.format(clipnum)))\n",
    "                plt.close()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "def plot_network_activation(ts_data, nois, network_labels, time, groups, group_labels, outdir):\n",
    "    \n",
    "    colors = ['b','k','r','g','y']\n",
    "    # convert to percent signal change\n",
    "    ts_psc_data = (ts_data - ts_data.min(axis=0, keepdims=True)) / (ts_data.max(axis=0, keepdims=True) - ts_data.min(axis=0, keepdims=True))\n",
    "    mean_sig = np.mean(ts_psc_data, axis=0, keepdims=True)\n",
    "    ts_psc_data = ((ts_psc_data-mean_sig)/mean_sig)*100\n",
    "\n",
    "    # average across network/region\n",
    "    ts_psc_net_data = np.zeros((ts_psc_data.shape[0], len(nois), ts_psc_data.shape[2]))\n",
    "    for i, n in enumerate(nois):\n",
    "        ts_psc_net_data[:,i,:] = np.mean(ts_psc_data[:,network_labels==n,:], axis=1)\n",
    "\n",
    "    # plot group level traces\n",
    "    fig, ax = plt.subplots(len(nois),1,figsize=(12,2*len(nois)), sharex=True)\n",
    "    for i, g in enumerate(groups):\n",
    "        mean_sig = np.mean(ts_psc_net_data[:,:,group_labels==g], axis=2)\n",
    "        for j, net in enumerate(nois):  \n",
    "            ax[j].plot(time, mean_sig[:,j], color=colors[i], label=g)\n",
    "            ax[j].set_xlim((0,time[-1]))\n",
    "            ax[j].set_title(net)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, 'network_activation_noleg.svg'))\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(outdir, 'network_activation_leg.svg'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941945a3-24df-46ba-b639-6efae0595be2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# How are total symptom scores associated with activation patterns in each age group (and across age) covarying age, sex, motion, and MDD symptoms?\n",
    "1. Use ISC to test 3 IS-RSA models with each video (Despicable Me, The Present) and sample (rubic, cbic)\n",
    "2. Determine best fitting model for each parcel separately for each video (Despicable Me, The Present) and sample (rubic, cbic)\n",
    "3. Determine which best-fitting models replicate across samples for each video\n",
    "3. Determine which best-fitting models replicate across all videos and samples\n",
    "\n",
    "Repeat procedures for each puberty and age groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd324eaa-1910-4d3a-a458-158371ac1173",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Full sample analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fce598-0ba9-4702-970b-319046331465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clin in ['SCARED_P_SC', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_GD']:\n",
    "for clin in ['SCARED_P_SP', 'SCARED_SR_SP']:\n",
    "    if 'SR' in clin:\n",
    "        other = 'MFQ_SR_Total'\n",
    "    else:\n",
    "        other = 'MFQ_P_Total'\n",
    "    for sample in ['rubic','cbic']:\n",
    "        for movie in ['TP','DM']:\n",
    "            sampleinfo = subinfo.loc[(subinfo['site']==sample) & (subinfo['movie']==movie) & np.isfinite(subinfo[clin]),:]\n",
    "            sampleinfo.loc[:,['age','meanFD', clin, other]] = StandardScaler().fit_transform(sampleinfo.loc[:,['age', 'meanFD', clin, other]])\n",
    "            sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other]] = \\\n",
    "            IterativeImputer(random_state=42).fit_transform(sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other]])\n",
    "            res = smf.ols('{0} ~ age + female + meanFD + {1}'.format(clin, other), data=sampleinfo).fit()\n",
    "            sampleinfo[clin] = res.resid.to_frame().iloc[:,0]\n",
    "\n",
    "            print(clin, movie, sample, 'N =', len(sampleinfo))\n",
    "            outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs', 'all', \n",
    "                                  'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "            os.makedirs(outdir, exist_ok=True)\n",
    "            group_data_file = os.path.join(outdir, 'compiled_timeseries_data_{0}_movie{1}.npy'.format(sample, movie))\n",
    "            if os.path.isfile(group_data_file):\n",
    "                group_data = np.load(group_data_file)\n",
    "            else:\n",
    "                group_data = compile_ts_data(sampleinfo, movie, data_dir, group_data_file)\n",
    "\n",
    "            outprefix = os.path.join(outdir, '{0}_movie{1}_'.format(sample, movie))\n",
    "            if os.path.isfile(outprefix + 'intersub_timeseries_ISC.npy'):\n",
    "                regional_sim_data = np.load(outprefix + 'intersub_timeseries_ISC.npy')\n",
    "            else:\n",
    "                regional_sim_data, mean_isc = intersubject_timeseries_correlation(group_data, outprefix)\n",
    "\n",
    "            outfile_prefix = os.path.join(outdir, '{0}_movie{1}_{2}_similarity'.format(sample, movie, clin))\n",
    "            isdistances = intersubject_distance(sampleinfo[clin].to_numpy(), outfile_prefix)\n",
    "\n",
    "            for sim in ['NN', 'AnnaKmin', 'AnnaKmaxminmax']:\n",
    "                outprefix = os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity'.format(sample, movie, clin, sim))\n",
    "                bx_sim_data = isdistances[sim]\n",
    "                if not os.path.isfile(outprefix + '_permsim_raw_rho.pscalar.nii'):\n",
    "                    isc_rho = regional_perm_bx_isrsa(regional_sim_data, bx_sim_data, outprefix)       \n",
    "                    \n",
    "    for movie in ['DM','TP']:\n",
    "        analysis_outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs', 'all', 'ts_isc_movie{0}_{1}'.format(movie, clin))\n",
    "        os.makedirs(analysis_outdir, exist_ok=True)\n",
    "\n",
    "        for sim in ['AnnaKmaxminmax','NN','AnnaKmin']:\n",
    "            disc_pval = os.path.join(out_dir, 'agegroup_similarity_regagesxs','all','ts_isc_rubic_movie{0}_{1}'.format(movie, clin),\n",
    "                                     'rubic_movie{0}_{1}_{2}_similarity_permsim_raw_pval.pscalar.nii'.format(movie, clin, sim))\n",
    "            disc_rho = os.path.join(out_dir,'agegroup_similarity_regagesxs', 'all','ts_isc_rubic_movie{0}_{1}'.format(movie, clin),\n",
    "                                    'rubic_movie{0}_{1}_{2}_similarity_permsim_raw_rho.pscalar.nii'.format(movie, clin, sim))\n",
    "            rep_pval = os.path.join(out_dir,'agegroup_similarity_regagesxs', 'all','ts_isc_cbic_movie{0}_{1}'.format(movie, clin),\n",
    "                                    'cbic_movie{0}_{1}_{2}_similarity_permsim_raw_pval.pscalar.nii'.format(movie, clin, sim))\n",
    "            rep_rho = os.path.join(out_dir, 'agegroup_similarity_regagesxs','all','ts_isc_cbic_movie{0}_{1}'.format(movie, clin),\n",
    "                                   'cbic_movie{0}_{1}_{2}_similarity_permsim_raw_rho.pscalar.nii'.format(movie, clin, sim))\n",
    "\n",
    "            outprefix = os.path.join(analysis_outdir, 'movie{0}_isc_{1}_{2}'.format(movie, clin, sim))\n",
    "            region_isrsa_fdr(disc_rho, disc_pval, rep_rho, rep_pval, outprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2976fbf-e835-4e14-98b1-ce2a6a3c098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = int(10000/len(parcel_labels))\n",
    "\n",
    "#for clin in ['SCARED_P_SC', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_GD']:\n",
    "for clin in ['SCARED_P_SP', 'SCARED_SR_SP']:    \n",
    "    if os.path.isdir(os.path.join(out_dir, 'agegroup_similarity_regagesxs','all','ts_isc_rubic_movie{0}_{1}'.format(movie, clin))):\n",
    "        for sample in ['rubic','cbic']:\n",
    "            for movie in ['TP','DM']:\n",
    "                print(sample, movie, clin)\n",
    "                outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs', 'all', \n",
    "                                      'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "\n",
    "                outprefix = os.path.join(outdir, '{0}_movie{1}_'.format(sample, movie))\n",
    "                if os.path.isfile(outprefix + 'intersub_timeseries_ISC.npy'):\n",
    "                    regional_sim_data = np.load(outprefix + 'intersub_timeseries_ISC.npy')\n",
    "                else:\n",
    "                    regional_sim_data, mean_isc = intersubject_timeseries_correlation(group_data, outprefix)\n",
    "\n",
    "                for sim in ['NN', 'AnnaKmin', 'AnnaKmaxminmax']:\n",
    "                    print(sample, movie, sim, clin)\n",
    "                    sig = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_pval.pscalar.nii'.format(sample, movie, clin, sim))).get_fdata()\n",
    "                    rho = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_rho.pscalar.nii'.format(sample, movie, clin, sim))).get_fdata()\n",
    "                    sigrho = rho\n",
    "                    sigrho[sig>0.05] = 0\n",
    "                    sim_filename = os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}.npy'.format(sample, movie, clin, sim))\n",
    "                    simdist_filename = os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}_bootdist.npy'.format(sample, movie, clin, sim))\n",
    "                    bx_sim_data = np.load(sim_filename)\n",
    "\n",
    "                    rhodist = np.zeros((niters, regional_sim_data.shape[0]))\n",
    "                    for parc in range(0,regional_sim_data.shape[0]):\n",
    "                        parcsim = regional_sim_data[parc,:,:]\n",
    "                        if sigrho[0,parc]>0:\n",
    "                            for i in range(0,niters):\n",
    "                                bootsample_size = np.random.randint(int(regional_sim_data.shape[1]*0.5),int(regional_sim_data.shape[1]*0.75))\n",
    "                                subsampmask = np.full(regional_sim_data.shape[1], 0)\n",
    "                                subsampmask[:bootsample_size] = 1\n",
    "                                np.random.shuffle(subsampmask)\n",
    "                                subsamp_bx = bx_sim_data[subsampmask==1, :][:, subsampmask==1]\n",
    "                                subsamp_brain = parcsim[subsampmask==1, :][:, subsampmask==1]\n",
    "                                res = static_brain_bx_isrsa(subsamp_brain, subsamp_bx)\n",
    "                                rhodist[i,parc] = res.loc[0,'SpearR']\n",
    "                    np.save(simdist_filename, rhodist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69fa48-3e21-4c2e-8294-1fdf7d4fd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.sqrt(0.05/len(parcel_labels))\n",
    "ciftkey = {0: {'label':'None', 'cifticolor':(1, 1, 1, 0), 'plotcolor':'#3893f5'}, # clear\n",
    "           1: {'label':'NN', 'cifticolor':(56/255, 147/255, 245/255, 1), 'plotcolor':'#3893f5'}, # blue\n",
    "           2: {'label':'AnnaKmin', 'cifticolor':(235/255, 18/255, 7/255, 1), 'plotcolor': '#eb1207'}, # red\n",
    "           3: {'label':'AnnaKmaxminmax', 'cifticolor':(255/255, 230/255, 0/255, 1), 'plotcolor': '#ffe600'}, # gold\n",
    "           12: {'label':'NN-AnnaKmin', 'cifticolor':(128/255, 4/255, 186/255, 1), 'plotcolor':'#8004ba'}, # purple\n",
    "           21: {'label':'NN-AnnaKmin', 'cifticolor':(128/255, 4/255, 186/255, 1), 'plotcolor':'#8004ba'}, # purple\n",
    "           13: {'label':'NN-AnnaKmaxminmax', 'cifticolor':(224/255, 152/255, 7/255, 1), 'plotcolor': '#e09807'}, # orange\n",
    "           31: {'label':'NN-AnnaKmaxminmax', 'cifticolor':(224/255, 152/255, 7/255, 1), 'plotcolor': '#e09807'}, # orange\n",
    "           23: {'label':'AnnaKmin-AnnaKmaxminmax', 'cifticolor':(2/255, 156/255, 12/255, 1), 'plotcolor': '#029c0c'}, # green\n",
    "           32: {'label':'AnnaKmin-AnnaKmaxminmax', 'cifticolor':(2/255, 156/255, 12/255, 1), 'plotcolor': '#029c0c'},\n",
    "           123: {'label':'All', 'cifticolor':(255/255, 255/255, 255/255, 1), 'plotcolor': '#029c0c'}} # white\n",
    "\n",
    "#for clin in ['SCARED_P_SC', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_GD']:\n",
    "for clin in ['SCARED_P_SP', 'SCARED_SR_SP']:\n",
    "    if os.path.isdir(os.path.join(out_dir, 'agegroup_similarity_regagesxs','all','ts_isc_rubic_movie{0}_{1}'.format(movie, clin))):\n",
    "        for sample in ['rubic','cbic']:\n",
    "            for movie in ['TP','DM']:\n",
    "                # per parcel, rank point estimates and identify best fitting model\n",
    "                fitkey = {'NN': {'value':1, 'cifticolor':(56/255, 147/255, 245/255, 1), 'plotcolor':'#3893f5'}, # blue\n",
    "                          'AnnaKmin': {'value':2, 'cifticolor':(235/255, 18/255, 7/255, 1), 'plotcolor': '#eb1207'}, # red\n",
    "                          'AnnaKmaxminmax': {'value':3, 'cifticolor':(255/255, 230/255, 0/255, 1), 'plotcolor': '#ffe600'}} # gold\n",
    "                sims = list(fitkey.keys())\n",
    "\n",
    "                print(sample, movie, clin)\n",
    "                outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs', 'all', 'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "\n",
    "                # load point estimates for each model of development\n",
    "                pointests = []\n",
    "                pointestsps = []\n",
    "                distributions = []\n",
    "                for est in sims:\n",
    "                    point = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_rho.pscalar.nii'.format(sample, movie, clin, est))).get_fdata()\n",
    "                    pointests.append(point)\n",
    "                    pointps = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_pval.pscalar.nii'.format(sample, movie, clin, est))).get_fdata()\n",
    "                    pointestsps.append(pointps)\n",
    "                    dist = np.load(os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}_bootdist.npy'.format(sample, movie, clin, est)))\n",
    "                    distributions.append(np.expand_dims(dist, axis=2))\n",
    "                distributions = np.concatenate(distributions, axis=2)\n",
    "                pointests = np.concatenate(pointests, axis=0)\n",
    "                pointestsps = np.concatenate(pointestsps, axis=0)\n",
    "\n",
    "                pointests_binarized = np.zeros_like(pointests)\n",
    "                pointests_binarized[(pointests>0) & (pointestsps<alpha)] = 1\n",
    "                sigpointestssum = np.sum(pointests_binarized, axis=0).astype(int)\n",
    "\n",
    "                rankorder = np.zeros((len(sims), pointests.shape[1]))\n",
    "\n",
    "                for parc in range(0, sigpointestssum.shape[0]):\n",
    "                    parc_bin = pointests_binarized[:,parc]\n",
    "                    if sigpointestssum[parc]==1:\n",
    "                        rankorder[0,parc] = np.where(parc_bin==1)[0][0] + 1\n",
    "                    elif sigpointestssum[parc]==2:\n",
    "                        models = np.where(parc_bin==1)[0]\n",
    "                        t, p = scp.stats.ttest_rel(distributions[:, parc,models[0]],distributions[:, parc,models[1]])\n",
    "                        if p<0.05:\n",
    "                            if pointests[models[0], parc] > pointests[models[1], parc]:\n",
    "                                rankorder[0,parc] = models[0] + 1\n",
    "                                rankorder[1,parc] = models[1] + 1\n",
    "                            else:\n",
    "                                rankorder[0,parc] = models[1] + 1\n",
    "                                rankorder[1,parc] = models[0] + 1\n",
    "                        else:\n",
    "                            rankorder[0,parc] = int('{0}{1}'.format(models[0],models[1]))\n",
    "                    elif sigpointestssum[parc]==3:\n",
    "                        tpoint = pointests[:,parc]\n",
    "                        rank = np.argsort(tpoint)\n",
    "                        t, p = scp.stats.ttest_rel(distributions[:,parc,rank[0]],distributions[:, parc,rank[1]])\n",
    "                        if p<0.05:\n",
    "                            rankorder[0,parc] = rank[0] + 1\n",
    "                            # test second versus third place\n",
    "                            t, p = scp.stats.ttest_ind(distributions[:,parc,rank[1]],distributions[:, parc,rank[2]])\n",
    "                            if p<0.05:\n",
    "                                rankorder[1,parc] = rank[1] + 1\n",
    "                                rankorder[2,parc] = rank[2] + 1\n",
    "                            else:\n",
    "                                rankorder[1,parc] = int('{0}{1}'.format(rank[1],rank[2]))\n",
    "                        else:\n",
    "                            t, p = scp.stats.ttest_rel(distributions[:, parc,rank[0]],distributions[:, parc,rank[2]])\n",
    "                            if p<0.05:\n",
    "                                rankorder[0,parc] = int('{0}{1}'.format(models[0],models[1]))\n",
    "                                rankorder[1,parc] = rank[2] + 1\n",
    "                            else:\n",
    "                                rankorder[0,parc] = 123\n",
    "                np.save(os.path.join(outdir, 'model_rank_orders.npy'), rankorder)\n",
    "\n",
    "                # make cifti label file with best model fit\n",
    "                ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "                data = nib.load(atlas_file).get_fdata()\n",
    "                ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "                newmap=dict()\n",
    "                newmap[0] = ax0[0][1][0]\n",
    "                for a in range(0,len(parcel_labels)):\n",
    "                    newmap[a+1] = (ciftkey[rankorder[0,a]]['label'] + '_n{0}'.format(a), ciftkey[rankorder[0,a]]['cifticolor'])\n",
    "                ax0.label[0] = newmap\n",
    "                img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "                nib.save(img, os.path.join(outdir, 'top_model_fits.dlabel.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6246dd7-34ba-485a-b09f-a71cc74fb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "for clin in ['SCARED_P_SC', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_GD','SCARED_P_SP', 'SCARED_SR_SP']:\n",
    "    # find consistent patterns across discovery and replication samples\n",
    "    for movie in ['TP','DM']:\n",
    "        disc_fits = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxs','all', 'ts_isc_rubic_movie{0}_{1}'.format(movie, clin), 'model_rank_orders.npy'))\n",
    "        rep_fits = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxs','all', 'ts_isc_cbic_movie{0}_{1}'.format(movie, clin), 'model_rank_orders.npy'))\n",
    "\n",
    "        outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs','all', 'ts_isc_movie{0}_{1}'.format(movie, clin))\n",
    "\n",
    "        ### make a cifti with the outputs\n",
    "        # make label file with best model fit\n",
    "        mask = np.zeros((disc_fits.shape[1]))\n",
    "        mask[(disc_fits[0]==rep_fits[0])]=1\n",
    "\n",
    "        both_fits = disc_fits[0]\n",
    "        both_fits[mask==0] = 0\n",
    "        np.save(os.path.join(outdir,'replicable_top_models.npy'), both_fits)\n",
    "\n",
    "        ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "        data = nib.load(atlas_file).get_fdata()\n",
    "        ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "        newmap=dict()\n",
    "        newmap[0] = ax0[0][1][0]\n",
    "        for a in range(0,len(parcel_labels)):\n",
    "            newmap[a+1] = (ciftkey[both_fits[a]]['label'] + '_n{0}'.format(a), ciftkey[both_fits[a]]['cifticolor'])\n",
    "        ax0.label[0] = newmap\n",
    "        img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "        nib.save(img, os.path.join(outdir, 'top_model_fits_replicable.dlabel.nii'))\n",
    "        \n",
    "    # find consistent patterns across both movies\n",
    "    outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs','all', 'ts_isc_bothmovies_{0}'.format(clin))\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    DM = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxs','all', 'ts_isc_movieDM_{0}'.format(clin),'replicable_top_models.npy'))\n",
    "    TP = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxs','all', 'ts_isc_movieTP_{0}'.format(clin),'replicable_top_models.npy'))\n",
    "\n",
    "    mask = np.zeros((DM.shape[0]))\n",
    "    mask[(DM==TP)]=1\n",
    "\n",
    "    both_fits = DM\n",
    "    both_fits[mask==0] = 0\n",
    "\n",
    "    np.save(os.path.join(outdir,'samebothvideos_top_models.npy'), both_fits)\n",
    "    ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "    data = nib.load(atlas_file).get_fdata()\n",
    "    ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "    newmap=dict()\n",
    "    newmap[0] = ax0[0][1][0]\n",
    "    for a in range(0,len(parcel_labels)):\n",
    "        newmap[a+1] = (ciftkey[both_fits[a]]['label'] + '_n{0}'.format(a), ciftkey[both_fits[a]]['cifticolor'])\n",
    "    ax0.label[0] = newmap\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "    nib.save(img, os.path.join(outdir, 'top_model_fits_replicable_bothmovies.dlabel.nii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb46dd1-d514-4368-abee-0c09ea76eced",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IS-RSA for each age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed591bf2-194b-4086-a1d5-754ec9bc2b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groups = np.unique(subinfo['age_group'])\n",
    "\n",
    "for age in groups:\n",
    "    agesubinfo = subinfo.loc[subinfo['age_group']==age,:]\n",
    "    if age=='younger':  \n",
    "        clins = ['SCARED_P_SC','SCARED_P_GD','SCARED_P_SP']\n",
    "    else:\n",
    "        clins = ['SCARED_SR_SC','SCARED_P_SC','SCARED_P_GD', 'SCARED_SR_GD','SCARED_P_SP', 'SCARED_SR_SP']\n",
    "\n",
    "    for clin in clins:\n",
    "        if 'SR' in clin:\n",
    "            other = 'MFQ_SR_Total'\n",
    "        else:\n",
    "            other = 'MFQ_P_Total'\n",
    "        for sample in ['rubic','cbic']:\n",
    "            for movie in ['TP','DM']:\n",
    "                sampleinfo = agesubinfo.loc[(agesubinfo['site']==sample) & (agesubinfo['movie']==movie) & np.isfinite(agesubinfo[clin]),:]\n",
    "                sampleinfo.loc[:,['age','meanFD', clin, other]] = StandardScaler().fit_transform(sampleinfo.loc[:,['age', 'meanFD', clin, other]])\n",
    "                sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other]] = \\\n",
    "                IterativeImputer(random_state=42).fit_transform(sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other]])\n",
    "                res = smf.ols('{0} ~ female + meanFD + {1}'.format(clin, other), data=sampleinfo).fit()\n",
    "                sampleinfo[clin] = res.resid.to_frame().iloc[:,0]\n",
    "                \n",
    "                print(age, clin, movie, sample, 'N =', len(sampleinfo))\n",
    "                outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs', 'noregage', age, \n",
    "                                      'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "                os.makedirs(outdir, exist_ok=True)\n",
    "                group_data_file = os.path.join(outdir, 'compiled_timeseries_data_{0}_movie{1}.npy'.format(sample, movie))\n",
    "                if os.path.isfile(group_data_file):\n",
    "                    group_data = np.load(group_data_file)\n",
    "                else:\n",
    "                    group_data = compile_ts_data(sampleinfo, movie, data_dir, group_data_file)\n",
    "\n",
    "                outprefix = os.path.join(outdir, '{0}_movie{1}_'.format(sample, movie))\n",
    "                if os.path.isfile(outprefix + 'intersub_timeseries_ISC.npy'):\n",
    "                    regional_sim_data = np.load(outprefix + 'intersub_timeseries_ISC.npy')\n",
    "                else:\n",
    "                    regional_sim_data, mean_isc = intersubject_timeseries_correlation(group_data, outprefix)\n",
    "\n",
    "                outfile_prefix = os.path.join(outdir, '{0}_movie{1}_{2}_similarity'.format(sample, movie, clin))\n",
    "                isdistances = intersubject_distance(sampleinfo[clin].to_numpy(), outfile_prefix)\n",
    "\n",
    "                for sim in ['NN', 'AnnaKmin', 'AnnaKmaxminmax']:\n",
    "                    outprefix = os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity'.format(sample, movie, clin, sim))\n",
    "                    bx_sim_data = isdistances[sim]\n",
    "                    if not os.path.isfile(outprefix + '_permsim_raw_rho.pscalar.nii'):\n",
    "                        isc_rho = regional_perm_bx_isrsa(regional_sim_data, bx_sim_data, outprefix)\n",
    "                        \n",
    "        for movie in ['DM','TP']:\n",
    "            analysis_outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs', 'noregage', age, 'ts_isc_movie{0}_{1}'.format(movie, clin))\n",
    "            os.makedirs(analysis_outdir, exist_ok=True)\n",
    "\n",
    "            for sim in ['AnnaKmaxminmax','NN','AnnaKmin']:\n",
    "                disc_pval = os.path.join(out_dir, 'agegroup_similarity_regagesxs','noregage', age,'ts_isc_rubic_movie{0}_{1}'.format(movie, clin),\n",
    "                                         'rubic_movie{0}_{1}_{2}_similarity_permsim_raw_pval.pscalar.nii'.format(movie, clin, sim))\n",
    "                disc_rho = os.path.join(out_dir,'agegroup_similarity_regagesxs', 'noregage', age,'ts_isc_rubic_movie{0}_{1}'.format(movie, clin),\n",
    "                                        'rubic_movie{0}_{1}_{2}_similarity_permsim_raw_rho.pscalar.nii'.format(movie, clin, sim))\n",
    "                rep_pval = os.path.join(out_dir,'agegroup_similarity_regagesxs', 'noregage', age,'ts_isc_cbic_movie{0}_{1}'.format(movie, clin),\n",
    "                                        'cbic_movie{0}_{1}_{2}_similarity_permsim_raw_pval.pscalar.nii'.format(movie, clin, sim))\n",
    "                rep_rho = os.path.join(out_dir, 'agegroup_similarity_regagesxs','noregage', age,'ts_isc_cbic_movie{0}_{1}'.format(movie, clin),\n",
    "                                       'cbic_movie{0}_{1}_{2}_similarity_permsim_raw_rho.pscalar.nii'.format(movie, clin, sim))\n",
    "\n",
    "                outprefix = os.path.join(analysis_outdir, 'movie{0}_isc_{1}_{2}'.format(movie, clin, sim))\n",
    "                region_isrsa_fdr(disc_rho, disc_pval, rep_rho, rep_pval, outprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b1c5c8-e464-483e-87d2-3459e9609eb5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "niters = int(10000/len(parcel_labels))\n",
    "\n",
    "for age in groups:\n",
    "    for clin in ['SCARED_SR_SC','SCARED_P_SC','SCARED_P_GD', 'SCARED_SR_GD','SCARED_P_SP', 'SCARED_SR_SP']:\n",
    "        if os.path.isdir(os.path.join(out_dir, 'agegroup_similarity_regagesxs','noregage',age,'ts_isc_rubic_movie{0}_{1}'.format(movie, clin))):\n",
    "            for sample in ['rubic','cbic']:\n",
    "                for movie in ['TP','DM']:\n",
    "                    print(sample, movie, clin)\n",
    "                    outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs', 'noregage',age, \n",
    "                                          'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "\n",
    "                    outprefix = os.path.join(outdir, '{0}_movie{1}_'.format(sample, movie))\n",
    "                    if os.path.isfile(outprefix + 'intersub_timeseries_ISC.npy'):\n",
    "                        regional_sim_data = np.load(outprefix + 'intersub_timeseries_ISC.npy')\n",
    "                    else:\n",
    "                        regional_sim_data, mean_isc = intersubject_timeseries_correlation(group_data, outprefix)\n",
    "\n",
    "                    for sim in ['NN', 'AnnaKmin', 'AnnaKmaxminmax']:\n",
    "                        print(sample, movie, sim, clin)\n",
    "                        sig = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_pval.pscalar.nii'.format(sample, movie, clin, sim))).get_fdata()\n",
    "                        rho = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_rho.pscalar.nii'.format(sample, movie, clin, sim))).get_fdata()\n",
    "                        sigrho = rho\n",
    "                        sigrho[sig>0.05] = 0\n",
    "                        sim_filename = os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}.npy'.format(sample, movie, clin, sim))\n",
    "                        simdist_filename = os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}_bootdist.npy'.format(sample, movie, clin, sim))\n",
    "                        bx_sim_data = np.load(sim_filename)\n",
    "\n",
    "                        rhodist = np.zeros((niters, regional_sim_data.shape[0]))\n",
    "                        for parc in range(0,regional_sim_data.shape[0]):\n",
    "                            parcsim = regional_sim_data[parc,:,:]\n",
    "                            if sigrho[0,parc]>0:\n",
    "                                for i in range(0,niters):\n",
    "                                    bootsample_size = np.random.randint(int(regional_sim_data.shape[1]*0.5),int(regional_sim_data.shape[1]*0.75))\n",
    "                                    subsampmask = np.full(regional_sim_data.shape[1], 0)\n",
    "                                    subsampmask[:bootsample_size] = 1\n",
    "                                    np.random.shuffle(subsampmask)\n",
    "                                    subsamp_bx = bx_sim_data[subsampmask==1, :][:, subsampmask==1]\n",
    "                                    subsamp_brain = parcsim[subsampmask==1, :][:, subsampmask==1]\n",
    "                                    res = static_brain_bx_isrsa(subsamp_brain, subsamp_bx)\n",
    "                                    rhodist[i,parc] = res.loc[0,'SpearR']\n",
    "                        np.save(simdist_filename, rhodist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529b687-8944-4400-b7d9-e3b90a6e70e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = np.sqrt(0.05/len(parcel_labels))\n",
    "ciftkey = {0: {'label':'None', 'cifticolor':(1, 1, 1, 0), 'plotcolor':'#3893f5'}, # clear\n",
    "           1: {'label':'NN', 'cifticolor':(56/255, 147/255, 245/255, 1), 'plotcolor':'#3893f5'}, # blue\n",
    "           2: {'label':'AnnaKmin', 'cifticolor':(235/255, 18/255, 7/255, 1), 'plotcolor': '#eb1207'}, # red\n",
    "           3: {'label':'AnnaKmaxminmax', 'cifticolor':(255/255, 230/255, 0/255, 1), 'plotcolor': '#ffe600'}, # gold\n",
    "           12: {'label':'NN-AnnaKmin', 'cifticolor':(128/255, 4/255, 186/255, 1), 'plotcolor':'#8004ba'}, # purple\n",
    "           21: {'label':'NN-AnnaKmin', 'cifticolor':(128/255, 4/255, 186/255, 1), 'plotcolor':'#8004ba'}, # purple\n",
    "           13: {'label':'NN-AnnaKmaxminmax', 'cifticolor':(224/255, 152/255, 7/255, 1), 'plotcolor': '#e09807'}, # orange\n",
    "           31: {'label':'NN-AnnaKmaxminmax', 'cifticolor':(224/255, 152/255, 7/255, 1), 'plotcolor': '#e09807'}, # orange\n",
    "           23: {'label':'AnnaKmin-AnnaKmaxminmax', 'cifticolor':(2/255, 156/255, 12/255, 1), 'plotcolor': '#029c0c'}, # green\n",
    "           32: {'label':'AnnaKmin-AnnaKmaxminmax', 'cifticolor':(2/255, 156/255, 12/255, 1), 'plotcolor': '#029c0c'},\n",
    "           123: {'label':'All', 'cifticolor':(255/255, 255/255, 255/255, 1), 'plotcolor': '#029c0c'}} # white\n",
    "\n",
    "for age in groups:\n",
    "    for clin in ['SCARED_SR_SC','SCARED_P_SC','SCARED_P_GD', 'SCARED_SR_GD','SCARED_P_SP', 'SCARED_SR_SP']:\n",
    "        if os.path.isdir(os.path.join(out_dir, 'agegroup_similarity_regagesxs','noregage',age,'ts_isc_rubic_movie{0}_{1}'.format(movie, clin))):\n",
    "            for sample in ['rubic','cbic']:\n",
    "                for movie in ['TP','DM']:\n",
    "                    # per parcel, rank point estimates and identify best fitting model\n",
    "                    fitkey = {'NN': {'value':1, 'cifticolor':(56/255, 147/255, 245/255, 1), 'plotcolor':'#3893f5'}, # blue\n",
    "                              'AnnaKmin': {'value':2, 'cifticolor':(235/255, 18/255, 7/255, 1), 'plotcolor': '#eb1207'}, # red\n",
    "                              'AnnaKmaxminmax': {'value':3, 'cifticolor':(255/255, 230/255, 0/255, 1), 'plotcolor': '#ffe600'}} # gold\n",
    "                    sims = list(fitkey.keys())\n",
    "\n",
    "                    print(sample, movie, clin)\n",
    "                    outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs','noregage', age, \n",
    "                                          'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "\n",
    "                    # load point estimates for each model of development\n",
    "                    pointests = []\n",
    "                    pointestsps = []\n",
    "                    distributions = []\n",
    "                    for est in sims:\n",
    "                        point = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_rho.pscalar.nii'.format(sample, movie, clin, est))).get_fdata()\n",
    "                        pointests.append(point)\n",
    "                        pointps = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_pval.pscalar.nii'.format(sample, movie, clin, est))).get_fdata()\n",
    "                        pointestsps.append(pointps)\n",
    "                        dist = np.load(os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}_bootdist.npy'.format(sample, movie, clin, est)))\n",
    "                        distributions.append(np.expand_dims(dist, axis=2))\n",
    "                    distributions = np.concatenate(distributions, axis=2)\n",
    "                    pointests = np.concatenate(pointests, axis=0)\n",
    "                    pointestsps = np.concatenate(pointestsps, axis=0)\n",
    "\n",
    "                    pointests_binarized = np.zeros_like(pointests)\n",
    "                    pointests_binarized[(pointests>0) & (pointestsps<alpha)] = 1\n",
    "                    sigpointestssum = np.sum(pointests_binarized, axis=0).astype(int)\n",
    "\n",
    "                    rankorder = np.zeros((len(sims), pointests.shape[1]))\n",
    "\n",
    "                    for parc in range(0, sigpointestssum.shape[0]):\n",
    "                        parc_bin = pointests_binarized[:,parc]\n",
    "                        if sigpointestssum[parc]==1:\n",
    "                            rankorder[0,parc] = np.where(parc_bin==1)[0][0] + 1\n",
    "                        elif sigpointestssum[parc]==2:\n",
    "                            models = np.where(parc_bin==1)[0]\n",
    "                            t, p = scp.stats.ttest_rel(distributions[:, parc,models[0]],distributions[:, parc,models[1]])\n",
    "                            if p<0.05:\n",
    "                                if pointests[models[0], parc] > pointests[models[1], parc]:\n",
    "                                    rankorder[0,parc] = models[0] + 1\n",
    "                                    rankorder[1,parc] = models[1] + 1\n",
    "                                else:\n",
    "                                    rankorder[0,parc] = models[1] + 1\n",
    "                                    rankorder[1,parc] = models[0] + 1\n",
    "                            else:\n",
    "                                rankorder[0,parc] = int('{0}{1}'.format(models[0],models[1]))\n",
    "                        elif sigpointestssum[parc]==3:\n",
    "                            tpoint = pointests[:,parc]\n",
    "                            rank = np.argsort(tpoint)\n",
    "                            t, p = scp.stats.ttest_rel(distributions[:,parc,rank[0]],distributions[:, parc,rank[1]])\n",
    "                            if p<0.05:\n",
    "                                rankorder[0,parc] = rank[0] + 1\n",
    "                                # test second versus third place\n",
    "                                t, p = scp.stats.ttest_ind(distributions[:,parc,rank[1]],distributions[:, parc,rank[2]])\n",
    "                                if p<0.05:\n",
    "                                    rankorder[1,parc] = rank[1] + 1\n",
    "                                    rankorder[2,parc] = rank[2] + 1\n",
    "                                else:\n",
    "                                    rankorder[1,parc] = int('{0}{1}'.format(rank[1],rank[2]))\n",
    "                            else:\n",
    "                                t, p = scp.stats.ttest_rel(distributions[:, parc,rank[0]],distributions[:, parc,rank[2]])\n",
    "                                if p<0.05:\n",
    "                                    rankorder[0,parc] = int('{0}{1}'.format(models[0],models[1]))\n",
    "                                    rankorder[1,parc] = rank[2] + 1\n",
    "                                else:\n",
    "                                    rankorder[0,parc] = 123\n",
    "                    np.save(os.path.join(outdir, 'model_rank_orders.npy'), rankorder)\n",
    "\n",
    "                    # make cifti label file with best model fit\n",
    "                    ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "                    data = nib.load(atlas_file).get_fdata()\n",
    "                    ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "                    newmap=dict()\n",
    "                    newmap[0] = ax0[0][1][0]\n",
    "                    for a in range(0,len(parcel_labels)):\n",
    "                        newmap[a+1] = (ciftkey[rankorder[0,a]]['label'] + '_n{0}'.format(a), ciftkey[rankorder[0,a]]['cifticolor'])\n",
    "                    ax0.label[0] = newmap\n",
    "                    img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "                    nib.save(img, os.path.join(outdir, 'top_model_fits.dlabel.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fca9e5-47d1-4a75-b63e-3a7823c5ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for age in groups:\n",
    "    for clin in ['SCARED_SR_SC','SCARED_P_SC','SCARED_P_GD', 'SCARED_SR_GD','SCARED_P_SP', 'SCARED_SR_SP']:\n",
    "        if os.path.isdir(os.path.join(out_dir, 'agegroup_similarity_regagesxs','noregage',age,'ts_isc_rubic_movie{0}_{1}'.format(movie, clin))):\n",
    "            # find consistent patterns across discovery and replication samples\n",
    "            for movie in ['TP','DM']:\n",
    "                disc_fits = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxs','noregage',age, \n",
    "                                                 'ts_isc_rubic_movie{0}_{1}'.format(movie, clin), 'model_rank_orders.npy'))\n",
    "                rep_fits = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxs','noregage',age, \n",
    "                                                'ts_isc_cbic_movie{0}_{1}'.format(movie, clin), 'model_rank_orders.npy'))\n",
    "\n",
    "                outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs','noregage',age, 'ts_isc_movie{0}_{1}'.format(movie, clin))\n",
    "\n",
    "                ### make a cifti with the outputs\n",
    "                # make label file with best model fit\n",
    "                mask = np.zeros((disc_fits.shape[1]))\n",
    "                mask[(disc_fits[0]==rep_fits[0])]=1\n",
    "\n",
    "                both_fits = disc_fits[0]\n",
    "                both_fits[mask==0] = 0\n",
    "                np.save(os.path.join(outdir,'replicable_top_models.npy'), both_fits)\n",
    "\n",
    "                ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "                data = nib.load(atlas_file).get_fdata()\n",
    "                ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "                newmap=dict()\n",
    "                newmap[0] = ax0[0][1][0]\n",
    "                for a in range(0,len(parcel_labels)):\n",
    "                    newmap[a+1] = (ciftkey[both_fits[a]]['label'] + '_n{0}'.format(a), ciftkey[both_fits[a]]['cifticolor'])\n",
    "                ax0.label[0] = newmap\n",
    "                img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "                nib.save(img, os.path.join(outdir, 'top_model_fits_replicable.dlabel.nii'))\n",
    "            \n",
    "            # find consistent patterns across movies\n",
    "            outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxs', 'noregage',age, 'ts_isc_bothmovies_{0}'.format(clin))\n",
    "            os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "            DM = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxs', 'noregage',age, \n",
    "                                      'ts_isc_movieDM_{0}'.format(clin),'replicable_top_models.npy'))\n",
    "            TP = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxs', 'noregage',age, \n",
    "                                      'ts_isc_movieTP_{0}'.format(clin),'replicable_top_models.npy'))\n",
    "\n",
    "            mask = np.zeros((DM.shape[0]))\n",
    "            mask[(DM==TP)]=1\n",
    "\n",
    "            both_fits = DM\n",
    "            both_fits[mask==0] = 0\n",
    "\n",
    "            np.save(os.path.join(outdir,'samebothvideos_top_models.npy'), both_fits)\n",
    "            ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "            data = nib.load(atlas_file).get_fdata()\n",
    "            ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "            newmap=dict()\n",
    "            newmap[0] = ax0[0][1][0]\n",
    "            for a in range(0,len(parcel_labels)):\n",
    "                newmap[a+1] = (ciftkey[both_fits[a]]['label'] + '_n{0}'.format(a), ciftkey[both_fits[a]]['cifticolor'])\n",
    "            ax0.label[0] = newmap\n",
    "            img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "            nib.save(img, os.path.join(outdir, 'top_model_fits_replicable_bothmovies.dlabel.nii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e99e05-8bfe-4090-887d-059e9da840c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# How are total symptom scores associated with activation patterns in each age group (and across age) covarying age, sex, depression, and  ADHD symptoms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26ef25-69ae-4d8d-bdc9-be7648284688",
   "metadata": {},
   "outputs": [],
   "source": [
    "for clin in ['SCARED_P_SC', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_GD','SCARED_P_SP', 'SCARED_SR_SP']:\n",
    "    if 'SR' in clin:\n",
    "        other = 'MFQ_SR_Total'\n",
    "    else:\n",
    "        other = 'MFQ_P_Total'\n",
    "    for sample in ['rubic','cbic']:\n",
    "        for movie in ['TP','DM']:\n",
    "            sampleinfo = subinfo.loc[(subinfo['site']==sample) & (subinfo['movie']==movie) & np.isfinite(subinfo[clin]),:]\n",
    "            sampleinfo.loc[:,['age','meanFD', clin, other, 'SWAN_Avg']] = StandardScaler().fit_transform(sampleinfo.loc[:,['age', 'meanFD', clin, other, 'SWAN_Avg']])\n",
    "            sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other,'SWAN_Avg']] = \\\n",
    "            IterativeImputer(random_state=42).fit_transform(sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other,'SWAN_Avg']].to_numpy())\n",
    "            res = smf.ols('{0} ~ age + female + meanFD + {1} + SWAN_Avg'.format(clin, other), data=sampleinfo).fit()\n",
    "            sampleinfo[clin] = res.resid.to_frame().iloc[:,0]\n",
    "\n",
    "            print(clin, movie, sample, 'N =', len(sampleinfo))\n",
    "            outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd', 'all', \n",
    "                                  'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "            os.makedirs(outdir, exist_ok=True)\n",
    "            group_data_file = os.path.join(outdir, 'compiled_timeseries_data_{0}_movie{1}.npy'.format(sample, movie))\n",
    "            if os.path.isfile(group_data_file):\n",
    "                group_data = np.load(group_data_file)\n",
    "            else:\n",
    "                group_data = compile_ts_data(sampleinfo, movie, data_dir, group_data_file)\n",
    "\n",
    "            outprefix = os.path.join(outdir, '{0}_movie{1}_'.format(sample, movie))\n",
    "            if os.path.isfile(outprefix + 'intersub_timeseries_ISC.npy'):\n",
    "                regional_sim_data = np.load(outprefix + 'intersub_timeseries_ISC.npy')\n",
    "            else:\n",
    "                regional_sim_data, mean_isc = intersubject_timeseries_correlation(group_data, outprefix)\n",
    "\n",
    "            outfile_prefix = os.path.join(outdir, '{0}_movie{1}_{2}_similarity'.format(sample, movie, clin))\n",
    "            isdistances = intersubject_distance(sampleinfo[clin].to_numpy(), outfile_prefix)\n",
    "\n",
    "            for sim in ['NN', 'AnnaKmin', 'AnnaKmaxminmax']:\n",
    "                outprefix = os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity'.format(sample, movie, clin, sim))\n",
    "                bx_sim_data = isdistances[sim]\n",
    "                if not os.path.isfile(outprefix + '_permsim_raw_rho.pscalar.nii'):\n",
    "                    isc_rho = regional_perm_bx_isrsa(regional_sim_data, bx_sim_data, outprefix)\n",
    "                    \n",
    "\n",
    "for clin in ['SCARED_P_SC', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_GD','SCARED_P_SP', 'SCARED_SR_SP']:\n",
    "    for movie in ['DM','TP']:\n",
    "        analysis_outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd', 'all', 'ts_isc_movie{0}_{1}'.format(movie, clin))\n",
    "        os.makedirs(analysis_outdir, exist_ok=True)\n",
    "\n",
    "        for sim in ['AnnaKmaxminmax','NN','AnnaKmin']:\n",
    "            disc_pval = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd','all','ts_isc_rubic_movie{0}_{1}'.format(movie, clin),\n",
    "                                     'rubic_movie{0}_{1}_{2}_similarity_permsim_raw_pval.pscalar.nii'.format(movie, clin, sim))\n",
    "            disc_rho = os.path.join(out_dir,'agegroup_similarity_regagesxsadhd', 'all','ts_isc_rubic_movie{0}_{1}'.format(movie, clin),\n",
    "                                    'rubic_movie{0}_{1}_{2}_similarity_permsim_raw_rho.pscalar.nii'.format(movie, clin, sim))\n",
    "            rep_pval = os.path.join(out_dir,'agegroup_similarity_regagesxsadhd', 'all','ts_isc_cbic_movie{0}_{1}'.format(movie, clin),\n",
    "                                    'cbic_movie{0}_{1}_{2}_similarity_permsim_raw_pval.pscalar.nii'.format(movie, clin, sim))\n",
    "            rep_rho = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd','all','ts_isc_cbic_movie{0}_{1}'.format(movie, clin),\n",
    "                                   'cbic_movie{0}_{1}_{2}_similarity_permsim_raw_rho.pscalar.nii'.format(movie, clin, sim))\n",
    "\n",
    "            outprefix = os.path.join(analysis_outdir, 'movie{0}_isc_{1}_{2}'.format(movie, clin, sim))\n",
    "            region_isrsa_fdr(disc_rho, disc_pval, rep_rho, rep_pval, outprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec6bca-f7bc-4c7b-9bc3-9052498dffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = int(10000/len(parcel_labels))\n",
    "\n",
    "#for clin in ['SCARED_P_SC', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_GD']:\n",
    "for clin in ['SCARED_P_SP', 'SCARED_SR_SP']:\n",
    "    for sample in ['rubic','cbic']:\n",
    "        for movie in ['TP','DM']:\n",
    "            print(sample, movie, clin)\n",
    "            outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd', 'all', \n",
    "                                  'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "\n",
    "            outprefix = os.path.join(outdir, '{0}_movie{1}_'.format(sample, movie))\n",
    "            if os.path.isfile(outprefix + 'intersub_timeseries_ISC.npy'):\n",
    "                regional_sim_data = np.load(outprefix + 'intersub_timeseries_ISC.npy')\n",
    "            else:\n",
    "                regional_sim_data, mean_isc = intersubject_timeseries_correlation(group_data, outprefix)\n",
    "\n",
    "            for sim in ['NN', 'AnnaKmin', 'AnnaKmaxminmax']:\n",
    "                print(sample, movie, sim, clin)\n",
    "                sig = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_pval.pscalar.nii'.format(sample, movie, clin, sim))).get_fdata()\n",
    "                rho = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_rho.pscalar.nii'.format(sample, movie, clin, sim))).get_fdata()\n",
    "                sigrho = rho\n",
    "                sigrho[sig>0.05] = 0\n",
    "                sim_filename = os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}.npy'.format(sample, movie, clin, sim))\n",
    "                simdist_filename = os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}_bootdist.npy'.format(sample, movie, clin, sim))\n",
    "                bx_sim_data = np.load(sim_filename)\n",
    "\n",
    "                rhodist = np.zeros((niters, regional_sim_data.shape[0]))\n",
    "                for parc in range(0,regional_sim_data.shape[0]):\n",
    "                    parcsim = regional_sim_data[parc,:,:]\n",
    "                    if sigrho[0,parc]>0:\n",
    "                        for i in range(0,niters):\n",
    "                            bootsample_size = np.random.randint(int(regional_sim_data.shape[1]*0.5),int(regional_sim_data.shape[1]*0.75))\n",
    "                            subsampmask = np.full(regional_sim_data.shape[1], 0)\n",
    "                            subsampmask[:bootsample_size] = 1\n",
    "                            np.random.shuffle(subsampmask)\n",
    "                            subsamp_bx = bx_sim_data[subsampmask==1, :][:, subsampmask==1]\n",
    "                            subsamp_brain = parcsim[subsampmask==1, :][:, subsampmask==1]\n",
    "                            res = static_brain_bx_isrsa(subsamp_brain, subsamp_bx)\n",
    "                            rhodist[i,parc] = res.loc[0,'SpearR']\n",
    "                np.save(simdist_filename, rhodist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553571d4-c8f3-4693-975e-cb24a3234cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.sqrt(0.05/len(parcel_labels))\n",
    "ciftkey = {0: {'label':'None', 'cifticolor':(1, 1, 1, 0), 'plotcolor':'#3893f5'}, # clear\n",
    "           1: {'label':'NN', 'cifticolor':(56/255, 147/255, 245/255, 1), 'plotcolor':'#3893f5'}, # blue\n",
    "           2: {'label':'AnnaKmin', 'cifticolor':(235/255, 18/255, 7/255, 1), 'plotcolor': '#eb1207'}, # red\n",
    "           3: {'label':'AnnaKmaxminmax', 'cifticolor':(255/255, 230/255, 0/255, 1), 'plotcolor': '#ffe600'}, # gold\n",
    "           12: {'label':'NN-AnnaKmin', 'cifticolor':(128/255, 4/255, 186/255, 1), 'plotcolor':'#8004ba'}, # purple\n",
    "           21: {'label':'NN-AnnaKmin', 'cifticolor':(128/255, 4/255, 186/255, 1), 'plotcolor':'#8004ba'}, # purple\n",
    "           13: {'label':'NN-AnnaKmaxminmax', 'cifticolor':(224/255, 152/255, 7/255, 1), 'plotcolor': '#e09807'}, # orange\n",
    "           31: {'label':'NN-AnnaKmaxminmax', 'cifticolor':(224/255, 152/255, 7/255, 1), 'plotcolor': '#e09807'}, # orange\n",
    "           23: {'label':'AnnaKmin-AnnaKmaxminmax', 'cifticolor':(2/255, 156/255, 12/255, 1), 'plotcolor': '#029c0c'}, # green\n",
    "           32: {'label':'AnnaKmin-AnnaKmaxminmax', 'cifticolor':(2/255, 156/255, 12/255, 1), 'plotcolor': '#029c0c'},\n",
    "           123: {'label':'All', 'cifticolor':(255/255, 255/255, 255/255, 1), 'plotcolor': '#029c0c'}} # white\n",
    "\n",
    "#for clin in ['SCARED_P_SC', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_GD']:\n",
    "for clin in ['SCARED_P_SP', 'SCARED_SR_SP']:\n",
    "    for sample in ['rubic','cbic']:\n",
    "        for movie in ['TP','DM']:\n",
    "            # per parcel, rank point estimates and identify best fitting model\n",
    "            fitkey = {'NN': {'value':1, 'cifticolor':(56/255, 147/255, 245/255, 1), 'plotcolor':'#3893f5'}, # blue\n",
    "                      'AnnaKmin': {'value':2, 'cifticolor':(235/255, 18/255, 7/255, 1), 'plotcolor': '#eb1207'}, # red\n",
    "                      'AnnaKmaxminmax': {'value':3, 'cifticolor':(255/255, 230/255, 0/255, 1), 'plotcolor': '#ffe600'}} # gold\n",
    "            sims = list(fitkey.keys())\n",
    "\n",
    "            print(sample, movie, clin)\n",
    "            outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd', 'all', 'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "\n",
    "            # load point estimates for each model of development\n",
    "            pointests = []\n",
    "            pointestsps = []\n",
    "            distributions = []\n",
    "            for est in sims:\n",
    "                point = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_rho.pscalar.nii'.format(sample, movie, clin, est))).get_fdata()\n",
    "                pointests.append(point)\n",
    "                pointps = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_pval.pscalar.nii'.format(sample, movie, clin, est))).get_fdata()\n",
    "                pointestsps.append(pointps)\n",
    "                dist = np.load(os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}_bootdist.npy'.format(sample, movie, clin, est)))\n",
    "                distributions.append(np.expand_dims(dist, axis=2))\n",
    "            distributions = np.concatenate(distributions, axis=2)\n",
    "            pointests = np.concatenate(pointests, axis=0)\n",
    "            pointestsps = np.concatenate(pointestsps, axis=0)\n",
    "\n",
    "            pointests_binarized = np.zeros_like(pointests)\n",
    "            pointests_binarized[(pointests>0) & (pointestsps<alpha)] = 1\n",
    "            sigpointestssum = np.sum(pointests_binarized, axis=0).astype(int)\n",
    "\n",
    "            rankorder = np.zeros((len(sims), pointests.shape[1]))\n",
    "\n",
    "            for parc in range(0, sigpointestssum.shape[0]):\n",
    "                parc_bin = pointests_binarized[:,parc]\n",
    "                if sigpointestssum[parc]==1:\n",
    "                    rankorder[0,parc] = np.where(parc_bin==1)[0][0] + 1\n",
    "                elif sigpointestssum[parc]==2:\n",
    "                    models = np.where(parc_bin==1)[0]\n",
    "                    t, p = scp.stats.ttest_rel(distributions[:, parc,models[0]],distributions[:, parc,models[1]])\n",
    "                    if p<0.05:\n",
    "                        if pointests[models[0], parc] > pointests[models[1], parc]:\n",
    "                            rankorder[0,parc] = models[0] + 1\n",
    "                            rankorder[1,parc] = models[1] + 1\n",
    "                        else:\n",
    "                            rankorder[0,parc] = models[1] + 1\n",
    "                            rankorder[1,parc] = models[0] + 1\n",
    "                    else:\n",
    "                        rankorder[0,parc] = int('{0}{1}'.format(models[0],models[1]))\n",
    "                elif sigpointestssum[parc]==3:\n",
    "                    tpoint = pointests[:,parc]\n",
    "                    rank = np.argsort(tpoint)\n",
    "                    t, p = scp.stats.ttest_rel(distributions[:,parc,rank[0]],distributions[:, parc,rank[1]])\n",
    "                    if p<0.05:\n",
    "                        rankorder[0,parc] = rank[0] + 1\n",
    "                        # test second versus third place\n",
    "                        t, p = scp.stats.ttest_ind(distributions[:,parc,rank[1]],distributions[:, parc,rank[2]])\n",
    "                        if p<0.05:\n",
    "                            rankorder[1,parc] = rank[1] + 1\n",
    "                            rankorder[2,parc] = rank[2] + 1\n",
    "                        else:\n",
    "                            rankorder[1,parc] = int('{0}{1}'.format(rank[1],rank[2]))\n",
    "                    else:\n",
    "                        t, p = scp.stats.ttest_rel(distributions[:, parc,rank[0]],distributions[:, parc,rank[2]])\n",
    "                        if p<0.05:\n",
    "                            rankorder[0,parc] = int('{0}{1}'.format(models[0],models[1]))\n",
    "                            rankorder[1,parc] = rank[2] + 1\n",
    "                        else:\n",
    "                            rankorder[0,parc] = 123\n",
    "            np.save(os.path.join(outdir, 'model_rank_orders.npy'), rankorder)\n",
    "\n",
    "            # make cifti label file with best model fit\n",
    "            ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "            data = nib.load(atlas_file).get_fdata()\n",
    "            ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "            newmap=dict()\n",
    "            newmap[0] = ax0[0][1][0]\n",
    "            for a in range(0,len(parcel_labels)):\n",
    "                newmap[a+1] = (ciftkey[rankorder[0,a]]['label'] + '_n{0}'.format(a), ciftkey[rankorder[0,a]]['cifticolor'])\n",
    "            ax0.label[0] = newmap\n",
    "            img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "            nib.save(img, os.path.join(outdir, 'top_model_fits.dlabel.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b9a93-fb37-437c-8d5c-252b8273a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clin in ['SCARED_P_SC', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_GD']:\n",
    "for clin in ['SCARED_P_SP', 'SCARED_SR_SP']:\n",
    "    # find consistent patterns across discovery and replication samples\n",
    "    for movie in ['TP','DM']:\n",
    "        disc_fits = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd','all', 'ts_isc_rubic_movie{0}_{1}'.format(movie, clin), 'model_rank_orders.npy'))\n",
    "        rep_fits = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd','all', 'ts_isc_cbic_movie{0}_{1}'.format(movie, clin), 'model_rank_orders.npy'))\n",
    "\n",
    "        outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd','all', 'ts_isc_movie{0}_{1}'.format(movie, clin))\n",
    "\n",
    "        ### make a cifti with the outputs\n",
    "        # make label file with best model fit\n",
    "        mask = np.zeros((disc_fits.shape[1]))\n",
    "        mask[(disc_fits[0]==rep_fits[0])]=1\n",
    "\n",
    "        both_fits = disc_fits[0]\n",
    "        both_fits[mask==0] = 0\n",
    "        np.save(os.path.join(outdir,'replicable_top_models.npy'), both_fits)\n",
    "\n",
    "        ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "        data = nib.load(atlas_file).get_fdata()\n",
    "        ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "        newmap=dict()\n",
    "        newmap[0] = ax0[0][1][0]\n",
    "        for a in range(0,len(parcel_labels)):\n",
    "            newmap[a+1] = (ciftkey[both_fits[a]]['label'] + '_n{0}'.format(a), ciftkey[both_fits[a]]['cifticolor'])\n",
    "        ax0.label[0] = newmap\n",
    "        img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "        nib.save(img, os.path.join(outdir, 'top_model_fits_replicable.dlabel.nii'))\n",
    "        \n",
    "    # find consistent patterns across both movies\n",
    "    outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd','all', 'ts_isc_bothmovies_{0}'.format(clin))\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    DM = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd','all', 'ts_isc_movieDM_{0}'.format(clin),'replicable_top_models.npy'))\n",
    "    TP = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd','all', 'ts_isc_movieTP_{0}'.format(clin),'replicable_top_models.npy'))\n",
    "\n",
    "    mask = np.zeros((DM.shape[0]))\n",
    "    mask[(DM==TP)]=1\n",
    "\n",
    "    both_fits = DM\n",
    "    both_fits[mask==0] = 0\n",
    "\n",
    "    np.save(os.path.join(outdir,'samebothvideos_top_models.npy'), both_fits)\n",
    "    ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "    data = nib.load(atlas_file).get_fdata()\n",
    "    ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "    newmap=dict()\n",
    "    newmap[0] = ax0[0][1][0]\n",
    "    for a in range(0,len(parcel_labels)):\n",
    "        newmap[a+1] = (ciftkey[both_fits[a]]['label'] + '_n{0}'.format(a), ciftkey[both_fits[a]]['cifticolor'])\n",
    "    ax0.label[0] = newmap\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "    nib.save(img, os.path.join(outdir, 'top_model_fits_replicable_bothmovies.dlabel.nii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c73090-c1f6-4052-aa24-43c8c93d9d66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IS-RSA for each age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02afc82-5d88-4ff8-b08b-b5900bd0e15c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groups = np.unique(subinfo['age_group'])\n",
    "\n",
    "for age in groups:\n",
    "    agesubinfo = subinfo.loc[subinfo['age_group']==age,:]\n",
    "    if age=='younger':  \n",
    "        clins = ['SCARED_P_SC','SCARED_P_GD','SCARED_P_SP']\n",
    "    else:\n",
    "        clins = ['SCARED_P_SC', 'SCARED_P_GD','SCARED_P_SP', 'SCARED_SR_SC','SCARED_SR_GD', 'SCARED_SR_SP']\n",
    "\n",
    "    for clin in clins:\n",
    "        if 'SR' in clin:\n",
    "            other = 'MFQ_SR_Total'\n",
    "        else:\n",
    "            other = 'MFQ_P_Total'\n",
    "        for sample in ['rubic','cbic']:\n",
    "            for movie in ['TP','DM']:\n",
    "                sampleinfo = agesubinfo.loc[(agesubinfo['site']==sample) & (agesubinfo['movie']==movie) & np.isfinite(agesubinfo[clin]),:]\n",
    "                sampleinfo.loc[:,['age','meanFD', clin, other, 'SWAN_Avg']] = StandardScaler().fit_transform(sampleinfo.loc[:,['age', 'meanFD', clin, other, 'SWAN_Avg']])\n",
    "                sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other, 'SWAN_Avg']] = \\\n",
    "                IterativeImputer(random_state=42).fit_transform(sampleinfo.loc[:,['age', 'female', 'meanFD', clin, other, 'SWAN_Avg']])\n",
    "                res = smf.ols('{0} ~ age + female + meanFD + {1} + SWAN_Avg'.format(clin, other), data=sampleinfo).fit()\n",
    "                sampleinfo[clin] = res.resid.to_frame().iloc[:,0]\n",
    "                \n",
    "                print(age, clin, movie, sample, 'N =', len(sampleinfo))\n",
    "                outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd', age, \n",
    "                                      'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "                os.makedirs(outdir, exist_ok=True)\n",
    "                group_data_file = os.path.join(outdir, 'compiled_timeseries_data_{0}_movie{1}.npy'.format(sample, movie))\n",
    "                if os.path.isfile(group_data_file):\n",
    "                    group_data = np.load(group_data_file)\n",
    "                else:\n",
    "                    group_data = compile_ts_data(sampleinfo, movie, data_dir, group_data_file)\n",
    "\n",
    "                outprefix = os.path.join(outdir, '{0}_movie{1}_'.format(sample, movie))\n",
    "                if os.path.isfile(outprefix + 'intersub_timeseries_ISC.npy'):\n",
    "                    regional_sim_data = np.load(outprefix + 'intersub_timeseries_ISC.npy')\n",
    "                else:\n",
    "                    regional_sim_data, mean_isc = intersubject_timeseries_correlation(group_data, outprefix)\n",
    "\n",
    "                outfile_prefix = os.path.join(outdir, '{0}_movie{1}_{2}_similarity'.format(sample, movie, clin))\n",
    "                isdistances = intersubject_distance(sampleinfo[clin].to_numpy(), outfile_prefix)\n",
    "\n",
    "                for sim in ['NN', 'AnnaKmin', 'AnnaKmaxminmax']:\n",
    "                    outprefix = os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity'.format(sample, movie, clin, sim))\n",
    "                    bx_sim_data = isdistances[sim]\n",
    "                    if not os.path.isfile(outprefix + '_permsim_raw_rho.pscalar.nii'):\n",
    "                        isc_rho = regional_perm_bx_isrsa(regional_sim_data, bx_sim_data, outprefix)\n",
    "                        \n",
    "        for movie in ['DM','TP']:\n",
    "            analysis_outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd', age, 'ts_isc_movie{0}_{1}'.format(movie, clin))\n",
    "            os.makedirs(analysis_outdir, exist_ok=True)\n",
    "\n",
    "            for sim in ['AnnaKmaxminmax','NN','AnnaKmin']:\n",
    "                disc_pval = os.path.join(out_dir, 'agegroup_similarity_regagesxs',age,'ts_isc_rubic_movie{0}_{1}'.format(movie, clin),\n",
    "                                         'rubic_movie{0}_{1}_{2}_similarity_permsim_raw_pval.pscalar.nii'.format(movie, clin, sim))\n",
    "                disc_rho = os.path.join(out_dir,'agegroup_similarity_regagesxs', age,'ts_isc_rubic_movie{0}_{1}'.format(movie, clin),\n",
    "                                        'rubic_movie{0}_{1}_{2}_similarity_permsim_raw_rho.pscalar.nii'.format(movie, clin, sim))\n",
    "                rep_pval = os.path.join(out_dir,'agegroup_similarity_regagesxs', age,'ts_isc_cbic_movie{0}_{1}'.format(movie, clin),\n",
    "                                        'cbic_movie{0}_{1}_{2}_similarity_permsim_raw_pval.pscalar.nii'.format(movie, clin, sim))\n",
    "                rep_rho = os.path.join(out_dir, 'agegroup_similarity_regagesxs',age,'ts_isc_cbic_movie{0}_{1}'.format(movie, clin),\n",
    "                                       'cbic_movie{0}_{1}_{2}_similarity_permsim_raw_rho.pscalar.nii'.format(movie, clin, sim))\n",
    "\n",
    "                outprefix = os.path.join(analysis_outdir, 'movie{0}_isc_{1}_{2}'.format(movie, clin, sim))\n",
    "                region_isrsa_fdr(disc_rho, disc_pval, rep_rho, rep_pval, outprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba632e-a471-41e6-85a5-e253100f29e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "niters = int(10000/len(parcel_labels))\n",
    "\n",
    "for age in groups:\n",
    "    for clin in ['SCARED_P_SC', 'SCARED_SR_GD','SCARED_P_SP', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_SP']:\n",
    "        if os.path.isdir(os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd',age,'ts_isc_rubic_movie{0}_{1}'.format(movie, clin))):\n",
    "            for sample in ['rubic','cbic']:\n",
    "                for movie in ['TP','DM']:\n",
    "                    print(sample, movie, clin)\n",
    "                    outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd', age, \n",
    "                                          'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "\n",
    "                    outprefix = os.path.join(outdir, '{0}_movie{1}_'.format(sample, movie))\n",
    "                    if os.path.isfile(outprefix + 'intersub_timeseries_ISC.npy'):\n",
    "                        regional_sim_data = np.load(outprefix + 'intersub_timeseries_ISC.npy')\n",
    "                    else:\n",
    "                        regional_sim_data, mean_isc = intersubject_timeseries_correlation(group_data, outprefix)\n",
    "\n",
    "                    for sim in ['NN', 'AnnaKmin', 'AnnaKmaxminmax']:\n",
    "                        print(sample, movie, sim, clin)\n",
    "                        sig = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_pval.pscalar.nii'.format(sample, movie, clin, sim))).get_fdata()\n",
    "                        rho = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_rho.pscalar.nii'.format(sample, movie, clin, sim))).get_fdata()\n",
    "                        sigrho = rho\n",
    "                        sigrho[sig>0.05] = 0\n",
    "                        sim_filename = os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}.npy'.format(sample, movie, clin, sim))\n",
    "                        simdist_filename = os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}_bootdist.npy'.format(sample, movie, clin, sim))\n",
    "                        bx_sim_data = np.load(sim_filename)\n",
    "\n",
    "                        rhodist = np.zeros((niters, regional_sim_data.shape[0]))\n",
    "                        for parc in range(0,regional_sim_data.shape[0]):\n",
    "                            parcsim = regional_sim_data[parc,:,:]\n",
    "                            if sigrho[0,parc]>0:\n",
    "                                for i in range(0,niters):\n",
    "                                    bootsample_size = np.random.randint(int(regional_sim_data.shape[1]*0.5),int(regional_sim_data.shape[1]*0.75))\n",
    "                                    subsampmask = np.full(regional_sim_data.shape[1], 0)\n",
    "                                    subsampmask[:bootsample_size] = 1\n",
    "                                    np.random.shuffle(subsampmask)\n",
    "                                    subsamp_bx = bx_sim_data[subsampmask==1, :][:, subsampmask==1]\n",
    "                                    subsamp_brain = parcsim[subsampmask==1, :][:, subsampmask==1]\n",
    "                                    res = static_brain_bx_isrsa(subsamp_brain, subsamp_bx)\n",
    "                                    rhodist[i,parc] = res.loc[0,'SpearR']\n",
    "                        np.save(simdist_filename, rhodist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cde0b0-325e-40fc-b4d2-42f3c609a1a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = np.sqrt(0.05/len(parcel_labels))\n",
    "ciftkey = {0: {'label':'None', 'cifticolor':(1, 1, 1, 0), 'plotcolor':'#3893f5'}, # clear\n",
    "           1: {'label':'NN', 'cifticolor':(56/255, 147/255, 245/255, 1), 'plotcolor':'#3893f5'}, # blue\n",
    "           2: {'label':'AnnaKmin', 'cifticolor':(235/255, 18/255, 7/255, 1), 'plotcolor': '#eb1207'}, # red\n",
    "           3: {'label':'AnnaKmaxminmax', 'cifticolor':(255/255, 230/255, 0/255, 1), 'plotcolor': '#ffe600'}, # gold\n",
    "           12: {'label':'NN-AnnaKmin', 'cifticolor':(128/255, 4/255, 186/255, 1), 'plotcolor':'#8004ba'}, # purple\n",
    "           21: {'label':'NN-AnnaKmin', 'cifticolor':(128/255, 4/255, 186/255, 1), 'plotcolor':'#8004ba'}, # purple\n",
    "           13: {'label':'NN-AnnaKmaxminmax', 'cifticolor':(224/255, 152/255, 7/255, 1), 'plotcolor': '#e09807'}, # orange\n",
    "           31: {'label':'NN-AnnaKmaxminmax', 'cifticolor':(224/255, 152/255, 7/255, 1), 'plotcolor': '#e09807'}, # orange\n",
    "           23: {'label':'AnnaKmin-AnnaKmaxminmax', 'cifticolor':(2/255, 156/255, 12/255, 1), 'plotcolor': '#029c0c'}, # green\n",
    "           32: {'label':'AnnaKmin-AnnaKmaxminmax', 'cifticolor':(2/255, 156/255, 12/255, 1), 'plotcolor': '#029c0c'},\n",
    "           123: {'label':'All', 'cifticolor':(255/255, 255/255, 255/255, 1), 'plotcolor': '#029c0c'}} # white\n",
    "\n",
    "for age in groups:\n",
    "    for clin in ['SCARED_P_SC', 'SCARED_SR_GD','SCARED_P_SP', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_SP']:\n",
    "        if os.path.isdir(os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd',age,'ts_isc_rubic_movie{0}_{1}'.format(movie, clin))):\n",
    "            for sample in ['rubic','cbic']:\n",
    "                for movie in ['TP','DM']:\n",
    "                    # per parcel, rank point estimates and identify best fitting model\n",
    "                    fitkey = {'NN': {'value':1, 'cifticolor':(56/255, 147/255, 245/255, 1), 'plotcolor':'#3893f5'}, # blue\n",
    "                              'AnnaKmin': {'value':2, 'cifticolor':(235/255, 18/255, 7/255, 1), 'plotcolor': '#eb1207'}, # red\n",
    "                              'AnnaKmaxminmax': {'value':3, 'cifticolor':(255/255, 230/255, 0/255, 1), 'plotcolor': '#ffe600'}} # gold\n",
    "                    sims = list(fitkey.keys())\n",
    "\n",
    "                    print(sample, movie, clin)\n",
    "                    outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd', age, 'ts_isc_{0}_movie{1}_{2}'.format(sample, movie, clin))\n",
    "\n",
    "                    # load point estimates for each model of development\n",
    "                    pointests = []\n",
    "                    pointestsps = []\n",
    "                    distributions = []\n",
    "                    for est in sims:\n",
    "                        point = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_rho.pscalar.nii'.format(sample, movie, clin, est))).get_fdata()\n",
    "                        pointests.append(point)\n",
    "                        pointps = nib.load(os.path.join(outdir, '{0}_movie{1}_{2}_{3}_similarity_permsim_raw_pval.pscalar.nii'.format(sample, movie, clin, est))).get_fdata()\n",
    "                        pointestsps.append(pointps)\n",
    "                        dist = np.load(os.path.join(outdir, '{0}_movie{1}_{2}_similarity_{3}_bootdist.npy'.format(sample, movie, clin, est)))\n",
    "                        distributions.append(np.expand_dims(dist, axis=2))\n",
    "                    distributions = np.concatenate(distributions, axis=2)\n",
    "                    pointests = np.concatenate(pointests, axis=0)\n",
    "                    pointestsps = np.concatenate(pointestsps, axis=0)\n",
    "\n",
    "                    pointests_binarized = np.zeros_like(pointests)\n",
    "                    pointests_binarized[(pointests>0) & (pointestsps<alpha)] = 1\n",
    "                    sigpointestssum = np.sum(pointests_binarized, axis=0).astype(int)\n",
    "\n",
    "                    rankorder = np.zeros((len(sims), pointests.shape[1]))\n",
    "\n",
    "                    for parc in range(0, sigpointestssum.shape[0]):\n",
    "                        parc_bin = pointests_binarized[:,parc]\n",
    "                        if sigpointestssum[parc]==1:\n",
    "                            rankorder[0,parc] = np.where(parc_bin==1)[0][0] + 1\n",
    "                        elif sigpointestssum[parc]==2:\n",
    "                            models = np.where(parc_bin==1)[0]\n",
    "                            t, p = scp.stats.ttest_rel(distributions[:, parc,models[0]],distributions[:, parc,models[1]])\n",
    "                            if p<0.05:\n",
    "                                if pointests[models[0], parc] > pointests[models[1], parc]:\n",
    "                                    rankorder[0,parc] = models[0] + 1\n",
    "                                    rankorder[1,parc] = models[1] + 1\n",
    "                                else:\n",
    "                                    rankorder[0,parc] = models[1] + 1\n",
    "                                    rankorder[1,parc] = models[0] + 1\n",
    "                            else:\n",
    "                                rankorder[0,parc] = int('{0}{1}'.format(models[0],models[1]))\n",
    "                        elif sigpointestssum[parc]==3:\n",
    "                            tpoint = pointests[:,parc]\n",
    "                            rank = np.argsort(tpoint)\n",
    "                            t, p = scp.stats.ttest_rel(distributions[:,parc,rank[0]],distributions[:, parc,rank[1]])\n",
    "                            if p<0.05:\n",
    "                                rankorder[0,parc] = rank[0] + 1\n",
    "                                # test second versus third place\n",
    "                                t, p = scp.stats.ttest_ind(distributions[:,parc,rank[1]],distributions[:, parc,rank[2]])\n",
    "                                if p<0.05:\n",
    "                                    rankorder[1,parc] = rank[1] + 1\n",
    "                                    rankorder[2,parc] = rank[2] + 1\n",
    "                                else:\n",
    "                                    rankorder[1,parc] = int('{0}{1}'.format(rank[1],rank[2]))\n",
    "                            else:\n",
    "                                t, p = scp.stats.ttest_rel(distributions[:, parc,rank[0]],distributions[:, parc,rank[2]])\n",
    "                                if p<0.05:\n",
    "                                    rankorder[0,parc] = int('{0}{1}'.format(models[0],models[1]))\n",
    "                                    rankorder[1,parc] = rank[2] + 1\n",
    "                                else:\n",
    "                                    rankorder[0,parc] = 123\n",
    "                    np.save(os.path.join(outdir, 'model_rank_orders.npy'), rankorder)\n",
    "\n",
    "                    # make cifti label file with best model fit\n",
    "                    ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "                    data = nib.load(atlas_file).get_fdata()\n",
    "                    ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "                    newmap=dict()\n",
    "                    newmap[0] = ax0[0][1][0]\n",
    "                    for a in range(0,len(parcel_labels)):\n",
    "                        newmap[a+1] = (ciftkey[rankorder[0,a]]['label'] + '_n{0}'.format(a), ciftkey[rankorder[0,a]]['cifticolor'])\n",
    "                    ax0.label[0] = newmap\n",
    "                    img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "                    nib.save(img, os.path.join(outdir, 'top_model_fits.dlabel.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b80e8-b342-4275-ab05-2e2c2c7de66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for age in groups:\n",
    "    for clin in ['SCARED_P_SC', 'SCARED_SR_GD','SCARED_P_SP', 'SCARED_SR_SC','SCARED_P_GD', 'SCARED_SR_SP']:\n",
    "        if os.path.isdir(os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd',age,'ts_isc_rubic_movie{0}_{1}'.format(movie, clin))):\n",
    "            # find consistent patterns across discovery and replication samples\n",
    "            for movie in ['TP','DM']:\n",
    "                disc_fits = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd',age, 'ts_isc_rubic_movie{0}_{1}'.format(movie, clin), 'model_rank_orders.npy'))\n",
    "                rep_fits = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd',age, 'ts_isc_cbic_movie{0}_{1}'.format(movie, clin), 'model_rank_orders.npy'))\n",
    "\n",
    "                outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd',age, 'ts_isc_movie{0}_{1}'.format(movie, clin))\n",
    "\n",
    "                ### make a cifti with the outputs\n",
    "                # make label file with best model fit\n",
    "                mask = np.zeros((disc_fits.shape[1]))\n",
    "                mask[(disc_fits[0]==rep_fits[0])]=1\n",
    "\n",
    "                both_fits = disc_fits[0]\n",
    "                both_fits[mask==0] = 0\n",
    "                np.save(os.path.join(outdir,'replicable_top_models.npy'), both_fits)\n",
    "\n",
    "                ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "                data = nib.load(atlas_file).get_fdata()\n",
    "                ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "                newmap=dict()\n",
    "                newmap[0] = ax0[0][1][0]\n",
    "                for a in range(0,len(parcel_labels)):\n",
    "                    newmap[a+1] = (ciftkey[both_fits[a]]['label'] + '_n{0}'.format(a), ciftkey[both_fits[a]]['cifticolor'])\n",
    "                ax0.label[0] = newmap\n",
    "                img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "                nib.save(img, os.path.join(outdir, 'top_model_fits_replicable.dlabel.nii'))\n",
    "            \n",
    "            # find consistent patterns across movies\n",
    "            outdir = os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd', age, 'ts_isc_bothmovies_{0}'.format(clin))\n",
    "            os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "            DM = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd', age, 'ts_isc_movieDM_{0}'.format(clin),'replicable_top_models.npy'))\n",
    "            TP = np.load(os.path.join(out_dir, 'agegroup_similarity_regagesxsadhd', age, 'ts_isc_movieTP_{0}'.format(clin),'replicable_top_models.npy'))\n",
    "\n",
    "            mask = np.zeros((DM.shape[0]))\n",
    "            mask[(DM==TP)]=1\n",
    "\n",
    "            both_fits = DM\n",
    "            both_fits[mask==0] = 0\n",
    "\n",
    "            np.save(os.path.join(outdir,'samebothvideos_top_models.npy'), both_fits)\n",
    "            ax1 = nib.load(atlas_file).header.get_axis(1)\n",
    "            data = nib.load(atlas_file).get_fdata()\n",
    "            ax0 = nib.load(atlas_file).header.get_axis(0)\n",
    "            newmap=dict()\n",
    "            newmap[0] = ax0[0][1][0]\n",
    "            for a in range(0,len(parcel_labels)):\n",
    "                newmap[a+1] = (ciftkey[both_fits[a]]['label'] + '_n{0}'.format(a), ciftkey[both_fits[a]]['cifticolor'])\n",
    "            ax0.label[0] = newmap\n",
    "            img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "            nib.save(img, os.path.join(outdir, 'top_model_fits_replicable_bothmovies.dlabel.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6f6b0-ca02-4aee-b701-9de61523ec0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
